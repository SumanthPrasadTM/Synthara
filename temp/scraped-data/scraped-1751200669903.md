# Web Scraping Results for: "Create medical dataset for AFI analysis with real measurements"

**Generated on:** 2025-06-29T12:37:49.903Z
**Total Sources:** 5
**Total Content Length:** 3,28,807 characters

## Table of Contents

1. [aimi.stanford.edu](#source-1)
2. [docs.aireadi.org](#source-2)
3. [www.nature.com](#source-3)
4. [www.nature.com](#source-4)
5. [pmc.ncbi.nlm.nih.gov](#source-5)

---

## Source 1: aimi.stanford.edu {#source-1}

**URL:** https://aimi.stanford.edu/shared-datasets
**Content Type:** Web Content
**Content Length:** 10,023 characters
**Scraped At:** 2025-06-29T12:37:49.886Z

**Detected Structure:** Tables, Structured Text

### Content:

# Shared Datasets

Main content start

Stanford AIMI shares annotated data to foster transparent and reproducible collaborative research to advance AI in medicine.

Our datasets are available to the public to view and use without charge for non-commercial research purposes. **For research use**, please click on the dataset titles below to be taken to the dataset download page. **For commercial use**, please submit a [commercial use interest form](https://forms.gle/8v76dJKSVvUaz7DJA) to start a conversation around the details.

**PLEASE NOTE:  All users of the AIMI data/images are expected to acknowledge Stanford AIMI in all publications, presentations, etc, with the following language:** _“This research used data provided by the Stanford Center for Artificial Intelligence in Medicine and Imaging (AIMI). AIMI curated a publicly available imaging data repository containing clinical imaging and data from Stanford Health Care, the Stanford Children’s Hospital, the University Healthcare Alliance and Packard Children's Health Alliance clinics provisioned for research use by the Stanford Medicine Research Data Repository (STARR).”_

* * *

## **Featured Datasets**

|     |     |
| --- | --- |
| [CheXpert Plus](https://stanfordaimi.azurewebsites.net/datasets/5158c524-d3ab-4e02-96e9-6ee9efc110a1) | Notable for its organization and depth, the CheXpert Plus dataset is a comprehensive collection that brings together text and images in the medical field, featuring a total of 223,462 unique pairs of radiology reports and chest X-rays across 187,711 studies from 64,725 patients. |

## All Datasets

| Name | Description |
| --- | --- |
| **Name** <br>[BrainMetShare](https://aimi.stanford.edu/datasets/brainmetshare) | **Description** <br>156 pre- and post-contrast whole brain MRI studies, including high-resolution, multi-modal pre- and post-contrast sequences in patients with at least 1 brain metastasis accompanied by ground-truth segmentations by radiologists. |
| **Name** <br>[COCA - Coronary Calcium and Chest CTs](https://aimi.stanford.edu/datasets/coca-coronary-calcium-chest-ct) | **Description** <br>We provide two datasets: 1) gated coronary CT DICOM images with corresponding coronary artery calcium segmentations and scores (xml files) 2) non-gated chest CT DICOM images with coronary artery calcium scores |
| **Name** <br>[CT Pulmonary Angiography](https://aimi.stanford.edu/datasets/CT-Pulmonary-Angiography) | **Description** <br>A collection of CT pulmonary angiography (CTPA) for patients susceptible to Pulmonary Embolism (PE). In addition to slice-level PE labels, we provide labels for PE location, RV/LV ratio, and PE type. |
| **Name** <br>[CheXlocalize](https://aimi.stanford.edu/datasets/chexlocalize) | **Description** <br>CheXlocalize is a radiologist-annotated segmentation dataset on chest X-rays. The dataset consists of two types of radiologist annotations for the localization of 10 pathologies: pixel-level segmentations and most-representative points.  The validation and test sets consist of 234 chest X-rays from 200 patients and 668 chest X-rays from 500 patients, respectively. |
| **Name** <br>[CheXpert Demo Data](https://aimi.stanford.edu/datasets/chexpert-demo-data) | **Description** <br>Self-reported race labels for the popular CheXpert dataset in the interest of open science, experimental validation and reproducibility, and to encourage further work in this important area. |
| **Name** [CheXpert Plus](https://aimi.stanford.edu/datasets/chexpert-plus) | **Description** The CheXpert Plus dataset is a comprehensive collection that brings together text and images in the medical field, featuring a total of 223,462 unique pairs of radiology reports and chest X-rays across 187,711 studies from 64,725 patients. |
| **Name** <br>[CheXpert: Chest X-rays](https://aimi.stanford.edu/datasets/chexpert-chest-x-rays) | **Description** <br>224,316 chest radiographs of 65,240 patients who underwent a radiographic examination at Stanford between October 2002 and July 2017, in both inpatient and outpatient centers. |
| **Name** <br>[CheXphoto](https://aimi.stanford.edu/datasets/chexphoto-chest-x-rays) | **Description** <br>A training set of natural photos and synthetic transformations of 10,507 x-rays from 3,000 unique patients that were sampled at random from the CheXpert training set, and a validation and test set of natural and synthetic transformations applied to all 234 x-rays from 200 patients and 668 x-rays from 500 patients in the CheXpert validation and test sets, respectively. |
| **Name** <br>[DDI - Diverse Dermatology Images](https://aimi.stanford.edu/datasets/ddi-diverse-dermatology-images) | **Description** <br>Artificial intelligence (AI) may aid in triaging skin disease.  However, most AI models have not been rigorously assessed on images of diverse skin tones or uncommon diseases. To ascertain potential biases in algorithm performance in this context, we curated the Diverse Dermatology Images (DDI) dataset - the first publicly available, deeply curated, and pathologically confirmed image dataset with diverse skin tones. |
| **Name** <br>[EchoNet-Dynamic](https://aimi.stanford.edu/datasets/echonet-dynamic-cardiac-ultrasound) | **Description** <br>10,030 labeled echocardiogram videos and human expert annotations (measurements, tracings, and calculations) to provide a baseline to study cardiac motion and chamber sizes. |
| **Name** <br>[EchoNet-LVH](https://aimi.stanford.edu/datasets/echonet-lvh) | **Description** <br>The EchoNet-LVH dataset includes 12,000 labeled echocardiogram videos and human expert annotations (measurements, tracings, and calculations) to provide a baseline to study cardiac chamber size and wall thickness. |
| **Name** <br>[EchoNet-Pediatric](https://aimi.stanford.edu/datasets/echonet-pediatric) | **Description** <br>The EchoNet-Peds database includes 7,643 labeled echocardiogram videos and human expert annotations (measurements, tracings, and calculations) to provide a baseline to study cardiac motion and chamber sizes. The database includes patients ranging from 0-18 years (43% female) with a wide range of sizes. |
| **Name** [EchoNet- Tee-View-Classifier](https://aimi.stanford.edu/datasets/echonet-tee-view-classifier) | **Description** Intraoperative TEE videos from approximately 500 unique adult cardiac surgery patients from Stanford University Medical Center. This dataset represents the external test dataset for our [TEE view classification study.](https://www.nature.com/articles/s41598-023-50735-8) |
| **Name** [INSPECT: A Multimodal Dataset for Pulmonary Embolism Diagnosis and Prognosis](https://aimi.stanford.edu/datasets/inspect-Multimodal-Dataset-for-Pulmonary-Embolism-Diagnosis-and-Prognosis) | **Description** INSPECT contains data from 19,438 patients, including CT images, sections of radiology reports, and structured electronic health record (EHR) data (including demographics, diagnoses, procedures, and vitals). |
| **Name** <br>[LERA - Lower Extremity Radiographs](https://aimi.stanford.edu/datasets/lera-lower-extremity-radiographs) | **Description** <br>182 patients who underwent a radiographic examination at the Stanford  between 2003 and 2014. Includes images of the foot, knee, ankle, or hip associated with each patient. |
| **Name** [MRA-MIDAS: Multimodal Image Dataset for AI-based Skin Cancer](https://aimi.stanford.edu/datasets/mra-midas-Multimodal-Image-Dataset-for-AI-based-Skin-Cancer) | **Description** Melanoma Research Alliance Multimodal Image Dataset for AI-based Skin Cancer (MRA-MIDAS) dataset, the first publicly available, prospectively-recruited, systematically-paired dermoscopic and clinical image-based dataset across a range of skin-lesion diagnoses. |
| **Name** <br>[MRNet: Knee MRIs](https://aimi.stanford.edu/datasets/mrnet-knee-mris) | **Description** <br>1,370 knee MRI exams performed at Stanford. Contains 1,104 (80.6%) abnormal exams, with 319 (23.3%) ACL tears and 508 (37.1%) meniscal tears; labels were obtained through manual extraction from clinical reports. |
| **Name** <br>[MURA: MSK X-rays](https://aimi.stanford.edu/datasets/mura-msk-xrays) | **Description** <br>A large dataset of musculoskeletal radiographs containing 40,561 images from 14,863 studies, where each study is manually labeled by radiologists as either normal or abnormal. |
| **Name** <br>[RadFusion: Multimodal Pulmonary Embolism Dataset](https://aimi.stanford.edu/datasets/radfusion-multimodal-pulmonary-embolism-dataset) | **Description** <br>1794 patients susceptible to pulmonary embolism at Stanford. The dataset consists of chest CT, patient demographics and medical history. |
| **Name** <br>[RadGraph: CheXpert Results](https://aimi.stanford.edu/datasets/radgraph-chexpert-results) | **Description** <br>RadGraph is a dataset of entities and relations in full-text chest X-ray radiology reports based on a novel information extraction schema designed to structure radiology reports. |
| **Name** <br>[SinoCT](https://aimi.stanford.edu/datasets/sinoct) | **Description** <br>9776 head CTs with reconstructed images and a high-quality simulated sinogram, each labeled as normal/abnormal by experienced radiologists at the time of interpretation. Labels for hemorrhage are available. |
| **Name** <br>[SKM-TEA](https://aimi.stanford.edu/datasets/skm-tea-knee-mri) | **Description** <br>Imaging data and annotations for 155 quantitative double echo steady state MRI knee scans acquired clinically at Stanford. The data includes the raw kspace, DICOM images, segmentations of six tissues, and bounding boxes for 16 pathologies. |
| **Name** <br>[Thyroid Ultrasound Cine-clip](https://aimi.stanford.edu/datasets/thyroid-ultrasound-cine-clip) | **Description** <br>167 patients with biopsy-confirmed thyroid nodules (n=192) at Stanford. The dataset consists of ultrasound cine-clip images, radiologist-annotated segmentations, patient demographics, lesion size and location, TI-RADS descriptors, and histopathological diagnoses. |

Back to Top

---

## Source 2: docs.aireadi.org {#source-2}

**URL:** https://docs.aireadi.org/docs/1/dataset/healthsheet
**Content Type:** Web Content
**Content Length:** 62,343 characters
**Scraped At:** 2025-06-29T12:37:49.901Z

**Detected Structure:** Plain Text

### Content:

[Skip to main content](https://docs.aireadi.org/docs/1/dataset/healthsheet#__docusaurus_skipToContent_fallback)

This documentation is for v **1.0.0** of the dataset, which is no longer accessible.

Refer to the documentation for the latest version of the dataset v **[2.0.0](https://docs.aireadi.org/docs/2/dataset/healthsheet)**.

Version: 1.0.0

On this page

## General information [​](https://docs.aireadi.org/docs/1/dataset/healthsheet\#general-information "Direct link to General information")

1. Provide a 2 sentence summary of this dataset.

The Artificial Intelligence Ready and Equitable Atlas for Diabetes Insights (AI-READI) is a dataset consisting of data collected from individuals with and without Type 2 Diabetes Mellitus (T2DM) and harmonized across 3 data collection sites. The composition of the dataset was designed with future studies using AI/Machine Learning in mind. This included recruitment sampling procedures aimed at achieving approximately equal distribution of participants across sex, race, and diabetes severity, as well as the design of a data acquisition protocol across multiple domains (survey data, phyftrainsical measurements, clinical data, imaging data, wearable device data, etc.) to enable downstream AI/ML analyses that may not be feasible with existing data sources such as claims or electronic health records data. The goal is to better understand salutogenesis (the pathway from disease to health) in T2DM. Some data that are not considered to be sensitive personal health data will be available to the public for download upon agreement with a license that defines how the data can be used. The full dataset will be accessible by entering into a data use agreement. The public dataset will include survey data, blood and urine lab results, fitness activity levels, clinical measurements (e.g. monofilament and cognitive function testing), retinal images, ECG, blood sugar levels, and home air quality. The data held under controlled access include 5-digit zip code, sex, race, ethnicity, genetic sequencing data, medications, past health records, and traffic and accident reports. Of note, the overall enrollment goal is to have balanced distribution between different racial groups. As enrollment is ongoing, the pilot data release and periodic updates to data releases may not have achieved balanced distribution across groups.

2. Has the dataset been audited before? If yes, by whom and what are the results?

The dataset has not undergone any formal external audits. However, the dataset has been reviewed internally by AI-READI team members for quality checks and to ensure that no personally identifiable information was accidentally included.


## Dataset versioning [​](https://docs.aireadi.org/docs/1/dataset/healthsheet\#dataset-versioning "Direct link to Dataset versioning")

### Version [​](https://docs.aireadi.org/docs/1/dataset/healthsheet\#version "Direct link to Version")

A dataset will be considered to have a new version if there are major differences from a previous release. Some examples are a change in the number of patients/participants, or an increase in the data modalities covered.

### Sub-versions [​](https://docs.aireadi.org/docs/1/dataset/healthsheet\#sub-versions "Direct link to Sub-versions")

A sub-version tends to apply smaller scale changes to a given version. Some datasets in healthcare are released without labels and predefined tasks, or will be later labeled by researchers for specific tasks and problems, to form sub-versions of the dataset.

The following set of questions clarifies the information about the current (latest) version of the dataset. It is important to report the rationale for labeling the data in any of the versions and sub-versions that this datasheet addresses, funding resources, and motivations behind each released version of the dataset.

1. Does the dataset get released as static versions or is it dynamically updated?

a. If static, how many versions of the dataset exist?

b. If dynamic, how frequently is the dataset updated?

The dataset gets released as static versions. This is the first version of the dataset and consists of data collected during the pilot data collection phase. After that, there are plans to release new versions of the dataset approximately once a year with additional data from participants who have been enrolled since the last dataset version release.

2. Is this datasheet created for the original version of the dataset? If not, which version of the dataset is this datasheet for?

Yes, this datasheet is created for the first version of the dataset.

3. Are there any datasheets created for any versions of this dataset?

No, there are no previous datasheets created, since this is the first version of the dataset.

4. Does the current version/subversion of the dataset come with predefined task(s), labels, and recommended data splits (e.g., for training, development/validation, testing)? If yes, please provide a high-level description of the introduced tasks, data splits, and labeling, and explain the rationale behind them. Please provide the related links and references. If not, is there any resource (website, portal, etc.) to keep track of all defined tasks and/or associated label definitions? (please note that more detailed questions w.r.t labeling is provided in further sections)

See response to question #6 under “Labeling and subjectivity of labeling”.

5. If the dataset has multiple versions, and this datasheet represents one of them, answer the following questions:

a. What are the characteristics that have been changed between different versions of the dataset?

b. Explain the motivation/rationale for creating the current version of the dataset.

c. Does this version have more subjects/patients represented in the data, or fewer?

d. Does this version of the dataset have extended data or new data from the same patients as the older versions? Were any patients, data fields, or data points removed? If so, why?

e. Do we expect more versions of the dataset to be released?

f. Is this datasheet for a version of the dataset? If yes, does this sub-version of the dataset introduce a new task, labeling, and/or recommended data splits? If the answer to any of these questions is yes, explain the rationale behind it.

g. Are you aware of any widespread version(s)/subversion(s) of the dataset? If yes, what is the addressed task, or application that is addressed?

N/A, since this is the first version of the dataset.


## Motivation [​](https://docs.aireadi.org/docs/1/dataset/healthsheet\#motivation "Direct link to Motivation")

Reasons and motivations behind creating the dataset, including but not limited to funding interests.

For any of the following questions, if a healthsheet has already been created for this dataset, then refer to those answers when filling in the below information.

1. For what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description.

The purpose for creating the dataset was to enable future generations of artificial intelligence/machine learning (AI/ML) research to provide critical insights into type 2 diabetes mellitus (T2DM), including salutogenic pathways to return to health. T2DM is a growing public health threat. Yet, the current understanding of T2DM, especially in the context of salutogenesis, is limited. Given the complexity of T2DM, AI-based approaches may help with improving our understanding but a key issue is the lack of data ready for training AI models. The AI-READI dataset is intended to fill this gap.

2. What are the applications that the dataset is meant to address? (e.g., administrative applications, software applications, research)

The multi-modal dataset being collected is being gathered to facilitate downstream pseudotime manifolds and various applications in artificial intelligence.

3. Are there any types of usage or applications that are discouraged from using this dataset? If so, why?

The AI READI dataset License imposes certain restrictions on the usage of the data. The restrictions are described in the License files available at [https://doi.org/10.5281/zenodo.10642459](https://doi.org/10.5281/zenodo.10642459). Briefly, the Licensee shall not: “(i) make clinical treatment decisions based on the Data, as it is intended solely as a research resource, or (ii) use or attempt to use the Data, alone or in concert with other information, to compromise or otherwise infringe the confidentiality of information on an individual person who is the source of any Data or any clinical data or biological sample from which Data has been generated (a 'Data Subject') and their right to privacy, to identify or contact any individual Data Subject or group of Data Subjects, to extract or extrapolate any identifying information about a Data Subject, to establish a particular Data Subject's membership in a particular group of persons, or otherwise to cause harm or injury to any Data Subject.”

4. Who created this dataset (e.g., which team, research group), and on behalf of which entity (e.g., company, institution, organization)?

This dataset was created by members of the AI-READI project, hereby referred to as the AI-READI Consortium. Details about each member and their institutions are available on the project website at [https://aireadi.org](https://aireadi.org/).

5. Who funded the creation of the dataset? If there is an associated grant, please provide the name of the grantor and the grant name and number. If the funding institution differs from the research organization creating and managing the dataset, please state how.

The creation of the dataset was funded by the National Institutes of Health (NIH) through their Bridge2AI Program ( [https://commonfund.nih.gov/bridge2ai](https://commonfund.nih.gov/bridge2ai)). The grant number is OT2ODO32644 and more information about the funding is available at [https://reporter.nih.gov/search/T-mv2dbzIEqp9V6UJjHpgw/project-details/10885481](https://reporter.nih.gov/search/T-mv2dbzIEqp9V6UJjHpgw/project-details/10885481). Note that the funding institution is not creating or managing the dataset. The dataset is created and managed by the awardees of the grant (c.f. answer to the previous question).

6. What is the distribution of backgrounds and experience/expertise of the dataset curators/generators?

There is a wide range of experience within the project team, including senior, mid-career, and early career faculty members as well as clinical research coordinators, staff, and interns. They collectively cover many areas of expertise including clinical research, data collection, data management, data standards, bioinformatics, team science, and ethics, among others. Visit [https://aireadi.org/team](https://aireadi.org/team) for more information.


## Data Composition [​](https://docs.aireadi.org/docs/1/dataset/healthsheet\#data-composition "Direct link to Data Composition")

What is the dataset made of? What are the modalities, and schema involved in creating the preliminary version of the dataset or following versions and subversions?

### Instances [​](https://docs.aireadi.org/docs/1/dataset/healthsheet\#instances "Direct link to Instances")

Refers to the unit of interest. The unit might be different in the datasheet compared to the downstream use case: an instance might relate to a patient in the database, but will be used to provide predictions for specific events for that patient, treating each event as separate.

01. What do the instances that comprise the dataset represent (e.g., documents, images, people, countries)? Are there multiple types of instances? Please provide a description.

    Each instance represents an individual patient.

02. How many instances are there in total (of each type, if appropriate) (breakdown based on schema, provide data stats)?

    There are 204 instances in this current version of the dataset (version 1, released spring 2024).

03. How many patients / subjects does this dataset represent? Answer this for both the preliminary dataset and the current version of the dataset.

    See previous question.

04. Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable). Answer this question for the preliminary version and the current version of the dataset in question.

    The dataset contains all possible instances. More specifically, the dataset contains data from all participants who have been enrolled during the pilot data collection phase for AI-READI.

05. What data modality does each patient data consist of? If the data is hierarchical, provide the modality details for all levels (e.g: text, image, physiological signal). Break down in all levels and specify the modalities and devices.

    Multiple modalities of data are collected for each participant, including survey data, clinical data, retinal imaging data, environmental sensor data, continuous glucose monitor data, and wearable activity monitor data. These encompass tabular data, imaging data, and physiological signal/waveform data. There is no unstructured text data included in this dataset. The exact forms used for data collection in REDCap are available [here](https://docs.aireadi.org/v1/REDCap%20surveys%20and%20forms.pdf). Furthermore, all modalities, file formats, and devices are detailed in the dataset documentation at [https://docs.aireadi.org/](https://docs.aireadi.org/).

06. What data does each instance consist of? “Raw” data (e.g., unprocessed text or images) or features? In either case, please provide a description.

    Each instance consists of all of the data available for an individual participating in the study. See answer to question 5 for the data types associated with each instance.

07. Is any information missing from individual instances? If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable).

    Yes, not all modalities are available for all participants. Some participants elected not to participate in some study elements. In a few cases, the data collection device did not have any stored results or was returned too late to retrieve the results (e.g. battery died, data was lost). In a few cases, there may have been a data collision at some point in the process and data has been lost.

08. Are relationships between individual instances made explicit? (e.g., They are all part of the same clinical trial, or a patient has multiple hospital visits and each visit is one instance)? If so, please describe how these relationships are made explicit.

    Yes - all instances are part of the same prospective data generation project (AI-READI). There is currently only one visit per participant.

09. Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a description. (e.g., losing data due to battery failure, or in survey data subjects skip the question, radiological sources of noise).

    In cases of survey data, skipped questions or incomplete responses are expected. In cases of using wearables, improper use, technical failure such as battery failure or system malfunction are expected. In cases of imaging data, patient uncooperation, noise that may obscure the images and technical failure such as system malfunction, and data transfer failures are expected.

10. Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, other datasets)? If it links to or relies on external resources,

    a. are there guarantees that they will exist, and remain constant, over time;

    b. are there official archival versions of the complete dataset (i.e., including the external resources as they existed at the time the dataset was created);

    c. are there any restrictions (e.g., licenses, fees) associated with any of the external resources that might apply to a future user? Please provide descriptions of all external resources and any restrictions associated with them, as well as links or other access points, as appropriate.

    The dataset is self-contained but does rely on the dataset documentation for users requiring additional information about the provenance of the dataset. The documentation is available at [https://docs.aireadi.org](https://docs.aireadi.org/). The documentation is shared under the CC-BY 4.0 license, so there are no restrictions associated with its use.

11. Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals' non-public communications that is confidential)? If so, please provide a description.

    No, the dataset does not contain data that might be considered confidential. No personally identifiable information is included in the dataset.

12. Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise pose any safety risk (such as psychological safety and anxiety)? If so, please describe why.

    No

13. If the dataset has been de-identified, were any measures taken to avoid the re-identification of individuals? Examples of such measures: removing patients with rare pathologies or shifting time stamps.

    N/A

14. Does the dataset contain data that might be considered sensitive in any way (e.g., data that reveals racial or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)? If so, please provide a description.

    No, the public dataset will not contain data that is considered sensitive. However, the controlled access dataset will contain data regarding racial and ethnic origins, location (5-digit zip code), as well as motor vehicle accident reports.


## Devices and Contextual Attributes in Data Collection [​](https://docs.aireadi.org/docs/1/dataset/healthsheet\#devices-and-contextual-attributes-in-data-collection "Direct link to Devices and Contextual Attributes in Data Collection")

1. For data that requires a device or equipment for collection or the context of the experiment, answer the following additional questions or provide relevant information based on the device or context that is used (for example)

a. If there was an MRI machine used, what is the MRI machine and model used?

b. If heart rate was measured what is the device for heart rate variation that is used?

c. If cortisol measurement is reported at multi site, provide details,

d. If smartphones were used to collect the data, provide the names of models.

e. And so on,..

The devices included in the study are as follows, and more details can be found at [https://docs.aireadi.org](https://docs.aireadi.org/):

**Environmental sensor device**

Participants will be sent home with an environmental sensor (a custom-designed sensor unit called the LeeLab Anura), which they will use for 10 continuous days before returning the devices to the clinical research coordinators for data download.

**Continuous glucose monitor (Dexcom G6)**

The Dexcom G6 is a real-time, integrated continuous glucose monitoring system (iCGM) that directly monitors blood glucose levels without requiring finger sticks. It must be worn continuously in order to collect data.

**Wearable accelerometer (Physical activity monitor)**

The Garmin Vivosmart 5 Fitness Activity tracker will be used to measure data related to physical activity.

**Heart rate**

Heart rate can be read from EKG or blood pressure measurement devices.

**Blood pressure**

Blood pressure devices used for the study across the various data acquisition sites are: OMRON HEM 907XL Blood Pressure Monitor, Medline MDS4001 Automatic Digital Blood Pressure Monitor, and Welch Allyn 6000 series Vital signs monitor with Welch Allyn FlexiPort Reusable Blood Pressure Cuff.

**Visual acuity**

M&S Technologies EVA device to test visual acuity. The test is administered at a distance of 4 meters from a touch-screen monitor that is 12x20 inches. Participants will read letters from the screen. Photopic Conditions: No neutral density filters are used. A general occluder will be used for photopic testing. The participant wears their own prescription spectacles or trial frames. For Mesopic conditions, a neutral density (ND) filter will be used. The ND filter will either be a lens added to trial frames to reduce incoming light on the tested eye, OR a handheld occluder with a neutral density
filter (which we will designate as “ND-occluder) over the glasses will be used. The ND-occluder is different from a standard occluder and is used only for vision testing under mesopic conditions.

**Contrast sensitivity**

The MARS Letter Contrast Sensitivity test (Perceptrix) was conducted monocularly under both Photopic conditions (with a general occluder) and Mesopic conditions (using a Neutral Density occluder with a low luminance filter lens). The standardized order of MARS cards was as follows: Photopic OD, Photopic OS, Mesopic OD, and Mesopic OS. The background luminance of the charts fell within the range of 60 to 120 cd/m2, with an optimal level of 85 cd/m2. Illuminance was recommended to be between 189 to 377 lux, with an optimal level of 267 lux. While the designed viewing distance was 50 cm, it could vary between 40 to 59 cm. Patients were required to wear their appropriate near correction: reading glasses or trial frames with +2.00D lenses. All testing was carried out under undilated conditions. Patients were instructed to read the letter left to right across each line on the chart. Patients were encouraged to guess, even if they perceived the letters as too faint. Testing was terminated either when the patient made two consecutive errors or reached the end of the chart. The log contrast sensitivity (log CS) values were recorded by multiplying the number of errors prior to the final correct letter by 0.04 and subtracting the result from the log CS value at the final correct letter. If a patient reached the end of the chart without making two consecutive errors, the final correct letter was simply the last one correctly identified.

**Autorefraction**

KR 800 Auto Keratometer/Refractor.

**EKG**

Philips (manufacturer of Pagewriter TC30 Cardiograph)

**Lensometer**

Lensometer devices used at data acquisition sites across the study include: NIDEK LM-600P Auto Lensometer, Topcon-CL-200 computerized Lensometer, and Topcon-CL-300 computerized Lensometer

**Undilated fundus photography - Optomed Aurora**

The Optomed Aurora IQ is a handheld fundus camera that can take non-mydriatic images of the ocular fundus. It has a 50° field of view, 5 Mpix sensor, and high-contrast optical design. The camera is non-mydriatic, meaning it doesn't require the pupil to be dilated, so it can be used for detailed viewing of the retina. Images taken during the AI-READI visit, are undilated images taken in a dark room while a patient is sitting on a comfortable chair, laying back. As it becomes challenging to get a good view because of the patients not being dilated and the handheld nature of this imaging modality, the quality of the images vary from patient to patient and within the same patient.

**Dilated fundus photography - Eidon**

The iCare EIDON is a widefield TrueColor confocal fundus imaging system that can capture images up to 200°. It comes with multiple imaging modalities, including TrueColor, blue, red, Red-Free, and infrared confocal images. The system offers widefield, ultra-high-resolution imaging and the capability to image through cataract and media opacities. It operates without dilation (minimum pupil 2.5 mm) and provides the flexibility of both fully automated and fully manual modes. Additionally, the iCare EIDON features an all-in-one compact design, eliminating the need for an additional PC. AI READI images using EIDON include two main modalities: 1. Single Field Central IR/FAF 2. Smart Horizontal Mosaic. Imaging is done in fully automated mode in a dark room with the machine moving and positioning according to the patient's head aiming at optimizing the view and minimizing operator's involvement/operator induced noise.

**Spectralis HRA (Heidelberg Engineering)**

The Heidelberg Spectralis HRA+OCT is an ophthalmic imaging system that combines optical coherence tomography (OCT) with retinal angiography. It is a modular, upgradable platform that allows clinicians to configure it for their specific diagnostic workflow. It has the confocal scanning laser ophthalmoscope (cSLO) technology that not only offers documentation of clinical findings but also often highlights critical diagnostic details that are not visible on traditional clinical ophthalmoscopy. Since cSLO imaging minimizes the effects of light scatter, it can be used effectively even in patients with cataracts. For AI READI subjects, imaging is done in a dark room using the following modalities: ONH-RC, PPole-H, and OCTA of the macula. As the machine is operated by the imaging team and is not fully automated, quality issues may arise, which may lead to skipping this modality and missing data.

**Triton DRI OCT (Topcon Healthcare)**

The DRI OCT Triton is a device from Topcon Healthcare that combines swept-source OCT technology with multimodal fundus imaging. The DRI OCT Triton uses swept-source technology to visualize the deepest layers of the eye, including through cataracts. It also enhances visualization of outer retinal structures and deep pathologies. The DRI OCT Triton has a 1,050 nm wavelength light source and a non-mydriatic color fundus camera. AI READI imaging is done in a dark room with minimal intervention from the imager as the machine positioning is done automatically. This leads to higher quality images with minimal operator induced error. Imaging is done in 12.0X12.0 mm and 6.0X6.0 mm OCTA, and 12.0 mm X9.0 mmX6.0 mm 3D Horizontal and Radial scan modes.

**Maestro2 3D OCT (Topcon Healthcare)**

The Maestro2 is a robotic OCT and color fundus camera system from Topcon Healthcare. It can capture a 12 mm x 9 mm wide-field OCT scan that includes the macula and optic disc. The Maestro2 can also capture high-resolution non-mydriatic, true color fundus photography, OCT, and OCTA with a single button press. Imaging is done in a dark room and automatically with minimal involvement of the operator. Protocols include 12.0 mm X9.0 mm widefield, 6.0 mm X 6.0 mm 3D macula scan and 6.0 mm X 6.0 mm OCTA (scan rate: 50 kHz).

**FLIO (Heidelberg Engineering)**

Fluorescence Lifetime Imaging Ophthalmoscopy (FLIO) is an advanced imaging technique used in ophthalmology. It is a non-invasive method that provides valuable information about the metabolic and functional status of the retina. FLIO is based on the measurement of fluorescence lifetimes, which is the duration a fluorophore remains in its excited state before emitting a photon and returning to the ground state. FLIO utilizes this fluorescence lifetime information to capture and analyze the metabolic processes occurring in the retina. Different retinal structures and molecules exhibit distinct fluorescence lifetimes, allowing for the visualization of metabolic changes, cellular activity, and the identification of specific biomolecules. The imaging is done by an operator in a dark room analogous to a straightforward heidelberg spectralis OCT. However, as it takes longer than a usual spectralis OCT and exposes patients to uncomfortable levels of light, it is kept to be performed as the last modality of an AI READI visit. Because of this patients may not be at their best possible compliance.

**Cirrus 5000 Angioplex (Carl Zeiss Meditec)**

The Zeiss Cirrus 5000 Angioplex is a high-definition optical coherence tomography (OCT) system that offers non-invasive imaging of retinal microvasculature. The imaging is done in a dark room by an operator and it is pretty straightforward and analogous to what is done in the ophthalmology clinics on a day to day basis. Imaging protocols include 512 X 512 and 200 X 200 macula and ONH scans and also OCTA of the macula. Zeiss Cirrus 5000 also provides a 60-degree OCTA widefield view. 8x8mm single scans and 14x14mm automated OCTA montage allow for rapid peripheral assessment of the retina as well.

**Monofilament testing for peripheral neuropathy**

Monofilament test is a standard clinical test to monitor peripheral neuropathy in diabetic patients. It is done using a standard 10g monofilament applying pressure to different points on the plantar surface of the feet. If patients sense the monofilament, they confirm by saying “yes”; if patients do not sense the monofilament after it bends, they are considered to be insensate. When the sequence is completed, the insensate area is retested for confirmation. This sequence is further repeated randomly at each of the testing sites on each foot until results are obtained.The results are recorded on an iPad, Laptop, or a paper questionnaire and are directly added to the project's RedCap by the clinical research staff.

**Montreal Cognitive Assessment (MoCA)**

The Montreal Cognitive Assessment (MoCA) is a simple, in-office screening tool that helps detect mild cognitive impairment and early onset of dementia. The MoCA evaluates cognitive domains such as: Memory, Executive functioning, Attention, Language, Visuospatial, Orientation, Visuoconstructional skills, Conceptual thinking, Calculations. The MoCA generates a total score and six domain-specific index scores. The maximum score is 30, and anything below 24 is a sign of cognitive impairment. A final total score of 26 and above is considered normal. Some disadvantages of the MoCA include: Professionals require training to score the test, A person's level of education may affect the test, Socioeconomic factors may affect the test, People living with depression or other mental health issues may score similarly to those with mild dementia. AI READI research staff perform this test on an iPad using a pre-installed software (MoCA Duo app downloaded from the app store) that captures all the patients responses in an interactive manner.


## Challenge in tests and confounding factors [​](https://docs.aireadi.org/docs/1/dataset/healthsheet\#challenge-in-tests-and-confounding-factors "Direct link to Challenge in tests and confounding factors")

1. Which factors in the data might limit the generalization of potentially derived models? Is this information available as auxiliary labels for challenge tests? For instance:

a. Number and diversity of devices included in the dataset.

b. Data recording specificities, e.g., the view for a chest x-ray image.

c. Number and diversity of recording sites included in the dataset.

d. Distribution shifts over time.

While the AI-READI's cross-sectional database ultimately aims to achieve balance across race/ethnicity, biological sex, and diabetes presence and severity, the pilot study is not balanced across these parameters.

Three recording sites were strategically selected to achieve diverse recruitment: the University of Alabama at Birmingham (UAB), the University of California San Diego (UCSD), and the University of Washington (UW). The sites were chosen for geographic diversity across the United States and to ensure diverse representation across various racial and ethnic groups. Individuals from all demographic backgrounds were recruited at all 3 sites.

Factors influencing the generalization of derived models include the predominantly urban and hospital-based recruitment, which may not fully capture diverse cultural and socioeconomic backgrounds. The study cohort may not provide a comprehensive representation of the population, as it does not include other races/ethnicities such as Pacific Islanders and Native Americans.

Information on device make and model, including specific modalities like macula scans or wide scans during OCT, were documented to ensure repeatability. Moreover, the study included multiple devices for one measure to enhance generalizability and represent the diverse range of equipment utilized in clinical settings.

2. What confounding factors might be present in the data?

a. Interactions between demographic or historically marginalized groups and data recordings, e.g., were women patients recorded in one site, and men in another?

b. Interactions between the labels and data recordings, e.g. were healthy patients recorded on one device and diseased patients on another?

Uniform data collection protocols were implemented for all subjects, irrespective of their race/ethnicity, biological sex, or diabetes severity, across all study sites. The selection of study sites was intended to ensure equitable representation and minimize the potential for sampling bias.


## Collection and use of demographic information [​](https://docs.aireadi.org/docs/1/dataset/healthsheet\#collection-and-use-of-demographic-information "Direct link to Collection and use of demographic information")

1. Does the dataset identify any demographic sub-populations (e.g., by age, gender, sex, ethnicity)?

No

2. If no,

a. Is there any regulation that prevents demographic data collection in your study (for example, the country that the data is collected in)?

No

b. Are you employing methods to reduce the disparity of error rate between different demographic subgroups when demographic labels are unavailable? Please describe.

We are suggesting a split for training/validation/testing models that is aimed at reducing disparities in models developed using this dataset.


## Pre-processing / de-identification [​](https://docs.aireadi.org/docs/1/dataset/healthsheet\#pre-processing--de-identification "Direct link to Pre-processing / de-identification")

1. Was there any pre-processing for the de-identification of the patients? Provide the answer for the preliminary and the current version of the dataset

N/A

2. Was there any pre-processing for cleaning the data? Provide the answer for the preliminary and the current version of the dataset

There were several quality control measures used at the time of data entry/acquisition. For example, clinical data outside of expected min/max ranges were flagged in REDCap, which was visible in reports viewed by clinical research coordinators (CRCs) and Data Managers. Using these REDCap reports as guides, Data Managers and CRCs examined participant records and determined if an error was likely. Data were checked for the following and edited if errors were detected:


1. Credibility, based on range checks to determine if all responses fall within a prespecified reasonable range

2. Incorrect flow through prescribed skip patterns

3. Missing data that can be directly filed from other portions of an individual’ s record

4. The omission and/or duplication of records


Editing was only done under the guidance and approval of the site PI. If corrected data was available from elsewhere in the respondent’s answers, the error was corrected. If there was no logical or appropriate way to correct the data, the Data site PI reviewed the values and made decisions about whether those values should be removed from the data.

Once data were sent from each of the study sites to the central project team, additional processing steps were conducted in preparation for dissemination. For example, all data were mapped to standardized terminologies when possible, such as the Observational Medical Outcomes Partnership (OMOP) Common Data Model, a common data model for observational health data, and the Digital Imaging and Communications in Medicine (DICOM), a commonly used standard for medical imaging data. Details about the data processing approaches for each data domain/modality are described in the dataset documentation at [https://docs.aireadi.org](https://docs.aireadi.org/).

3. Was the “raw” data (post de-identification) saved in addition to the preprocessed/cleaned data (e.g., to support unanticipated future uses)? If so, please provide a link or other access point to the “raw” data.

The raw data is saved and expected to be preserved by the AI-READI project at least for the duration of the project but is not anticipated to be shared outside the project team right now, because it has not been mapped to standardized terminologies and because the raw data may accidentally include personal health information or personally identifiable information (e.g. in free text fields). There is a possibility that raw data may be included in future releases of the controlled access dataset.

4. Were instances excluded from the dataset at the time of preprocessing? If so, why? For example, instances related to patients under 18 might be discarded.

No data were excluded from the dataset at the time of preprocessing. However, regarding to study recruitment (i.e. ability to participate in the study), the following eligibility criteria were used:

Inclusion Criteria:


   - Able to provide consent
   - ≥ 40 years old
   - Persons with or without type 2 diabetes
   - Must speak and read English

Exclusion Criteria:
   - Must not be pregnant
   - Must not have gestational diabetes
   - Must not have Type 1 diabetes
5. If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)? Answer this question for both the preliminary dataset and the current version of the dataset

N/A


## Labeling and subjectivity of labeling [​](https://docs.aireadi.org/docs/1/dataset/healthsheet\#labeling-and-subjectivity-of-labeling "Direct link to Labeling and subjectivity of labeling")

### Labeling [​](https://docs.aireadi.org/docs/1/dataset/healthsheet\#labeling "Direct link to Labeling")

In medical domains, researchers usually take a dataset and appropriate it for a defined task. researchers may have their own guidance. It is important to know what the incentive of the original creators was, if there was a guideline or there is a guideline for the current version or sub-versions of the dataset?

01. Is there an explicit label or target associated with each data instance? Please respond for both the preliminary dataset and the current version.

    a. If yes:


    1. What are the labels provided?

    2. Who performed the labeling? For example, was the labeling done by a clinician, ML researcher, university or hospital?


b. What labeling strategy was used?

    1. Gold standard label available in the data (e.g. cancers validated by biopsies)

    2. Proxy label computed from available data:
       1. Which label definition was used? (e.g. Acute Kidney Injury has multiple definitions)

       2. Which tables and features were considered to compute the label?
    3. Which proportion of the data has gold standard labels?


c. Human-labeled data

    1. How many labellers were considered?

    2. What is the demographic of the labellers? (countries of residence, of origin, number of years of experience, age, gender, race, ethnicity, …)

    3. What guidelines did they follow?

    4. How many labellers provide a label per instance?

       If multiple labellers per instance:
       1. What is the rater agreement? How was disagreement handled?
       2. Are all labels provided, or summaries (e.g. maximum vote)?
    5. Is there any subjective source of information that may lead to inconsistencies in the responses? (e.g: multiple people answering a survey having different interpretation of scales, multiple clinicians using scores, or notes)

    6. On average, how much time was required to annotate each instance?

    7. Were the raters compensated for their time? If so, by whom and what amount? What was the compensation strategy (e.g. fixed number of cases, compensated per hour, per cases per hour)?


No specific labeling was performed in the dataset, as the dataset is a hypothesis-agnostic dataset aimed at facilitating multiple potential downstream AI/ML applications.

02. What are the human level performances in the applications that the dataset is supposed to address?

    N/A

03. Is the software used to preprocess/clean/label the instances available? If so, please provide a link or other access point.

    N/A – no labeling was performed

04. Is there any guideline that the future researchers are recommended to follow when creating new labels / defining new tasks?

    No, we do not have formal guidelines in place.

05. Are there recommended data splits (e.g., training, development/validation, testing)? Are there units of data to consider, whatever the task? If so, please provide a description of these splits, explaining the rationale behind them. Please provide the answer for both the preliminary dataset and the current version or any sub-version that is widely used.

    The current version of the dataset comes with recommended data splits. Because sex, race, and ethnicity data are not being released with the public version of the dataset, the project team has prepared data splits into proportions (70%/15%/15%) that can be used for subsequent training/validation/testing where the validation and test sets are balanced for sex, race/ethnicity and diabetes status (with and without diabetes).

06. Is there an explicit label or target associated with each data instance? Please respond for both the preliminary dataset and the current version.

    a. If yes:


    1. What are the labels provided?

    2. Who performed the labeling? For example, was the labeling done by a clinician, ML researcher, university or hospital?


b. What labeling strategy was used?

    1. Gold standard label available in the data (e.g. cancers validated by biopsies)

    2. Proxy label computed from available data:
       1. Which label definition was used? (e.g. Acute Kidney Injury has multiple definitions)

       2. Which tables and features were considered to compute the label?
    3. Which proportion of the data has gold standard labels?


c. Human-labeled data

    1. How many labellers were considered?

    2. What is the demographic of the labellers? (countries of residence, of origin, number of years of experience, age, gender, race, ethnicity, …)

    3. What guidelines did they follow?

    4. How many labellers provide a label per instance?

       If multiple labellers per instance:
       1. What is the rater agreement? How was disagreement handled?
       2. Are all labels provided, or summaries (e.g. maximum vote)?
    5. Is there any subjective source of information that may lead to inconsistencies in the responses? (e.g: multiple people answering a survey having different interpretation of scales, multiple clinicians using scores, or notes)

    6. On average, how much time was required to annotate each instance?

    7. Were the raters compensated for their time? If so, by whom and what amount? What was the compensation strategy (e.g. fixed number of cases, compensated per hour, per cases per hour)?


No specific labeling was performed in the dataset, as the dataset is a hypothesis-agnostic dataset aimed at facilitating multiple potential downstream AI/ML applications.

07. What are the human level performances in the applications that the dataset is supposed to address?
    N/A

08. Is the software used to preprocess/clean/label the instances available? If so, please provide a link or other access point.
    N/A – no labeling was performed

09. Is there any guideline that the future researchers are recommended to follow when creating new labels / defining new tasks?
    No, we do not have formal guidelines in place.

10. Are there recommended data splits (e.g., training, development/validation, testing)? Are there units of data to consider, whatever the task? If so, please provide a description of these splits, explaining the rationale behind them. Please provide the answer for both the preliminary dataset and the current version or any sub-version that is widely used.

    The current version of the dataset comes with recommended data splits. Because sex, race, and ethnicity data are not being released with the public version of the dataset, the project team has prepared data splits into proportions (70%/15%/15%) that can be used for subsequent training/validation/testing where the validation and test sets are balanced for sex, race/ethnicity and diabetes status (with and without diabetes).


## Collection Process [​](https://docs.aireadi.org/docs/1/dataset/healthsheet\#collection-process "Direct link to Collection Process")

01. Were any REB/IRB approval (e.g., by an institutional review board or research ethics board) received? If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation.

    The initial IRB approval at the University of Washington was received on December 20, 2022. The initial approval letter can be found [here](https://docs.aireadi.org/v1/Approval_STUDY00016228_Lee_initial.pdf). Under FWA #00006878, the IRB approved activity for the AI-READI study from 12/16/2022 to 12/15/2023. A modification to the initial IRB application was filed on 5/5/2023 and approved on 5/10/2023. An annual renewal application to the IRB about the status and progress of the study is required and due within 90 days of expiration.

02. How was the data associated with each instance acquired? Was the data directly observable (e.g., medical images, labs or vitals), reported by subjects (e.g., survey responses, pain levels, itching/burning sensations), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how.

    The acquisition of data varied based on the domain; some data were directly observable (such as labs, vitals, and retinal imaging), whereas other data were reported by subjects (e.g. survey responses). Verification of data entry was performed when possible (e.g. cross-referencing entered medications with medications that were physically brought in or photographed by each study participant). Details for each data domain are available in [https://docs.aireadi.org](https://docs.aireadi.org/).

03. What mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program, software API)? How were these mechanisms or procedures validated? Provide the answer for all modalities and collected data. Has this information been changed through the process? If so, explain why.

    The procedures for data collection and processing is available at [https://docs.aireadi.org](https://docs.aireadi.org/).

04. Who was involved in the data collection process (e.g., patients, clinicians, doctors, ML researchers, hospital staff, vendors, etc.) and how were they compensated (e.g., how much were contributors paid)?

    Details about the AI-READI team members involved in the data collection process are available at [https://aireadi.org/team](https://aireadi.org/team). Their effort was supported by the National Institutes of Health award OT2OD032644 based on the percentage of effort contributed, and salaries which aligned with the funding guidelines at each site. Study subjects received a compensation of $200 for the study visit also through the grant funding.

05. Over what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created.

    The timeline for the overall project spans four years, encompassing one year dedicated to protocol development and training, and years 2-4 allocated for subject recruitment and data collection. Approximately 4% of participants are expected to undergo a follow-up examination in Year 4. The data collection process is specifically tailored to enable downstream pseudotime manifold analysis—an approach used to predict disease trajectories. This involves gathering and learning from complex, multimodal data from participants exhibiting varying disease severity, ranging from normal to insulin-dependent Type 2 Diabetes Mellitus (T2DM). The timeframe also allows for the collection of the highest number of subjects possible to ensure a balanced representation of racial and ethnic groups and mitigate biases in computer vision algorithms.

    For this version of the dataset, the timeframe for data collection was July 18, 2023 to November 30, 2023.

06. Does the dataset relate to people? If not, you may skip the remaining questions in this section.

    Yes

07. Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., hospitals, app company)?

    The data was collected directly from participants across the three recruiting sites. Recruitment pools were identified by screening Electronic Health Records (EHR) for diabetes and prediabetes ICD-10 codes for all patients who have had an encounter with the sites' health systems within the past 2 years.

08. Were the individuals in question notified about the data collection? If so, please describe (or show with screenshots or other information) how notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of the notification itself.

    Yes, each individual was aware of the data collection, as this was not passive data collection or secondary use of existing data, but rather active data collection directly from participants.

09. Did the individuals in question consent to the collection and use of their data? If so, please describe (or show with screenshots or other information) how consent was requested and provided, and provide a link or other access point to, or otherwise reproduce, the exact language to which the individuals consented.

    Informed consent to participate was required before participation in any part of the protocol (including questionnaires). Potential participants were given the option to read all consent documentation electronically (e-consent) before their visit and give their consent with an electronic signature without verbal communication with a clinical research coordinator. Participants may access e-consent documentation in REDCap and decide at that point they do not want to participate or would like additional information. The approved consent form for the principal project site University of Washington is available [here](https://docs.aireadi.org/v1/AI-READI%20Consent_Form_Standard_v2.0_Mod2%2017May2023_useforpilot.pdf). The other clinical sites had IRB reliance and used the same consent form, with minor institution-specific language incorporated depending on individual institutional requirements.

10. If consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses? If so, please provide a description, as well as a link or other access point to the mechanism (if appropriate).

    Participants were permitted to withdraw consent at any time and cease study participation. However, any data that had been shared or used up to that point would stay in the dataset. This is clearly communicated in the consent document.

11. In which countries was the data collected?

    USA

12. Has an analysis of the potential impact of the dataset and its use on data subjects (e.g., a [data protection impact analysis](https://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protection-regulation-gdpr/accountability-and-governance/data-protection-impact-assessments/)) been conducted? If so, please provide a description of this analysis, including the outcomes, as well as a link or other access point to any supporting documentation.

    No, a data protection impact analysis has not been conducted.


## Inclusion Criteria-Accessibility in data collection [​](https://docs.aireadi.org/docs/1/dataset/healthsheet\#inclusion-criteria-accessibility-in-data-collection "Direct link to Inclusion Criteria-Accessibility in data collection")

1. Is there any language-based communication with patients (e.g: English, French)? If yes, describe the choices of language(s) for communication. (for example, if there is an app used for communication, what are the language options?)

English language was used for communication with study participants.

2. What are the accessibility measurements and what aspects were considered when the study was designed and implemented?

Accessibility measurements were not specifically assessed. However, transportation assistance (rideshare services) was offered to study participants who endorsed barriers to transporting themselves to study visits.

3. If data is part of a clinical study, what are the inclusion criteria?

The eligibility criteria for the study were as follows:

Inclusion Criteria:


   - Able to provide consent
   - ≥ 40 years old
   - Persons with or without type 2 diabetes
   - Must speak and read English

Exclusion Criteria:
   - Must not be pregnant
   - Must not have gestational diabetes
   - Must not have Type 1 diabetes

## Uses [​](https://docs.aireadi.org/docs/1/dataset/healthsheet\#uses "Direct link to Uses")

1. Has the dataset been used for any tasks already? If so, please provide a description.

No

2. Does using the dataset require the citation of the paper or any other forms of acknowledgement? If yes, is it easily accessible through google scholar or other repositories

Yes, use of the dataset requires citation to the resources specified in [https://docs.aireadi.org](https://docs.aireadi.org/).

3. Is there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point. (besides Google scholar)

No

4. Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? For example, is there anything that a future user might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping, quality of service issues) or other undesirable harms (e.g., financial harms, legal risks) If so, please provide a description. Is there anything a future user could do to mitigate these undesirable harms?

No, to the extent of our knowledge, we do not currently anticipate any uses of the dataset that could result in unfair treatment or harm. However, there is a theoretical risk of future re-identification.

5. Are there tasks for which the dataset should not be used? If so, please provide a description. (for example, dataset creators could recommend against using the dataset for considering immigration cases, as part of insurance policies)

This is answered in a prior question (see details regarding license terms).


## Dataset Distribution [​](https://docs.aireadi.org/docs/1/dataset/healthsheet\#dataset-distribution "Direct link to Dataset Distribution")

1. Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created? If so, please provide a description.

The dataset will be distributed and be available for public use.

2. How will the dataset be distributed (e.g., tarball on website, API, GitHub)? Does the dataset have a digital object identifier (DOI)?

The dataset will be available through the FAIRhub platform ( [http://fairhub.io/](http://fairhub.io/)). The dataset’s DOI is [https://doi.org/10.60775/fairhub.1](https://doi.org/10.60775/fairhub.1)

3. When was/will the dataset be distributed?

The dataset was distributed in April 2024.

4. Assuming the dataset is available, will it be/is the dataset distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? If so, please describe this license and/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions.

We provide here the license file containing the terms for reusing the AI-READI dataset ( [https://doi.org/10.5281/zenodo.10642459](https://doi.org/10.5281/zenodo.10642459)). These license terms were specifically tailored to enable reuse of the AI-READI dataset (and other clinical datasets) for commercial or research purpose while putting strong requirements around data usage, security, and secondary sharing to protect study participants, especially when data is reused for artificial intelligence (AI) and machine learning (ML) related applications.

5. Have any third parties imposed IP-based or other restrictions on the data associated with the instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these restrictions.

Refer to license ( [https://doi.org/10.5281/zenodo.10642459](https://doi.org/10.5281/zenodo.10642459))

6. Do any export controls or other regulatory restrictions apply to the dataset or to individual instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation.

Refer to license ( [https://doi.org/10.5281/zenodo.10642459](https://doi.org/10.5281/zenodo.10642459))


## Maintenance [​](https://docs.aireadi.org/docs/1/dataset/healthsheet\#maintenance "Direct link to Maintenance")

1. Who will be supporting/hosting/maintaining the dataset?

The AI-READI team will be supporting and maintaining the dataset. The dataset is hosted on FAIRhub through Microsoft Azure.

2. How can the owner/curator/manager of the dataset be contacted (e.g., email address, forms, etc.)?

We refer to the README file included with the dataset for contact information.

3. Is there an erratum? If so, please provide a link or other access point.

N/A

4. Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)? If so, please describe how often, by whom, and how updates will be communicated to users (e.g., mailing list, GitHub)?

The dataset will not be updated. Rather, new versions of the dataset will be released with additional instances as more study participants complete the study visit.

5. If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were individuals in question told that their data would be retained for a fixed period of time and then deleted)? If so, please describe these limits and explain how they will be enforced.

There are no limits on the retention of the data associated with the instances.

6. Will older versions of the dataset continue to be supported/hosted/maintained? If so, please describe how and for how long. If not, please describe how its obsolescence will be communicated to users.

N/A - This is the first version of the dataset. In the future, when there are newer versions released, the previous versions will continue to be available on FAIRhub.

7. If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so? If so, please provide a description. Will these contributions be validated/verified? If so, please describe how. If not, why not? Is there a process for communicating/distributing these contributions to other users? If so, please provide a description.

No, currently there is no mechanism for others to extend or augment the AI-READI dataset outside of those who are involved in the project.


- [General information](https://docs.aireadi.org/docs/1/dataset/healthsheet#general-information)
- [Dataset versioning](https://docs.aireadi.org/docs/1/dataset/healthsheet#dataset-versioning)
  - [Version](https://docs.aireadi.org/docs/1/dataset/healthsheet#version)
  - [Sub-versions](https://docs.aireadi.org/docs/1/dataset/healthsheet#sub-versions)
- [Motivation](https://docs.aireadi.org/docs/1/dataset/healthsheet#motivation)
- [Data Composition](https://docs.aireadi.org/docs/1/dataset/healthsheet#data-composition)
  - [Instances](https://docs.aireadi.org/docs/1/dataset/healthsheet#instances)
- [Devices and Contextual Attributes in Data Collection](https://docs.aireadi.org/docs/1/dataset/healthsheet#devices-and-contextual-attributes-in-data-collection)
- [Challenge in tests and confounding factors](https://docs.aireadi.org/docs/1/dataset/healthsheet#challenge-in-tests-and-confounding-factors)
- [Collection and use of demographic information](https://docs.aireadi.org/docs/1/dataset/healthsheet#collection-and-use-of-demographic-information)
- [Pre-processing / de-identification](https://docs.aireadi.org/docs/1/dataset/healthsheet#pre-processing--de-identification)
- [Labeling and subjectivity of labeling](https://docs.aireadi.org/docs/1/dataset/healthsheet#labeling-and-subjectivity-of-labeling)
  - [Labeling](https://docs.aireadi.org/docs/1/dataset/healthsheet#labeling)
- [Collection Process](https://docs.aireadi.org/docs/1/dataset/healthsheet#collection-process)
- [Inclusion Criteria-Accessibility in data collection](https://docs.aireadi.org/docs/1/dataset/healthsheet#inclusion-criteria-accessibility-in-data-collection)
- [Uses](https://docs.aireadi.org/docs/1/dataset/healthsheet#uses)
- [Dataset Distribution](https://docs.aireadi.org/docs/1/dataset/healthsheet#dataset-distribution)
- [Maintenance](https://docs.aireadi.org/docs/1/dataset/healthsheet#maintenance)

---

## Source 3: www.nature.com {#source-3}

**URL:** https://www.nature.com/articles/s41597-023-02460-0
**Content Type:** Web Content
**Content Length:** 61,678 characters
**Scraped At:** 2025-06-29T12:37:49.902Z

**Detected Structure:** Plain Text

### Content:

[Skip to main content](https://www.nature.com/articles/s41597-023-02460-0#content)

Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain
the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in
Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles
and JavaScript.

A Real-world Dataset and Benchmark For Foundation Model Adaptation in Medical Image Classification


[Download PDF](https://www.nature.com/articles/s41597-023-02460-0.pdf)

[Download PDF](https://www.nature.com/articles/s41597-023-02460-0.pdf)

## Abstract

Foundation models, often pre-trained with large-scale data, have achieved paramount success in jump-starting various vision and language applications. Recent advances further enable adapting foundation models in downstream tasks efficiently using only a few training samples, e.g., in-context learning. Yet, the application of such learning paradigms in medical image analysis remains scarce due to the shortage of publicly accessible data and benchmarks. In this paper, we aim at approaches adapting the foundation models for medical image classification and present a novel dataset and benchmark for the evaluation, i.e., examining the overall performance of accommodating the large-scale foundation models downstream on a set of diverse real-world clinical tasks. We collect five sets of medical imaging data from multiple institutes targeting a variety of real-world clinical tasks (22,349 images in total), i.e., thoracic diseases screening in X-rays, pathological lesion tissue screening, lesion detection in endoscopy images, neonatal jaundice evaluation, and diabetic retinopathy grading. Results of multiple baseline methods are demonstrated using the proposed dataset from both accuracy and cost-effective perspectives.

### Similar content being viewed by others

![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41551-025-01365-0/MediaObjects/41551_2025_1365_Fig1_HTML.png)

### [A data-efficient strategy for building high-performing medical foundation models](https://www.nature.com/articles/s41551-025-01365-0?fromPaywallRec=false)

Article05 March 2025

![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs42256-024-00807-9/MediaObjects/42256_2024_807_Fig1_HTML.png)

### [Foundation model for cancer imaging biomarkers](https://www.nature.com/articles/s42256-024-00807-9?fromPaywallRec=false)

ArticleOpen access15 March 2024

![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41551-024-01283-7/MediaObjects/41551_2024_1283_Fig1_HTML.png)

### [A foundation model for enhancing magnetic resonance images and downstream segmentation, registration and diagnostic tasks](https://www.nature.com/articles/s41551-024-01283-7?fromPaywallRec=false)

Article05 December 2024

## Background & Summary

In the new trend of training even larger and universal foundation models (e.g., Vision Transformers[1](https://www.nature.com/articles/s41597-023-02460-0#ref-CR1 "Dosovitskiy, A. et al. An image is worth 16 × 16 words: Transformers for image recognition at scale. In International Conference on Learning Representations (2021)."), GPTs[2](https://www.nature.com/articles/s41597-023-02460-0#ref-CR2 "Radford, A., et al. Improving language understanding by generative pre-training. OpenAI (2018)."), PubmedBERT[3](https://www.nature.com/articles/s41597-023-02460-0#ref-CR3 "Gu, Y. et al. Domain-specific language model pretraining for biomedical natural language processing. ACM Transactions on Comput. for Healthc. (HEALTH) 3, 1–23 (2020)."), and CLIP[4](https://www.nature.com/articles/s41597-023-02460-0#ref-CR4 "Radford, A. et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, 8748–8763 (2021).")) using thousands of millions of data samples (sometimes in multiple modalities), developing cost-effective model adaptation methods for detailed applications become the new gold, especially when it only demands very few data samples. On the other side, the shortage of publicly accessible datasets in medical imaging has largely blocked the development and application of large-scale deep learning models (training from scratch) in many clinical downstream tasks. It is because obtaining quality annotations remains a tedious task for medical professionals, e.g., hand-label volumetric data repeatedly. Providing a few textbook sample cases is more logically feasible and complies with the training process of medical residents. In the domain of medical image analysis, it is even more valuable to promote such learning paradigms when diseased cases are often rare in comparison to the numerous amount of normal population.

The common fine-tuning scheme[5](https://www.nature.com/articles/s41597-023-02460-0#ref-CR5 "Shin, H.-C. et al. Deep convolutional neural networks for computer-aided detection: Cnn architectures, dataset characteristics and transfer learning. IEEE transactions on medical imaging 35, 1285–1298 (2016).") with ImageNet[6](https://www.nature.com/articles/s41597-023-02460-0#ref-CR6 "Deng, J. et al. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, 248–255 (2009).") pre-trained models can diminish the need of large-scale data for the train-from-scratch scheme. However, it still requires a fair amount of data for faster fine-tuning while avoiding overfitting. Alternatively, few-shot methods could leverage more on the distinctive representation produced by the foundation models, which has succeeded in considerable language modeling[7](https://www.nature.com/articles/s41597-023-02460-0#ref-CR7 "Brown, T. et al. Language models are few-shot learners. Advances in neural information processing systems 33, 1877–1901 (2020).") and vision[8](https://www.nature.com/articles/s41597-023-02460-0#ref-CR8 "Dhillon, G. S., Chaudhari, P., Ravichandran, A. & Soatto, S. A baseline for few-shot image classification. arXiv preprint arXiv:1909.02729 (2019)."), [9](https://www.nature.com/articles/s41597-023-02460-0#ref-CR9 "Tian, Y., Wang, Y., Krishnan, D., Tenenbaum, J. B. & Isola, P. Rethinking few-shot image classification: a good embedding is all you need? In Proceedings of the European Conference on Computer Vision, 266–282 (Springer, 2020).") tasks. The existing techniques of adapting foundation models in medical image analysis[10](https://www.nature.com/articles/s41597-023-02460-0#ref-CR10 "Ouyang, C. et al. Self-supervision with superpixels: Training few-shot medical image segmentation without annotation. In Proceedings of the European Conference on Computer Vision, 762–780 (Springer, 2020)."), [11](https://www.nature.com/articles/s41597-023-02460-0#ref-CR11 "Singh, R. et al. Metamed: Few-shot medical image classification using gradient-based meta-learning. Pattern Recognition 120, 108111 (2021).") demand the employment of dedicated medical pre-trained models that is hard to produce even if self-supervised learning is utilized. Recently, cutting-edge techniques, e.g., prompt-based learning[12](https://www.nature.com/articles/s41597-023-02460-0#ref-CR12 "Zhou, K., Yang, J., Loy, C. C. & Liu, Z. Learning to prompt for vision-language models. International Journal of Computer Vision 130, 2337–2348 (2022)."), [13](https://www.nature.com/articles/s41597-023-02460-0#ref-CR13 "Zhou, K., Yang, J., Loy, C. C. & Liu, Z. Conditional prompt learning for vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 16816–16825 (2022)."), can leverage the foundation models pre-trained (via self-supervised learning, e.g., DINO[14](https://www.nature.com/articles/s41597-023-02460-0#ref-CR14 "Caron, M. et al. Emerging properties in self-supervised vision transformers. In Proceedings of the International Conference on Computer Vision (ICCV) (2021).") and MAE[15](https://www.nature.com/articles/s41597-023-02460-0#ref-CR15 "He, K. et al. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 16000–16009 (2022).")) using vast amounts of data from multiple modalities and domains and transfer these universal representations to tasks with very limited data[16](https://www.nature.com/articles/s41597-023-02460-0#ref-CR16 "Jia, M. et al. Visual prompt tuning. In Avidan, S., Brostow, G., Cisse, M., Farinella, G. M. & Hassner, T. (eds.) Computer Vision – ECCV 2022, 709–727 (Springer Nature Switzerland, Cham, 2022)."), [17](https://www.nature.com/articles/s41597-023-02460-0#ref-CR17 "Qin, Z., Yi, H., Lao, Q. & Li, K. Medical image understanding with pretrained vision language models: A comprehensive study. In ICLR (2023)."). The fundamental difference in technical routine has started reshaping the landscape of medical image analysis. Therefore, it is in urgent demand to set up datasets and benchmarks to promote innovation in this fast-marching research field and properly evaluate the performance gain and other cost-effective aspects. There are benchmarks[18](https://www.nature.com/articles/s41597-023-02460-0#ref-CR18 "Sun, L. et al. Few-shot medical image segmentation using a global correlation network with discriminative embedding                    https://doi.org/10.48550/arXiv.2012.05440                                     (2020)."), [19](https://www.nature.com/articles/s41597-023-02460-0#ref-CR19 "Shakeri, F. et al. Fhist: A benchmark for few-shot classification of histological images                    https://doi.org/10.48550/arXiv.2206.00092                                     (2022).") for the few-shot learning tasks. Nonetheless, they focus more on each individual data modality and task. Here, we will instead promote the generalizability of the few-shot learning methods, i.e., strengthening their overall performance on various data modalities and tasks.

In this paper, we proposed a novel dataset, **MedFMC**, with 22,349 images in total, which encapsulates five representative medical image classification tasks from real-world clinical daily routines. Fig. [1](https://www.nature.com/articles/s41597-023-02460-0#Fig1) presents sample images from each subset, and Table [1](https://www.nature.com/articles/s41597-023-02460-0#Tab1) shows the summary of data, including modality, number of samples, image size, classification tasks, and number of classes. Different from many existing public datasets in the medical domain, e.g., Chest X-rays[20](https://www.nature.com/articles/s41597-023-02460-0#ref-CR20 "Wang, X. et al. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 3462–3471 (2017)."), [21](https://www.nature.com/articles/s41597-023-02460-0#ref-CR21 "Irvin, J. A. et al. Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison. In AAAI (2019)."), [22](https://www.nature.com/articles/s41597-023-02460-0#ref-CR22 "Johnson, A. E. W. et al. Mimic-cxr: A large publicly available database of labeled chest radiographs. Sci. Data 6 (2019)."), MSD[23](https://www.nature.com/articles/s41597-023-02460-0#ref-CR23 "Antonelli, M. et al. The medical segmentation decathlon. Nature Communications 13 (2021)."), and HAM10000[24](https://www.nature.com/articles/s41597-023-02460-0#ref-CR24 "Tschandl, P., Rosendahl, C. & Kittler, H. The ham10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. Scientific Data 5 (2018)."), the proposed dataset and benchmark do not target advancing and evaluating the performance of each individual task with the conventional full-supervised training paradigm, which may require larger amount of data individually. Instead, we believe that this new dataset (as a union) provides valuable support to develop and **evaluate generalizable solutions of adapting foundation models** to a variety of medical downstream applications, e.g., using few samples as the prompts and the rest as testing standardly across all five tasks. In this study, we focus on 2D medical image classification as a start and cover the most common 2D medical imaging modalities. 3D data and other tasks, e.g., detection and segmentation, will be expanded and investigated in future work.

**Fig. 1**

[![figure 1](https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41597-023-02460-0/MediaObjects/41597_2023_2460_Fig1_HTML.png)](https://www.nature.com/articles/s41597-023-02460-0/figures/1)

Sample images from five subsets.

[Full size image](https://www.nature.com/articles/s41597-023-02460-0/figures/1)

**Table 1 Data summary of MedFMC.**

[Full size table](https://www.nature.com/articles/s41597-023-02460-0/tables/1)

The proposed datasets target promoting the following aspects of foundation model adaptation approaches:

- **Generalizability:** The proposed dataset has the capacity to examine the generalizability of the evaluated method from multiple perspectives. First, the benchmarked approach should achieve superior performance on all five prediction tasks, which are largely varied in data modality and image characteristics. Additionally, the composed five subsets of data are diversified in image sizes, data sample numbers, and classification tasks (e.g., multi-class, multi-label, and regression ones), as shown in Fig. [1](https://www.nature.com/articles/s41597-023-02460-0#Fig1).

- **Performance on Rare Diseases (Tail Classes):** The few-shot learning scheme fits perfectly for the long-tailed classification scenario, which often has only a few cases available for rare diseases in training. We will also face data scarcity in the testing phase, and separate evaluation metrics need to be recruited. The performance of algorithms on these tail classes can better reveal the power of pre-trained models and their adaptation techniques.

- **Prediction Accuracy and Adaptation Efficiency**: Besides evaluating the prediction accuracy of algorithms, we also pay attention to the efficiency of training (with fewer samples) in the cost of both data and computation. By combining both the accuracy and cost aspects in the evaluation metrics, we expect the advanced methods can further ease the effort of obtaining quality annotations and meanwhile lower the demand for computational resources.


Illustratively, we present the benchmarking results of several common learning paradigms, e.g., fine-tuning and few-shot approaches. During the training phase, a small amount of randomly picked data (a few samples, i.e., 1, 5, and 10) are utilized for the initial training, and the rest of the dataset is employed for the validation. Approaches with advanced cross-domain knowledge transfer techniques are expected to achieve higher performance scores in such a setting. The final metrics are computed on an average of ten individual runs of the same testing process.

## Methods

### IRB Ethics review and exemption

The presented retrospective research study has been reviewed by each involved institute individually, and patients consent to data sharing and the open publication of the data (otherwise waived as detailed below). The ChestDR is approved by Fengcheng People’s Hospital Ethics Committee (Ref. 2020 YiYanLunShen No.016) and Huanggang Hospital of Traditional Chinese Medicine Medical Research Ethics Committee (Ref. 2020 LunShen No.003), and the committee waived the consent since the retrospective research will not change the examination process of the patients. All data were adequately anonymized, and the risk of disclosing patient privacy via imaging data was minimal. The NeoJaundice was approved by Xuzhou Central Hospital Ethics Committee (Ref. XZXYLQ-20180517-008), and patients’ consent to the data collection was obtained from the guardians of the children. No identifying images are included, and the data are anonymized from the children. The Retino is approved by Shanghai Tenth People’s Hospital Ethics Committee (Ref. SHSY-IEC-4.1/20-154/01), which waived the consent since it is a retrospective research task and the risk of disclosing patient privacy via retinography images has been minimized. The Endo is approved by Renji Hospital Ethics Committee. The committee reviewed and waived consent since the research was a retrospective study, and the risk of disclosing patient privacy via the studied snapshot images was minimized. The ColonPath is derived from part of the DigestPath 2019 challenge data, accessible via [https://digestpath2019.grand-challenge.org/Dataset/](https://digestpath2019.grand-challenge.org/Dataset/), which was originally approved by the Histo Pathology Diagnostic Center Ethics Committee. The committee also waived the consent since It is a retrospective research task and the risk of disclosing patient privacy via pathology images is minimal.

### Shared pipeline for data collection and annotation

Fig. [2](https://www.nature.com/articles/s41597-023-02460-0#Fig2) illustrates the general data sample collection and annotation pipeline. MedFMC is composed of data with five different modalities in medical imaging, i.e., chest radiography, pathological images, endoscopy photos, dermatological images, and retinal images. The entire process consists of three major steps. First, the original data are listed and fetched from various systems, e.g., X-rays in the picture archiving and communication system (PACS), blood test results in Health Information System (HIS), endoscopy photos in the workstations, etc. Detailed processes are varied from modality to modality, which will be introduced in detail individually. Then, standardized anonymization of patient information (mainly the DICOM images) is performed before leaving the hospitals using the DICOM Anonymizer tool provided by the RSNA MIRC[25](https://www.nature.com/articles/s41597-023-02460-0#ref-CR25 "The RSNA MIRC project. Dicom anonymizer.                    http://mirc.rsna.org/download                                    ."). All image data are converted into 12-bit PNG images while the original image sizes are preserved. All image samples are manually examined to redact any privacy-related text or objects recorded in the images. Finally, a two-stage annotation process is conducted by first generating the initial labels, e.g., annotated by the medical trainees, blood test results extracted from the HIS, and grading prediction from a pre-trained model using public datasets. Senior professionals with over ten years of experience in their specialty, e.g., radiologist, pathologist, gastroenterologist, ophthalmologist, and pediatrician, verify the annotation for each image. In the following sections, we will discuss specific settings for each subset.

**Fig. 2**

[![figure 2](https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41597-023-02460-0/MediaObjects/41597_2023_2460_Fig2_HTML.png)](https://www.nature.com/articles/s41597-023-02460-0/figures/2)

Overview of MedFMC.

[Full size image](https://www.nature.com/articles/s41597-023-02460-0/figures/2)

#### ChestDR: Thoracic diseases screening in chest radiography

Chest X-ray is a regularly adopted imaging technique for daily clinical routine. Many thoracic diseases are reported, and further examinations are recommended for differential diagnoses. Due to the large amount and fast reporting requirements in certain emergency facilities, a swift screening and reporting of common thoracic diseases could largely improve the efficiency of the clinical process. Although a few chest x-ray datasets[20](https://www.nature.com/articles/s41597-023-02460-0#ref-CR20 "Wang, X. et al. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 3462–3471 (2017)."), [21](https://www.nature.com/articles/s41597-023-02460-0#ref-CR21 "Irvin, J. A. et al. Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison. In AAAI (2019)."), [22](https://www.nature.com/articles/s41597-023-02460-0#ref-CR22 "Johnson, A. E. W. et al. Mimic-cxr: A large publicly available database of labeled chest radiographs. Sci. Data 6 (2019).") are now publicly available, images with quality annotations (preferably verified by radiologists) are still a desired resource for training and evaluating the models.

A total of 4,848 frontal radiography images (from 4,848 patients) are provided in ChestDR, collected from two regional hospitals in Hubei and Jiangxi Province, China. A detailed distribution of 19 common thoracic diseases is presented in Fig. [3](https://www.nature.com/articles/s41597-023-02460-0#Fig3), which is sorted with the number of samples. Tail classes are highlighted in Red. Each PNG image is converted from the original DICOM files using the default window level and width (stored in the DICOM tags). The original image sizes are preserved. The initial disease labels are provided by a radiological resident (with the support of previously signed radiology reports) and then confirmed by a senior radiologist.

**Fig. 3**

[![figure 3](https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41597-023-02460-0/MediaObjects/41597_2023_2460_Fig3_HTML.png)](https://www.nature.com/articles/s41597-023-02460-0/figures/3)

Data samples and case summary of ChestDR.

[Full size image](https://www.nature.com/articles/s41597-023-02460-0/figures/3)

#### ColonPath: Lesion tissue screening in pathology patches

Pathology examination can support detecting early-stage cancer cells in small tissue slices. In the pathologist’s daily routine, they are required to look over several dozens of tissue slides, a tiresome and tedious job. In clinical diagnosis, quantifying cancer cells and regions is the primary goal for pathologists. The approaches for the classification of pathological tissue patches are desired to ease this process. They can help screen whether it exists regions of malignant cells in the entire slide in a sliding window manner.

The pathology whole slide image (WSI) is originally collected from the Histo Pathology Diagnostic Center, which is also published and utilized in the DigestPath Challenge 2019[26](https://www.nature.com/articles/s41597-023-02460-0#ref-CR26 "Da, Q. et al. Digestpath: A benchmark dataset with challenge review for the pathological detection and segmentation of digestive-system. Medical Image Analysis 80, 102485 (2022)."). Only the data for the lesion segmentation tasks are employed in this study. All WSIs were acquired during 2017–2019 with hematoxylin and eosin (HE) stains and scanned using the KF- BIO FK-Pro-120 slide scanner. Subsequently, the WSIs were re-scaled to ×20 magnification with a pixel resolution of 0.475 _μ_ m. Tissue patches are extracted from the WSI in a sliding window fashion with a fixed size of 1024 × 1024 and a stride of 768. A total of 396 patients’ 10,009 large tissue patches (with a uniform size of 1024 × 1024) of colonoscopy pathology examination will be available in ColonPath. Positive and negative patch samples (with and without the lesion tissue, computed based on the existing lesion region labels) are illustrated in Fig. [4](https://www.nature.com/articles/s41597-023-02460-0#Fig4) along with the number of samples in each category. The initial labels (whether containing lesion tissues) are provided by a trainee in the pathology specialty (with the support of computed labels) and then confirmed by a senior pathologist.

**Fig. 4**

[![figure 4](https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41597-023-02460-0/MediaObjects/41597_2023_2460_Fig4_HTML.png)](https://www.nature.com/articles/s41597-023-02460-0/figures/4)

Data samples and case summary of ( **a**) ColonPath and ( **b**) NeoJaundice.

[Full size image](https://www.nature.com/articles/s41597-023-02460-0/figures/4)

#### NeoJaundice: Neonatal jaundice evaluation in skin photos

Jaundice commonly occurs in newborn infants. However, most jaundice is benign and does not require any interference. Conventionally, newborns must be monitored by taking a blood test to examine the bilirubin level. The potential toxicity of bilirubin might lead to severe hyperbilirubinemia and, in rare cases, acute bilirubin encephalopathy or kernicterus. Recent techniques utilized skin photos of three different parts of the infants, i.e., head, face, and chest, to estimate the total serum bilirubin in the blood so as to avoid the repeated invasive blood test for infants.

A total of 745 infants’ 2,235 images (with an average size of 567 × 567) are collected in the NeoJaundice dataset from the Xuzhou central hospital. The initial binary labels are generated using the total serum bilirubin readings extracted from the hospital’s health information system with a threshold of 12.9 mg/dL and then confirmed by a senior experienced pediatrician. Samples of both low and high bilirubin levels are illustrated in Fig. [4](https://www.nature.com/articles/s41597-023-02460-0#Fig4) along with the number of samples in each category. Three images are acquired for each infant on body skins of the head, face, and chest, using digital cameras. The skin regions are surrounded by a standardized color card for color calibration purposes.

#### Endo: Lesion classification in colonoscopy images

Colorectal cancer is one of the most common and fatal cancers among men and women around the world. Abnormalities like polyps and ulcers are precursors to colorectal cancer and are often found in colonoscopy screening of people aged above 50. The risks largely increase along with aging. Colonoscopy is the gold standard for the detection and early diagnosis of such abnormalities with necessary biopsy on site, which could significantly affect the survival rate from colorectal cancer. Automatic detection of such lesions during the colonoscopy procedure could prevent missing lesions and ease the workload of gastroenterologists in colonoscopy.

A total of 80 patients’ 3,865 images (with an average size of 1280 × 1024) recorded during the colonoscopy examination on the workstations in Renji Hospital are produced in the Endo dataset. Four types of lesions, i.e., ulcer, erosion, polyp, and tumor, are included, which are illustrated in Fig. [5](https://www.nature.com/articles/s41597-023-02460-0#Fig5) along with the number of samples in each category. Non-relevant images are already excluded, while some noisy and degraded recordings remain to reflect the real-world data distribution. These noisy data are mainly caused by motions during the operation, which only occupy a small portion (<5%) of the images and often are labeled without any of the target lesions. The initial labels of lesions are performed by a junior gastroenterologist (with the support of health records and reports) and then confirmed by a senior experienced gastroenterologist.

**Fig. 5**

[![figure 5](https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41597-023-02460-0/MediaObjects/41597_2023_2460_Fig5_HTML.png)](https://www.nature.com/articles/s41597-023-02460-0/figures/5)

Data samples and case summary of ( **a**) Endo and ( **b**) Retino.

[Full size image](https://www.nature.com/articles/s41597-023-02460-0/figures/5)

#### Retino: Diabetic retinopathy grading in retina images

Diabetic retinopathy (DR) can lead to vision loss and blindness in patients with diabetes, mainly affecting the blood vessel in the retina. Therefore, it is important to have an exam of the retina each year for the early detection of DR. Currently, DR grading requires a trained ophthalmologist to manually evaluate color fundus photos of the retina, which is time-consuming and may delay the treatment of patients. Automated screening of DR has long been recognized and desired.

A total of 1,392 patients’ fundus images (one from each patient with an average size of 2736 × 1824) from Shanghai Tenth People’s Hospital are included in the Retino dataset, which is extracted from the retinal imaging workstations after the examination. Images are captured by Canon nonmydriatic fundus cameras that mainly adopted the 45° macula-centered imaging protocol. Samples of retina images in each of the five grades are illustrated in Fig. [5](https://www.nature.com/articles/s41597-023-02460-0#Fig5) along with the number of samples in each grade. A DenseNet-121 (with ImageNet pre-trained model weights) is first fine-tuned using the dataset from Kaggle’s “Diabetic Retinopathy Detection” challenge and produced the prediction for each image. Then, an ophthalmologist with over ten years of experience examined again based on the automated generated prediction, i.e., the presence of diabetic retinopathy on a scale of 0 to 4 (0: No DR; 1: Mild; 2: Moderate; 3: Severe; 4: Proliferative DR).

## Data Records

The MedFMC Dataset is published via figshare[27](https://www.nature.com/articles/s41597-023-02460-0#ref-CR27 "Wang, D. et al. A real-world dataset and benchmark for foundation1 model adaptation in medical image classification, figshare,                    https://doi.org/10.6084/m9.figshare.c.6476047.v1                                     (2023)."). Each dataset in MedFMC consists of all image data in a “images” folder and associated image-level labels for each image in a CSV file. Multi-label tasks (i.e., ChestDR and Endo) will have multiple columns with either 1 or 0 that represent the existence of corresponding disease patterns. Binary and multi-class classification tasks (i.e., ColonPath, NeoJaundice, and Retino) will have only a single label with the individual class number. The images are named differently across institutes, i.e., named with a random ID (ChestDR and NeoJaundice) and with a random ID together with the data of collection, not the examination (ColonPath, Endo, and Retino).

## Technical Validation

### Dataset partition

Each image subset is divided into two parts: the few-shot pool and testing subsets. The few-shot pool consists of samples with about 20% randomly selected patients, and the count of each class must be larger than 10. The remaining samples are used for testing. In transfer learning, we use all the images in the few-shot pool for training and validate the deep-learning-based classifier models using testing. In the few-shot setting, we randomly picked images of 1, 5, and 10 patients for each class from the few-shot pool to build the support set, and the testing subset is reserved for the model evaluation. We provide the data list of the few-shot pool and testing set together with sample lists of few-shot images in the repository (see the Code availability section).

### Few-shot learning baseline

In the experiment, we employ two few-shot baseline methods, i.e., Meta-Baseline[28](https://www.nature.com/articles/s41597-023-02460-0#ref-CR28 "Chen, Y., Liu, Z., Xu, H., Darrell, T. & Wang, X. Meta-baseline: Exploring simple meta-learning for few-shot learning. 2021 IEEE/CVF International Conference on Computer Vision (ICCV) 9042–9051 (2020).") and Visual Prompt Tuning (VPT)[16](https://www.nature.com/articles/s41597-023-02460-0#ref-CR16 "Jia, M. et al. Visual prompt tuning. In Avidan, S., Brostow, G., Cisse, M., Farinella, G. M. & Hassner, T. (eds.) Computer Vision – ECCV 2022, 709–727 (Springer Nature Switzerland, Cham, 2022)."). Meta-Baseline[28](https://www.nature.com/articles/s41597-023-02460-0#ref-CR28 "Chen, Y., Liu, Z., Xu, H., Darrell, T. & Wang, X. Meta-baseline: Exploring simple meta-learning for few-shot learning. 2021 IEEE/CVF International Conference on Computer Vision (ICCV) 9042–9051 (2020).") is chosen here as a classic few-shot method to evaluate across all five datasets. The input images are converted to the embedding features via three backbone networks and pre-trained model settings, including DenseNet 121 layers (Dense121) with ImageNet pre-trained weights in supervised learning (SL) and a Swin Transformer (Swin-base) with pre-trained weights from both fully-supervised and self-supervised learning (SSL) schemes (SimMIM[29](https://www.nature.com/articles/s41597-023-02460-0#ref-CR29 "Xie, Z. et al. Simmim: A simple framework for masked image modeling. In International Conference on Computer Vision and Pattern Recognition (CVPR) (2022)."), a form of Masked Auto-Encoder[15](https://www.nature.com/articles/s41597-023-02460-0#ref-CR15 "He, K. et al. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 16000–16009 (2022).")). Settings are specified when reporting the performance as shown in the left column of Table [2](https://www.nature.com/articles/s41597-023-02460-0#Tab2). We cluster the class centers in the support set using the extracted features and compute the cosine similarities between one image in the testing set and the class centers to determine the category. Additionally, we include VPT as an advanced method in training visual prompts for the few-shot classification tasks. In this case, a vanilla pre-trained model from the Swin-transformer repository (pre-trained on ImageNet21K and finetuned on ImageNet1k) is utilized to initialize the VPT-based few-shot tuning. We repeat the experiment 10 times (randomly picking few-shot samples) on the five medical image datasets and report the averaged testing results.

**Table 2 Results of few-shot learning baseline on MedFMC.**

[Full size table](https://www.nature.com/articles/s41597-023-02460-0/tables/2)

### Transfer learning baseline

We run the fine-tuning experiments using three representative networks, including DenseNet, EfficientNet, and Swin Transformer, on the five medical image datasets. The Swin transformer model is pre-trained on ImageNet21K with self-supervised learning and then finetuned on ImageNet1k with labels. The others are also pre-trained using ImageNet but with supervised learning. In our experiments, the fine-tuning is performed as linear probing, i.e., only tuning the classifier (fully connected) layers since the parameters in the representation layers are also frozen for the few-shot baseline methods. We also experimented with finetuning the entire network, which could generally improve the performance by 1–2% in accuracy. During the training and inference stage, all the input images are padded and rescaled to 384\*384 pixels. Common data augmentation tricks, i.e., random crop, resize, and horizontal flip, are adopted. The cross-entropy loss is employed as the loss function for the multi-class classification of three datasets, including ColonPath, NeoJaundice, and Retino, while the binary cross-entropy loss is computed for the multi-label classification of the remaining two datasets, i.e., ChestDR and Endo. The model parameters (except the fully connected classifier layer) are initialized by the ImageNet pre-trained model weights and frozen during the tuning. SGD optimizers with initial learning rates of 0.002 and 0.01 are applied for the model training of DenseNet and EfficientNet, respectively. The Swin transformer model is optimized by AdamW with an initial learning rate of 0.001. We trained these classification models on a single NVIDIA A100 for 20 epochs at a batch size of 8, using the framework of MMClassification[30](https://www.nature.com/articles/s41597-023-02460-0#ref-CR30 "Mmclassification.                    https://github.com/open-mmlab/mmclassification                                    .").

### Evaluation metrics

To evaluate the performance of transfer learning and few-shot learning baseline experimental results, we compute the overall accuracy (Acc) and area under the receiver operating characteristic curve (AUC) for the multi-class classification tasks in the datasets of ColonPath, NeoJaundice, and Retino, and the mean average precision (mAP) and AUC for the multi-label classification tasks in the datasets of ChestDR and Endo. Accuracy reflects the overall correct predictions among all the test images. The predicted label is determined with the maximum softmax outputs in the multi-class classification task. AUC is computed for each class to measure the capability of distinguishing between positive and negative classes at various threshold settings. The AP is the weighted average of precisions, while the mAP for all samples is the mean value of the AP scores for each class.

### Benchmarking results

#### Results of few-shot baselines

The classification performance of few-shot baselines on each dataset is shown in Table [2](https://www.nature.com/articles/s41597-023-02460-0#Tab2). More data can often provide better support for distinguishing the representations of testing data, but it comes with a higher data demand and more extensive computation cost. The classification performance on five datasets varies significantly, which indeed indicates the diverse task difficulty. The Meta-baseline also performs better on parts of the five sets and also has mixed results for multi-class and multi-label classifications. VPT clearly achieves the best overall performance considering additional tuning parameters (visual prompts) and a network fine-tuning process included in the approach. Regarding the network backbone, advanced architectures, e.g., Swin-transformer, does not always produce superior performance over convolutional neural network counterparts when using the same ImageNet pre-trained model (via either supervised learning-based or self-supervised learning-based schemes). Furthermore, the detailed performance of each disease/lesion class for the three multi-label and multi-class classification tasks are illustrated in Table [3](https://www.nature.com/articles/s41597-023-02460-0#Tab3). Especially the results for thoracic diseases classification are listed for head and tail classes separately. Higher or equivalent AUCs for these rare classes (tail ones) are achieved, which indicates that few-shot methods can benefit the classification of rare classes more than the common learning paradigms.

**Table 3 Results of sub-classes with meta-baseline and 10-shot patient data.**

[Full size table](https://www.nature.com/articles/s41597-023-02460-0/tables/3)

#### Choices of few-shot samples

Since we repeat the experiment 10 times (randomly picking few-shot samples) on the five medical image datasets. The choice can affect the classification performance of the averaged testing results. We list the STD in addition to the mean accuracy and AUC, as shown in Table [3](https://www.nature.com/articles/s41597-023-02460-0#Tab3). The variances of accuracy and AUCs are often fluctuant less than 5% in the example 10-shot setting.

#### Results of fine-tuning baselines

Table [4](https://www.nature.com/articles/s41597-023-02460-0#Tab4) shows the results of fine-tuning-based classification frameworks with all 20% patient data from the few-shot pool and with 10-shot sample data individually. We further list the results of head and tail classes (two columns as shown in Fig. [3](https://www.nature.com/articles/s41597-023-02460-0#Fig3)). There is still quite a gap between the classification accuracies for these two groups of methods when all 20% of data in the few-shot pool are utilized in fine-tuning, which is reasonable, considering more training samples are utilized. Nonetheless, the fine-tuning performance decrease to an equivalent level to the few-shot learning paradigms when only ten patients’ data are employed. When there is a scarcity of sample data, it is highly advantageous to utilize few-shot-based techniques. We do not show the results of fine-tuning using fewer (1 and 5) samples since we find the training hard to accomplish (either overfitting or underfitting) using very few data points, which reveals a critical limitation for the fine-tuning-based methods. Moreover, we provided the finetuning results with and without data augmentation at the bottom of Table [4](https://www.nature.com/articles/s41597-023-02460-0#Tab4). The difference between them is rather marginal.

**Table 4 Results of transfer learning baseline on MedFMC with 10-shot and 20% patient data.**

[Full size table](https://www.nature.com/articles/s41597-023-02460-0/tables/4)

## Usage Notes

The provided dataset is publicly available under the Creative Commons Zero (CC0) Attribution. Please note the presented datasets are not intended for the development of diagnosis-oriented algorithms and models. It should also not be utilized as the sole base of the clinical evaluation for each classification task.

## Code availability

The code repository of the presented few-shot methods can be accessed via [https://github.com/wllfore/MedFMC\_fewshot\_baseline](https://github.com/wllfore/MedFMC_fewshot_baseline). No custom code was used to generate or process the data described in the manuscript.

## References

01. Dosovitskiy, A. _et al_. An image is worth 16 × 16 words: Transformers for image recognition at scale. In _International Conference on Learning Representations_ (2021).

02. Radford, A., _et al_. Improving language understanding by generative pre-training. _OpenAI_ (2018).

03. Gu, Y. _et al_. Domain-specific language model pretraining for biomedical natural language processing. _ACM Transactions on Comput. for Healthc. (HEALTH)_ **3**, 1–23 (2020).

    [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Domain-specific%20language%20model%20pretraining%20for%20biomedical%20natural%20language%20processing&journal=ACM%20Transactions%20on%20Comput.%20for%20Healthc.%20%28HEALTH%29&volume=3&pages=1-23&publication_year=2020&author=Gu%2CY)

04. Radford, A. _et al_. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, 8748–8763 (2021).

05. Shin, H.-C. _et al_. Deep convolutional neural networks for computer-aided detection: Cnn architectures, dataset characteristics and transfer learning. _IEEE transactions on medical imaging_ **35**, 1285–1298 (2016).

    [Article](https://doi.org/10.1109%2FTMI.2016.2528162) [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=26886976) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Deep%20convolutional%20neural%20networks%20for%20computer-aided%20detection%3A%20Cnn%20architectures%2C%20dataset%20characteristics%20and%20transfer%20learning&journal=IEEE%20transactions%20on%20medical%20imaging&doi=10.1109%2FTMI.2016.2528162&volume=35&pages=1285-1298&publication_year=2016&author=Shin%2CH-C)

06. Deng, J. _et al_. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, 248–255 (2009).

07. Brown, T. _et al_. Language models are few-shot learners. _Advances in neural information processing systems_ **33**, 1877–1901 (2020).

    [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Language%20models%20are%20few-shot%20learners&journal=Advances%20in%20neural%20information%20processing%20systems&volume=33&pages=1877-1901&publication_year=2020&author=Brown%2CT)

08. Dhillon, G. S., Chaudhari, P., Ravichandran, A. & Soatto, S. A baseline for few-shot image classification. _arXiv preprint arXiv:1909.02729_ (2019).

09. Tian, Y., Wang, Y., Krishnan, D., Tenenbaum, J. B. & Isola, P. Rethinking few-shot image classification: a good embedding is all you need? In _Proceedings of the European Conference on Computer Vision_, 266–282 (Springer, 2020).

10. Ouyang, C. _et al_. Self-supervision with superpixels: Training few-shot medical image segmentation without annotation. In _Proceedings of the European Conference on Computer Vision_, 762–780 (Springer, 2020).

11. Singh, R. _et al_. Metamed: Few-shot medical image classification using gradient-based meta-learning. _Pattern Recognition_ **120**, 108111 (2021).

    [Article](https://doi.org/10.1016%2Fj.patcog.2021.108111) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Metamed%3A%20Few-shot%20medical%20image%20classification%20using%20gradient-based%20meta-learning&journal=Pattern%20Recognition&doi=10.1016%2Fj.patcog.2021.108111&volume=120&publication_year=2021&author=Singh%2CR)

12. Zhou, K., Yang, J., Loy, C. C. & Liu, Z. Learning to prompt for vision-language models. _International Journal of Computer Vision_ **130**, 2337–2348 (2022).

    [Article](https://link.springer.com/doi/10.1007/s11263-022-01653-1) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Learning%20to%20prompt%20for%20vision-language%20models&journal=International%20Journal%20of%20Computer%20Vision&doi=10.1007%2Fs11263-022-01653-1&volume=130&pages=2337-2348&publication_year=2022&author=Zhou%2CK&author=Yang%2CJ&author=Loy%2CCC&author=Liu%2CZ)

13. Zhou, K., Yang, J., Loy, C. C. & Liu, Z. Conditional prompt learning for vision-language models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 16816–16825 (2022).

14. Caron, M. _et al_. Emerging properties in self-supervised vision transformers. In _Proceedings of the International Conference on Computer Vision (ICCV)_ (2021).

15. He, K. _et al_. Masked autoencoders are scalable vision learners. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 16000–16009 (2022).

16. Jia, M. _et al_. Visual prompt tuning. In Avidan, S., Brostow, G., Cisse, M., Farinella, G. M. & Hassner, T. (eds.) _Computer Vision – ECCV 2022_, 709–727 (Springer Nature Switzerland, Cham, 2022).

17. Qin, Z., Yi, H., Lao, Q. & Li, K. Medical image understanding with pretrained vision language models: A comprehensive study. In _ICLR_ (2023).

18. Sun, L. _et al_. Few-shot medical image segmentation using a global correlation network with discriminative embedding [https://doi.org/10.48550/arXiv.2012.05440](https://doi.org/10.48550/arXiv.2012.05440) (2020).

19. Shakeri, F. _et al_. Fhist: A benchmark for few-shot classification of histological images [https://doi.org/10.48550/arXiv.2206.00092](https://doi.org/10.48550/arXiv.2206.00092) (2022).

20. Wang, X. _et al_. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. _2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_ 3462–3471 (2017).

21. Irvin, J. A. _et al_. Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison. In _AAAI_ (2019).

22. Johnson, A. E. W. _et al_. Mimic-cxr: A large publicly available database of labeled chest radiographs. _Sci. Data_ **6** (2019).

23. Antonelli, M. _et al_. The medical segmentation decathlon. _Nature Communications_ **13** (2021).

24. Tschandl, P., Rosendahl, C. & Kittler, H. The ham10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. _Scientific Data_ **5** (2018).

25. The RSNA MIRC project. Dicom anonymizer. [http://mirc.rsna.org/download](http://mirc.rsna.org/download).

26. Da, Q. _et al_. Digestpath: A benchmark dataset with challenge review for the pathological detection and segmentation of digestive-system. _Medical Image Analysis_ **80**, 102485 (2022).

    [Article](https://doi.org/10.1016%2Fj.media.2022.102485) [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=35679692) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Digestpath%3A%20A%20benchmark%20dataset%20with%20challenge%20review%20for%20the%20pathological%20detection%20and%20segmentation%20of%20digestive-system&journal=Medical%20Image%20Analysis&doi=10.1016%2Fj.media.2022.102485&volume=80&publication_year=2022&author=Da%2CQ)

27. Wang, D. _et al_. A real-world dataset and benchmark for foundation1 model adaptation in medical image classification, _figshare_, [https://doi.org/10.6084/m9.figshare.c.6476047.v1](https://doi.org/10.6084/m9.figshare.c.6476047.v1) (2023).

28. Chen, Y., Liu, Z., Xu, H., Darrell, T. & Wang, X. Meta-baseline: Exploring simple meta-learning for few-shot learning. _2021 IEEE/CVF International Conference on Computer Vision (ICCV)_ 9042–9051 (2020).

29. Xie, Z. _et al_. Simmim: A simple framework for masked image modeling. In _International Conference on Computer Vision and Pattern Recognition (CVPR)_ (2022).

30. Mmclassification. [https://github.com/open-mmlab/mmclassification](https://github.com/open-mmlab/mmclassification).


[Download references](https://citation-needed.springer.com/v2/references/10.1038/s41597-023-02460-0?format=refman&flavour=references)

## Acknowledgements

J.S. is supported by grants from Shanghai Science and Technology Innovation Initiative (21SQBS02302), and Cultivated Funding for Clinical Research Innovation, Ren Ji Hospital, Shanghai Jiao Tong University School of Medicine \[RJPY-LX-004\]. Q.D. is supported by Shanghai Municipal Science and Technology Key Project (Grant No. 20511100302).

## Author information

Author notes

1. These authors contributed equally: Dequan Wang, Xiaosong Wang.


### Authors and Affiliations

1. Shanghai AI Laboratory, Shanghai, China

Dequan Wang, Xiaosong Wang, Lilong Wang, Mengzhang Li, Junjun He, Kang Li, Yu Qiao & Shaoting Zhang

2. Shanghai Jiaotong University, Shanghai, China

Dequan Wang

3. Shanghai Ruijing Hospital, School of Medicine, Shanghai Jiao Tong University, Shanghai, China

Qian Da

4. Shanghai Tenth People’s Hospital of Tongji University, Shanghai, China

Xiaoqiang Liu

5. Xuzhou Central Hospital, Xuzhou, China

Xiangyu Gao

6. Renji Hospital, School of Medicine, Shanghai Jiao Tong University, Shanghai, China

Jun Shen

7. Sensetime Research, Shanghai, China

Tian Shen & Qi Duan

8. The First Affiliated Hospital of Zhengzhou University, Zhengzhou, China

Jie Zhao

9. West China Hospital, Sichuan University, Chengdu, China

Kang Li


Authors

01. Dequan Wang


    [View author publications](https://www.nature.com/search?author=Dequan%20Wang)





    Search author on:[PubMed](https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Dequan%20Wang) [Google Scholar](https://scholar.google.co.uk/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=%22Dequan%20Wang%22&as_publication=&as_ylo=&as_yhi=&as_allsubj=all&hl=en)

02. Xiaosong Wang


    [View author publications](https://www.nature.com/search?author=Xiaosong%20Wang)





    Search author on:[PubMed](https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Xiaosong%20Wang) [Google Scholar](https://scholar.google.co.uk/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=%22Xiaosong%20Wang%22&as_publication=&as_ylo=&as_yhi=&as_allsubj=all&hl=en)

03. Lilong Wang


    [View author publications](https://www.nature.com/search?author=Lilong%20Wang)





    Search author on:[PubMed](https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Lilong%20Wang) [Google Scholar](https://scholar.google.co.uk/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=%22Lilong%20Wang%22&as_publication=&as_ylo=&as_yhi=&as_allsubj=all&hl=en)

04. Mengzhang Li


    [View author publications](https://www.nature.com/search?author=Mengzhang%20Li)





    Search author on:[PubMed](https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Mengzhang%20Li) [Google Scholar](https://scholar.google.co.uk/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=%22Mengzhang%20Li%22&as_publication=&as_ylo=&as_yhi=&as_allsubj=all&hl=en)

05. Qian Da


    [View author publications](https://www.nature.com/search?author=Qian%20Da)





    Search author on:[PubMed](https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Qian%20Da) [Google Scholar](https://scholar.google.co.uk/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=%22Qian%20Da%22&as_publication=&as_ylo=&as_yhi=&as_allsubj=all&hl=en)

06. Xiaoqiang Liu


    [View author publications](https://www.nature.com/search?author=Xiaoqiang%20Liu)





    Search author on:[PubMed](https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Xiaoqiang%20Liu) [Google Scholar](https://scholar.google.co.uk/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=%22Xiaoqiang%20Liu%22&as_publication=&as_ylo=&as_yhi=&as_allsubj=all&hl=en)

07. Xiangyu Gao


    [View author publications](https://www.nature.com/search?author=Xiangyu%20Gao)





    Search author on:[PubMed](https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Xiangyu%20Gao) [Google Scholar](https://scholar.google.co.uk/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=%22Xiangyu%20Gao%22&as_publication=&as_ylo=&as_yhi=&as_allsubj=all&hl=en)

08. Jun Shen


    [View author publications](https://www.nature.com/search?author=Jun%20Shen)





    Search author on:[PubMed](https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Jun%20Shen) [Google Scholar](https://scholar.google.co.uk/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=%22Jun%20Shen%22&as_publication=&as_ylo=&as_yhi=&as_allsubj=all&hl=en)

09. Junjun He


    [View author publications](https://www.nature.com/search?author=Junjun%20He)





    Search author on:[PubMed](https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Junjun%20He) [Google Scholar](https://scholar.google.co.uk/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=%22Junjun%20He%22&as_publication=&as_ylo=&as_yhi=&as_allsubj=all&hl=en)

10. Tian Shen


    [View author publications](https://www.nature.com/search?author=Tian%20Shen)





    Search author on:[PubMed](https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Tian%20Shen) [Google Scholar](https://scholar.google.co.uk/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=%22Tian%20Shen%22&as_publication=&as_ylo=&as_yhi=&as_allsubj=all&hl=en)

11. Qi Duan


    [View author publications](https://www.nature.com/search?author=Qi%20Duan)





    Search author on:[PubMed](https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Qi%20Duan) [Google Scholar](https://scholar.google.co.uk/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=%22Qi%20Duan%22&as_publication=&as_ylo=&as_yhi=&as_allsubj=all&hl=en)

12. Jie Zhao


    [View author publications](https://www.nature.com/search?author=Jie%20Zhao)





    Search author on:[PubMed](https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Jie%20Zhao) [Google Scholar](https://scholar.google.co.uk/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=%22Jie%20Zhao%22&as_publication=&as_ylo=&as_yhi=&as_allsubj=all&hl=en)

13. Kang Li


    [View author publications](https://www.nature.com/search?author=Kang%20Li)





    Search author on:[PubMed](https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Kang%20Li) [Google Scholar](https://scholar.google.co.uk/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=%22Kang%20Li%22&as_publication=&as_ylo=&as_yhi=&as_allsubj=all&hl=en)

14. Yu Qiao


    [View author publications](https://www.nature.com/search?author=Yu%20Qiao)





    Search author on:[PubMed](https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Yu%20Qiao) [Google Scholar](https://scholar.google.co.uk/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=%22Yu%20Qiao%22&as_publication=&as_ylo=&as_yhi=&as_allsubj=all&hl=en)

15. Shaoting Zhang


    [View author publications](https://www.nature.com/search?author=Shaoting%20Zhang)





    Search author on:[PubMed](https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Shaoting%20Zhang) [Google Scholar](https://scholar.google.co.uk/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=%22Shaoting%20Zhang%22&as_publication=&as_ylo=&as_yhi=&as_allsubj=all&hl=en)


### Contributions

D.W. and X.W. conceptualized and compiled the dataset, created annotation protocols, and wrote most of the manuscript. L.W. and M.L. performed the technical validation. Q.D., X.L., X.G. and J.S. contributed to dataset curation and annotation. Q.D., T.S. and J.H. contributed to the dataset curation. J.Z., K.L., Y.Q. and S.Z. provided important scientific input and contributed to the writing of the manuscript. All authors read and approved the final version of the manuscript.

### Corresponding authors

Correspondence to
[Yu Qiao](mailto:qiaoyu@pjlab.org.cn) or [Shaoting Zhang](mailto:zhangshaoting@pjlab.org.cn).

## Ethics declarations

### Competing interests

The authors declare no competing interests.

## Additional information

**Publisher’s note** Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.

## Rights and permissions

**Open Access** This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit [http://creativecommons.org/licenses/by/4.0/](http://creativecommons.org/licenses/by/4.0/).

[Reprints and permissions](https://s100.copyright.com/AppDispatchServlet?title=A%20Real-world%20Dataset%20and%20Benchmark%20For%20Foundation%20Model%20Adaptation%20in%20Medical%20Image%20Classification&author=Dequan%20Wang%20et%20al&contentID=10.1038%2Fs41597-023-02460-0&copyright=The%20Author%28s%29&publication=2052-4463&publicationDate=2023-09-02&publisherName=SpringerNature&orderBeanReset=true&oa=CC%20BY)

## About this article

[![Check for updates. Verify currency and authenticity via CrossMark](<Base64-Image-Removed>)](https://crossmark.crossref.org/dialog/?doi=10.1038/s41597-023-02460-0)

### Cite this article

Wang, D., Wang, X., Wang, L. _et al._ A Real-world Dataset and Benchmark For Foundation Model Adaptation in Medical Image Classification.
_Sci Data_ **10**, 574 (2023). https://doi.org/10.1038/s41597-023-02460-0

[Download citation](https://citation-needed.springer.com/v2/references/10.1038/s41597-023-02460-0?format=refman&flavour=citation)

- Received: 06 April 2023

- Accepted: 09 August 2023

- Published: 02 September 2023

- DOI: https://doi.org/10.1038/s41597-023-02460-0


### Share this article

Anyone you share the following link with will be able to read this content:

Get shareable link

Sorry, a shareable link is not currently available for this article.

Copy to clipboard

Provided by the Springer Nature SharedIt content-sharing initiative


### Subjects

- [Computer science](https://www.nature.com/subjects/computer-science)
- [Scientific data](https://www.nature.com/subjects/scientific-data)

## This article is cited by

- ### [Artificial intelligence-based non-invasive bilirubin prediction for neonatal jaundice using 1D convolutional neural network](https://doi.org/10.1038/s41598-025-96100-9)



  - Fatemeh Makhloughi

_Scientific Reports_ (2025)

- ### [A fully open AI foundation model applied to chest radiography](https://doi.org/10.1038/s41586-025-09079-8)



  - DongAo Ma
  - Jiaxuan Pang
  - Jianming Liang

_Nature_ (2025)

- ### [Innovating robot-assisted surgery through large vision models](https://doi.org/10.1038/s44287-025-00166-6)



  - Zhe Min
  - Jiewen Lai
  - Hongliang Ren

_Nature Reviews Electrical Engineering_ (2025)

- ### [General-purpose foundation models for increased autonomy in robot-assisted surgery](https://doi.org/10.1038/s42256-024-00917-4)



  - Samuel Schmidgall
  - Ji Woong Kim
  - Axel Krieger

_Nature Machine Intelligence_ (2024)

- ### [Mining multi-center heterogeneous medical data with distributed synthetic learning](https://doi.org/10.1038/s41467-023-40687-y)



  - Qi Chang
  - Zhennan Yan
  - Dimitris N. Metaxas

_Nature Communications_ (2023)

Close bannerClose

![Nature Briefing AI and Robotics](https://www.nature.com/static/images/logos/nature-briefing-ai-and-robotics-logo-51b3cf6c52.svg)

Sign up for the _Nature Briefing: AI and Robotics_ newsletter — what matters in AI and robotics research, free to your inbox weekly.

Email address

Sign up

I agree my information will be processed in accordance with the _Nature_ and Springer Nature Limited [Privacy Policy](https://www.nature.com/info/privacy).

Close bannerClose

Get the most important science stories of the day, free in your inbox. [Sign up for Nature Briefing: AI and Robotics](https://www.nature.com/briefing/ai-and-robotics/?brieferEntryPoint=AIAndRoboticsBriefingBanner)

---

## Source 4: www.nature.com {#source-4}

**URL:** https://www.nature.com/articles/s41597-025-05233-z
**Content Type:** Web Content
**Content Length:** 65,969 characters
**Scraped At:** 2025-06-29T12:37:49.902Z

**Detected Structure:** Plain Text

### Content:

[Skip to main content](https://www.nature.com/articles/s41597-025-05233-z#content)

Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain
the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in
Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles
and JavaScript.

A Dataset of Medical Questions Paired with Automatically Generated Answers and Evidence-supported References


[Download PDF](https://www.nature.com/articles/s41597-025-05233-z.pdf)

[Download PDF](https://www.nature.com/articles/s41597-025-05233-z.pdf)

## Abstract

New Large Language Models (LLM)-based approaches to medical Question Answering show unprecedented improvements in the fluency, grammaticality, and other qualities of the generated answers. However, the systems occasionally produce coherent, topically relevant, and plausible answers that are not based on facts and may be misleading and even harmful. New types of datasets are needed to evaluate the truthfulness of generated answers and develop reliable approaches for detecting answers that are not supported by evidence. The MedAESQA (Medical Attributable and Evidence Supported Question Answering) dataset presented in this work is designed for developing, fine-tuning, and evaluating language generation models for their ability to attribute or support the stated facts by linking the statements to the relevant passages of reliable sources. The dataset comprises 40 naturally occurring aggregated deidentified questions. Each question has 30 human and LLM-generated answers in which each statement is linked to a scientific abstract that supports it. The dataset provides manual judgments on the accuracy of the statements and the relevancy of the scientific papers.

### Similar content being viewed by others

![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41591-024-03258-2/MediaObjects/41591_2024_3258_Fig1_HTML.png)

### [A toolbox for surfacing health equity harms and biases in large language models](https://www.nature.com/articles/s41591-024-03258-2?fromPaywallRec=false)

ArticleOpen access23 September 2024

![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41586-023-06291-2/MediaObjects/41586_2023_6291_Fig1_HTML.png)

### [Large language models encode clinical knowledge](https://www.nature.com/articles/s41586-023-06291-2?fromPaywallRec=false)

ArticleOpen access12 July 2023

![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41591-024-03423-7/MediaObjects/41591_2024_3423_Fig1_HTML.png)

### [Toward expert-level medical question answering with large language models](https://www.nature.com/articles/s41591-024-03423-7?fromPaywallRec=false)

ArticleOpen access08 January 2025

## Background & Summary

The unprecedented improvements in the quality of the answers to medical questions generated by AI models are enabled by datasets comprised of question-answer pairs, such as MultiMedQA[1](https://www.nature.com/articles/s41597-025-05233-z#ref-CR1 "Singhal, K. et al. Large language models encode clinical knowledge. Nature 620, 172–180 (2023)."). These traditional datasets were generated on the assumption that having pairs of questions and ideal answers along with the sets of relevant documents, such as PubMed abstracts, is sufficient to train and test the answer generation systems[2](https://www.nature.com/articles/s41597-025-05233-z#ref-CR2 "Demner-Fushman, D. & Lin, J. Answering clinical questions with knowledge-based and statistical techniques. Computational Linguistics 33, 63–103 (2007)."). This assumption held for the answers that were traditionally extracted from the relevant documents. With the advent of Large Language Models (LLMs) capable of generating answers using solely their internal representations of the training data, in so-called zero-shot settings, it became clear that the coherent, grammatically perfect, and topically relevant answers may not necessarily be factual. Studies on the evaluation of LLMs’ abilities to support (ground) generated statements with verifiable evidence from reliable sources have shown that the models may provide harmful answers[1](https://www.nature.com/articles/s41597-025-05233-z#ref-CR1 "Singhal, K. et al. Large language models encode clinical knowledge. Nature 620, 172–180 (2023)."), perform significantly worse on lay-user generated questions, and often fail to reference relevant sources[3](https://www.nature.com/articles/s41597-025-05233-z#ref-CR3 "Bašaragin, B. et al. How do you know that? teaching generative language models to reference answers to biomedical questions. In Demner-Fushman, D., Ananiadou, S., Miwa, M., Roberts, K. & Tsujii, J. (eds.) Proceedings of the 23rd Workshop on Biomedical Natural Language Processing, 536–547,                    https://doi.org/10.18653/v1/2024.bionlp-1.44                                     (Association for Computational Linguistics, Bangkok, Thailand, 2024)."). This can pose a risk to public health[4](https://www.nature.com/articles/s41597-025-05233-z#ref-CR4 "Biden, J. R. Executive order on the safe, secure, and trustworthy development and use of artificial intelligence (2023)."). Unsupported statements are, therefore, a major barrier to using LLMs in any applications that may affect health. Methods for grounding generated statements in reliable sources along with practical evaluation approaches are needed to overcome this barrier. To support these goals, we have developed MedAESQA (Medical attributable and evidence supported question answering) a publicly available dataset of naturally occurring health-related questions asked by the general population paired with sets of human and AI-generated answers. Each answer statement in the dataset is required to be supported by evidence, and the evidence and the documents containing the evidence are judged for accuracy and support. The dataset is designed to be used for developing, fine-tuning, and evaluating language generation models in several approaches that address the model’s ability to attribute or support the stated facts by linking the statements to the relevant passages of reliable sources. The approaches, Retrieval Augmented Generation (RAG)[5](https://www.nature.com/articles/s41597-025-05233-z#ref-CR5 "Lewis, P. et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems 33, 9459–9474 (2020).") and retrofit attribution[6](https://www.nature.com/articles/s41597-025-05233-z#ref-CR6 "Gao, L. et al. Rarr: Researching and revising what language models say, using language models. In The 61st Annual Meeting Of The Association For Computational Linguistics (2023)."), provide sources to the models to guide answer generation or to find support and post-edit the generated output, respectively. Additionally, approaches may interleave retrieval and generation tasks[7](https://www.nature.com/articles/s41597-025-05233-z#ref-CR7 "Jain, P., Baldini Soares, L. & Kwiatkowski, T. From RAG to riches: Retrieval interlaced with sequence generation. In Al-Onaizan, Y., Bansal, M. & Chen, Y.-N. (eds.) Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, 8887–8904,                    https://doi.org/10.18653/v1/2024.emnlp-main.502                                     (Association for Computational Linguistics, Miami, Florida, USA, 2024)."). The non-medical Question-Answering datasets that were used to support attribution include the Natural Questions dataset[8](https://www.nature.com/articles/s41597-025-05233-z#ref-CR8 "Kwiatkowski, T. et al. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics 7, 452–466,                    https://doi.org/10.1162/tacl_a_00276                                     (2019)."). The questions in this dataset consist of real anonymized aggregated queries seeking factual information using the Google search engine. The answers consist of a Wikipedia page, a bounding box on this page (effectively, a summary of the page), called the long answer, and the short answer, such as one or more named entities mentioned in the Wikipedia article, yes/no, or NULL, if the page does not answer the question.

In the medical domain, some datasets can be adapted to train models to support attribution. For example, the BioASQ data[9](https://www.nature.com/articles/s41597-025-05233-z#ref-CR9 "Krithara, A., Nentidis, A., Bougiatiotis, K. & Paliouras, G. Bioasq-qa: A manually curated corpus for biomedical question answering. Scientific Data 10, 170 (2023).") contains factoid, yes/no, list, and summary questions formulated by biomedical experts. The questions are linked to sets of biomedical terms (concepts) related to the question and a set of research articles that are sufficient to answer the question. Text snippets containing one or more sentences that answer the question fully or partially are marked in the articles by the experts. The dataset is primarily focused on drug-target-disease relations for medical investigations. The MEDIQA-AnS dataset[10](https://www.nature.com/articles/s41597-025-05233-z#ref-CR10 "Savery, M., Abacha, A. B., Gayen, S. & Demner-Fushman, D. Question-driven summarization of answers to consumer health questions. Scientific Data 7, 322 (2020).") contains consumer-health questions, the full text from reliable web pages, extracted passages from the full text, and manually created summaries. In general, an attribution verification dataset must contain a question and at least one answer in which each statement of a fact required to answer the question is annotated and linked to a corresponding statement in an evidence source that supports or contradicts the fact stated in the answer. While the above datasets may be retrofitted to adhere to this format, to the best of our knowledge, we present the first medical question answering dataset specifically generated to test attribution to identified sources when assessing the output of natural language generation models. The distinct characteristics of the MedAESQA dataset are as follows: 1) the questions are naturally occurring popular questions submitted by the public to the National Library of Medicine, 2) the questions are annotated with the main concept of interest and with the user’s intent, e.g., to learn a fact or to support a clinical decision; 3) each question has a manually generated answer in which each sentence is linked to a PubMed abstract; 4) each question has 30 answers automatically generated by large language models. Each statement in the automatically generated answers is manually judged as required, unnecessary, or inappropriate. Each PubMed abstract provided by the models to support the specific statements is also manually judged as supporting, contradicting, or topically relevant or not to the answer. Finally, in each document that supports or contradicts the answer statements, a specific passage of text is annotated as evidence that supports the judgment. Figure [1](https://www.nature.com/articles/s41597-025-05233-z#Fig1) provides the workflow of the dataset creation and an example of a data entry.

**Fig. 1**

[![figure 1](https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41597-025-05233-z/MediaObjects/41597_2025_5233_Fig1_HTML.png)](https://www.nature.com/articles/s41597-025-05233-z/figures/1)

The schematic workflow of the MedAESQA dataset creation. The dataset creation starts with annotating the question frame ➊ and question metadata, which are the topic and narrative ➋. Thereafter, the medical information expert interacts with PubMed ➌ to retrieve the relevant PubMed articles and generates the reference answer ➍ with the appropriate references for each assertion in the reference answer. In the next step, two assessors formulated the nuggets ➎ from the reference answers. The question and metadata were given to the LLMs ➏, and the LLMs interact (optionally) with the PubMed collections and retrieve the relevant documents ➐, which they used to generate the answers with appropriate references. Once the answers are generated ➑, human assessors read the answers ➒, verify the references, and provide the multi-axis assessments ➓. The dotted arrow shows the optional interaction. The human assessors provided information is shown with a green arrow, while machine-generated answers are shown with a blue arrow.

[Full size image](https://www.nature.com/articles/s41597-025-05233-z/figures/1)

## Methods

### Question Formulation

The MedAESQA questions are developed using information requests submitted by self-identified non-clinicians to the MedlinePlus[11](https://www.nature.com/articles/s41597-025-05233-z#ref-CR11 "MedlinePlus [Internet]. National Library of Medicine (US) (1998 -). Last accessed 7 April 2025, Available from:                    https://medlineplus.gov                                    .") service provided by the National Library of Medicine. We chose the previously unseen, most popular, forty questions asked by MedlinePlus users. Each question also includes medical informatics expert-coded _topic_ and _narrative_ to support the efficient retrieval of the relevant documents. The _topic_ signifies the key subject (focus) of the question, whereas _narrative_ provides the context and background information on the question. Additionally, we provide an expert-coded structured representation of the information in the question, which we call question frame[12](https://www.nature.com/articles/s41597-025-05233-z#ref-CR12 "Kilicoglu, H. et al. Semantic annotation of consumer health questions. BMC bioinformatics 19, 1–28 (2018)."). Frame representation resembles a predicate-argument structure where a predicate is connected to its arguments and their semantic roles, such as THEME and AGENT[13](https://www.nature.com/articles/s41597-025-05233-z#ref-CR13 "Gildea, D. & Jurafsky, D. Automatic labeling of semantic roles. Computational linguistics 28, 245–288 (2002)."). A question frame includes a trigger for the question type, one or more THEME arguments, and optional semantic roles, all tied to their text mentions. In a question frame, _question focus_ is the main theme of the question, which describes the key entities of the question, and the _question type_ signifies the aspect of interest about the question focus (expected answer type). To characterize the disorders, user intents, body systems and anatomical structures, and clinical specialties encompassed by the collection, we also label each question with its ‘ _Subject Matter_’, i.e., the broad medical area, such as genetics, or focus on clinical drugs; ‘ _Body System_’, and ‘ _Specialty_’, i.e., a clinical expert best suited to answer the question, such as cardiology or endocrinology. These labels show that the MedAESQA dataset contains questions that look for a variety of answers: _treatment_, _effect_, _etiology_, etc. In our further fine-grained analysis on Body System on MedAESQA questions, we found that questions cover almost all the body systems, starting from ‘ _immune_’ to ‘ _vision_’. We also analyzed the Clinical Specialty of the medical specialist that would typically address or manage the issue, and we observed that MedAESQA questions cover an array of diverse specialties. We have provided the distribution of the question frame (Task, Answer type, and Subject Matter) and list of all Body Systems and Specialties of MedAESQA questions in Table [1](https://www.nature.com/articles/s41597-025-05233-z#Tab1) and [2](https://www.nature.com/articles/s41597-025-05233-z#Tab2), respectively.

**Table 1 Distribution of the different question frame categories in the MedAESQA dataset.**

[Full size table](https://www.nature.com/articles/s41597-025-05233-z/tables/1)

**Table 2 Body Systems and Clinical Specialties covered in questions present in the MedAESQA dataset.**

[Full size table](https://www.nature.com/articles/s41597-025-05233-z/tables/2)

### Expert-curated Answers

Following the work of Attal _et al_.[14](https://www.nature.com/articles/s41597-025-05233-z#ref-CR14 "Attal, K., Ondov, B. & Demner-Fushman, D. A dataset for plain language adaptation of biomedical abstracts. Scientific Data 10, 8 (2023)."), a medical information expert utilizes the question focus and answer type to query PubMed[15](https://www.nature.com/articles/s41597-025-05233-z#ref-CR15 "PubMed [Internet]. National Library of Medicine (US) (1996 -). Last accessed 7 April 2025, Available from:                    https://pubmed.ncbi.nlm.nih.gov                                    .") and retrieve articles that could potentially provide the answer to the question. In the next step of answer formulation, the expert reads the titles and abstracts of relevant articles and formulates an answer sentence by considering one or more abstracts. For each answer sentence, the expert also includes the appropriate PMIDs to provide evidence for the assertions stated in the answer sentence. By following the aforementioned strategy, an answer (with multiple sentences) is formulated in such a way that it remains complete, accurate, coherent, and evidence-supported with appropriate PubMed identifiers (PMIDs) for each assertion.

### Expert-curated Nuggets

Additionally, we provide manually generated information nuggets for factual evaluation. An information nugget can be used by an assessor to make a binary decision as to whether the fact represented by said nugget is contained in a response[16](https://www.nature.com/articles/s41597-025-05233-z#ref-CR16 "Voorhees, E. M. & Buckland, L. Overview of the trec 2003 question answering track. In TREC, vol. 2003, 54–68 (2003)."). An assessor may determine nuggets to be required for an answer and may match nuggets to the sentences that contain them. This allows for a finer level of granularity in the evaluation and the assessment of an atomic fact rather than a sentence as a whole. Nuggets were generated from the 40 expert-curated answers in the MedAESQA dataset, where exactly one nugget was generated for every fact contained in an answer. We used a Predicate (subject, object) form to capture the information nuggets. Each medical concept in a nugget is associated with a Concept Unique Identifier (CUI) from the Unified Medical Language System (UMLS)[17](https://www.nature.com/articles/s41597-025-05233-z#ref-CR17 "Lindberg, D. A., Humphreys, B. L. & McCray, A. T. The unified medical language system. Yearbook of medical informatics 2, 41–51 (1993)."). These CUIs were identified by manually searching the UMLS Metathesaurus Browser for the closest match. Some facts required more complex nugget structure including, but not limited to, “if, then” clauses and comparisons. An attempt was made to normalize language across answers with common predicates and formatting (e.g. Treat (treatment, condition) or Prevent (method of prevention, condition)), while retaining information from the original sentence as much as possible. Each nugget was reviewed by at least two reviewers.

### Machine-generated Answers

To generate answers with appropriate references, we organized a community evaluation[18](https://www.nature.com/articles/s41597-025-05233-z#ref-CR18 "Gupta, D., Demner-Fushman, D., Hersh, W., Bedrick, S. & Roberts, K. Overview of TREC 2024 biomedical generative retrieval (BioGen) track. In The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024) (2024). Last accessed 7 April 2025,                    https://trec.nist.gov/pubs/trec33/papers/Overview_biogen.pdf                                    .") at the 2024 Text Retrieval Conference (TREC). The participants proposed their approaches for generating answers. Analyzing the participants’ approaches, we outline a framework that takes an input question along with additional metadata (topic and narrative) and provides the generated answer as output.

The detailed steps include:

1. 1.
**Query Formulation and Expansion:** Given a topic, question, and narrative, a query is formulated to search PubMed articles to obtain the relevant documents from PubMed collection. The query can be formulated by considering either the topic, question, narrative, or any combination of these metadata available with each question. To improve the retrieval process query expansion can also be applied where a query is expanded or transformed with additional terms or phrases that are semantically related or contextually relevant.

2. 2.
**Document Retrieval**: To retrieve the relevant documents, the 2024 annual baseline snapshot of Medline/PubMed, which goes approximately through the end of 2023 was used. We provided a pre-processed set of 20, 727, 695 PMIDs representing the abstracts in the 2023 snapshot. The approaches have primarily used lexical retrieval (BM25[19](https://www.nature.com/articles/s41597-025-05233-z#ref-CR19 "Robertson, S. & Zaragoza, H. et al. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends in Information Retrieval 3, 333–389 (2009).")) to retrieve the top-k relevant documents for each question by utilizing the index built upon the title and abstract of the PubMed collection. The approaches also experimented with extracting the relevant snippets from the documents and considered the snippets as the relevant passages for the next stage of the framework.

3. 3.
**Document Reranking:** The ranking of the documents/snippets is an important step to further improve the ranking of documents retrieved in the first stage of the retrieval system. The goal is to reorder the retrieved documents to present the most relevant and high-quality results at the top of the list. Multiple re-rankers were utilized to re-rank the documents/snippets: pointwise (monoT5[20](https://www.nature.com/articles/s41597-025-05233-z#ref-CR20 "Nogueira, R., Jiang, Z., Pradeep, R. & Lin, J. Document ranking with a pretrained sequence-to-sequence model. In Findings of the Association for Computational Linguistics: EMNLP 2020, 708–718 (2020)."), TAS-B[21](https://www.nature.com/articles/s41597-025-05233-z#ref-CR21 "Hofstätter, S., Lin, S.-C., Yang, J.-H., Lin, J. & Hanbury, A. Efficiently teaching an effective dense retriever with balanced topic aware sampling. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, 113–122 (2021)."), ANCE[22](https://www.nature.com/articles/s41597-025-05233-z#ref-CR22 "Xiong, L. et al. Approximate nearest neighbor negative contrastive learning for dense text retrieval. In International Conference on Learning Representations (2020).")), pairwise (duoT5), and listwise (RankGPT[23](https://www.nature.com/articles/s41597-025-05233-z#ref-CR23 "Sun, W. et al. Is chatgpt good at search? investigating large language models as re-ranking agents. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 14918–14937 (2023).")) approaches to rerank the documents/snippets.

4. 4.
**Answer Generation:** The reranked documents along with the corresponding question were used to generate an answer to the question. Various open (Mistral-7B[24](https://www.nature.com/articles/s41597-025-05233-z#ref-CR24 "Jiang, A. Q. et al. Mistral 7B [Internet] (2023). Last accessed 7 April 2025,                    https://mistral.ai/news/announcing-mistral-7b                                    ."), Llama3.1[25](https://www.nature.com/articles/s41597-025-05233-z#ref-CR25 "Dubey, A. et al. The llama 3 herd of models [Internet] (2024). Last accessed 7 April 2025,                    https://ai.meta.com/research/publications/the-llama-3-herd-of-models/                                    .")) and closed-sourced (gemini-1.5-flash-001[26](https://www.nature.com/articles/s41597-025-05233-z#ref-CR26 "Gemini [Internet]. Google (2025). Last accessed 7 April 2025,                    https://blog.google/technology/ai/google-gemini-ai/                                    ."), gpt4o-mini, GPT-4o[27](https://www.nature.com/articles/s41597-025-05233-z#ref-CR27 "Achiam, J. et al. Gpt-4 technical report [Internet] (2023). Last accessed 7 April 2025,                    https://cdn.openai.com/papers/gpt-4.pdf                                    .")) LLMs were utilized to generate the answers, additionally, LLMs were instructed to cite the appropriate PMIDs of the ranked documents while stating the fact in the answer.

5. 5.
**Post-hoc Citations (Optional):** This is an optional alternative step in our framework, where an answer is generated first without referencing any documents, and in the post-hoc stage, each sentence is required to cite supporting documents. LLMs were employed to provide appropriate citations for each sentence from the reranked list of the documents.


### Human Judgment on Answers

We evaluated two different aspects of the answers: **(a)** reference attribution and **(b)** the quality and factuality of the answers. The former aims to judge the support the referred documents provide for an assertion generated by the machine and the latter focuses on evaluating the answer to a clinical question asked by clinicians to answer health-related questions asked by their patients. We envisioned that clinicians would review each answer and subsequently explain it in plain language. The evaluation was conducted by clinicians employed by Centaur Labs[28](https://www.nature.com/articles/s41597-025-05233-z#ref-CR28 "Centaur Labs [Internet]. Accurate and scalable health data labeling and model evaluation (2025). Last accessed 7 April 2025,                    https://centaur.ai                                    .") in a crowdsourcing manner. Each final judgment is a consensus on the majority vote of at least three annotators. We also computed the inter-annotator agreement score, which is defined as the percentage of annotators who assigned the majority label to the total number of annotators. We have provided the details of inter-annotator agreement scores on different annotation tasks in the creation of the MedAESQA dataset in the Table [3](https://www.nature.com/articles/s41597-025-05233-z#Tab3).

**Table 3 Detailed agreement scores on different annotation tasks in the creation of the MedAESQA dataset.**

[Full size table](https://www.nature.com/articles/s41597-025-05233-z/tables/3)

We follow a two-step judgment on machine-generated answers:

- **Step 1: Evaluating Answer Alignment with Questions and Answer Quality and Completeness:** We begin the evaluation by assessing whether the machine-generated text, as a whole, directly answers the question. In the next step, we examined the relevance of each assertion in the answer sentences to the question. Toward this, we categorized each assertion in the generated answer using one of the following four labels:


  - **Required**: Given assertion is necessary to have in the generated answer for completeness of the answer.

  - **Unnecessary**: Given assertion is not required to be included in the generated answer. An assertion can be categorized as unnecessary for multiple reasons including **(a)** it provides general information on the topic, **(b)** it recommends seeing a doctor, while the task states the patient has already contacted the provider, or the provider is asking the question.

  - **Borderline**: An assertion can be marked borderline, if it is relevant, possibly even -“good to know”,- but not required to be part of the answer. For example, if the question is about the most commonly used treatments, information about treatments in the early stages of clinical trials is not necessary.

  - **Inappropriate**: If an assertion may harm the patient if followed, it is marked as inappropriate. E.g., if, according to the answer, physical therapy reduces the pain level, but the patient experiences more pain due to hip mobilization, the patient may start doubting they are receiving adequate treatment.


We have provided examples of the borderline and unnecessary answer sentences for some healthcare questions in Table [4](https://www.nature.com/articles/s41597-025-05233-z#Tab4).

**Table 4 Sample examples showing the assertion relevance category for _borderline_ and _unnecessary_ labels.**

[Full size table](https://www.nature.com/articles/s41597-025-05233-z/tables/4)

- **Step 2: Evaluating Answer Alignment with Evidence Support:** In the second step, we evaluated the referenced document(s) for each generated answer sentence to determine the relationship between the document and the generated assertion, if any. Each cited document was labeled with one of four possible relationships to the answer sentence: _‘Supports’_, _‘Contradicts’_, _‘Neutral’_, or _‘Not Relevant’_. Additionally, the experts also provided a passage from the referenced document to support their assessment of the evidence relation.
  - **Supports**: A relation between the referenced document and the answer sentence is marked as _support_, if there is at least one sentence in the referenced document that supports/agrees with the assertion made in the answer sentence, e.g.: _“opioids were the mainstay of perioperative pain control”_. In addition, no other sentence in the document contradicts the statement.

  - **Contradicts**: A relation between the referenced document and the answer sentence is marked as _contradicts_ if there is at least one sentence in the referenced document that disagrees with the assertion or states its opposite, e.g.: _“Increasing pain levels after the first week postoperatively, for 3 days, are most likely to be caused by the change to more extensive mobilization and physiotherapy in the rehabilitation unit.”_ (The answer in this case stated that the pain decreases steadily after the surgery.)

  - **Neutral**: The referenced document is marked _neutral_, if it is topically relevant, but lacks any information to validate or invalidate the assertion made in the answer sentence.

  - **Not relevant**: The referenced document is considered _Not Relevant_ if the referenced document is not relevant to the sentence.

## Data Records

We have archived MedAESQA[29](https://www.nature.com/articles/s41597-025-05233-z#ref-CR29 "Gupta, D., Bartels, D. & Demner-Fushman, D. Medaesqa,                    https://doi.org/10.17605/OSF.IO/YDBZQ                                     (2025).") data records with Open Science Framework (OSF), available at [https://doi.org/10.17605/OSF.IO/ydbzq](https://doi.org/10.17605/OSF.IO/ydbzq). The OSF link contains a directory called MedAESQA. The MedAESQA directory contains the entire dataset in a JSON file, which lists data items. The README file also contains detailed information about each field in the dataset, including sample code to process the data. Each item in the JSON file contains the relevant key (metadata name) and value (metadata information) pairs, which are _question\_id_, _question_, _question\_frame_, _expert\_curated\_answer_, _expert\_curated\_nuggets_ and _machine\_generated\_answers_. The detailed statistics of the MedAESQA dataset are shown in Table [5](https://www.nature.com/articles/s41597-025-05233-z#Tab5). Figure [2](https://www.nature.com/articles/s41597-025-05233-z#Fig2), shows the JSON tree to visualize the data samples.

**Table 5 Detailed MedAESQA dataset statistics for questions, expert-curated answers, machine-generated answers, citations, and evidence.**

[Full size table](https://www.nature.com/articles/s41597-025-05233-z/tables/5)

**Fig. 2**

[![figure 2](https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41597-025-05233-z/MediaObjects/41597_2025_5233_Fig2_HTML.png)](https://www.nature.com/articles/s41597-025-05233-z/figures/2)

An example of a JSON tree for one of the data objects in the MedAESQA dataset. The illustration shows answers for only three machine-generation approaches M1, M2, and M3, however, the dataset has answers from 30 machine-generation approaches (M1 to M30) along with the citation assessment for each answer sentence in the generated answers. The answers and nuggets are truncated in the example.

[Full size image](https://www.nature.com/articles/s41597-025-05233-z/figures/2)

## Technical Validation

To generate multiple answers and validate the MedAESQA dataset, we organized a community evaluation[18](https://www.nature.com/articles/s41597-025-05233-z#ref-CR18 "Gupta, D., Demner-Fushman, D., Hersh, W., Bedrick, S. & Roberts, K. Overview of TREC 2024 biomedical generative retrieval (BioGen) track. In The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024) (2024). Last accessed 7 April 2025,                    https://trec.nist.gov/pubs/trec33/papers/Overview_biogen.pdf                                    .") at the 2024 Text Retrieval Conference (TREC), in which participants were provided with the questions and PubMed collection and asked to generate the answers in which the appropriate PMIDs supported each assertion (equated to sentences in this evaluation). We acknowledge that in this edition of the TREC evaluation, which resulted in the MedAESQA dataset, we limit ourselves to the title and abstract of the PubMed document, which may inhibit real-world usability of supporting the assertions. In the future, we plan to use the full-text PubMed document to assess the validity of the assertion. Participants were asked to generate the answer sentence that has to be supported by up to three attributions (cited references), with a maximum of 30 documents allowed per answer. Documents had to be cited in the answers using PMIDs enclosed in square brackets, as illustrated in Figure [1](https://www.nature.com/articles/s41597-025-05233-z#Fig1). For each question, we received thirty answers generated using different approaches. All the answer-generation approaches are depicted in the Appendix. The different strategies used by the participants offer diverse answers. Furthermore, each answer as a whole and the assertions with corresponding citations are manually evaluated by the experts, which provides the level of correctness of the machine-generated answers. We next describe the metrics we used to validate the answer quality, associations between the citations and assertions, and relevance of the cited documents.

### Benchmarking Metrics

We conducted a comprehensive evaluation of machine-generated answers across multiple levels and dimensions.

- **Answer Quality**: We evaluate the quality of the generated answers considering multiple perspectives: accuracy, precision, recall, and redundancy. The details of the metrics are as follows:
  - **Answer Accuracy** evaluates the accuracy of the generated answer using human-provided judgment. It measures how many of the answers to the total of 40 questions were deemed acceptable (judged as answering the question at least partially) for each answer generation approach.




    Accuracy=NumberofAcceptableAnswersTotalNumberofQuestions





    (1)


  - **Answer Completeness (Recall)** evaluates the extent to which a given answer covers the facts (aspects) deemed required by the assessors. The required aspects are aggregated across all system-generated answers. To identify answer aspects, we cluster the answer sentences using sentence embeddings generated by the SentenceTransformer model (sentence-transformers/all-mpnet-base-v2) and the SimCSE model (princeton-nlp/sup-simcse-roberta-large). We set up multiple evaluation levels for computing recall. In a strict evaluation, only sentences judged required and supported by evidence were considered for grouping. For a lenient evaluation, all sentences judged required were considered. For a relaxed evaluation, the borderline sentences were considered in addition to the required sentences. The number of aspects for the automated grouping is set to 10 using K-means clustering.




    Completeness=NumberofDistinctClustersContainingSentencesfromAnswerNumberofClusters





    (2)


  - **Answer Precision** assesses the proportion of the assertions that were judged required or acceptable in the answer.




    Precision=NumberofGeneratedRequiredSentencesTotalNumberofGeneratedSentences





    (3)


  - **Redundancy Score** quantifies _unnecessary_ answer sentences and penalizes a system for generating unnecessary sentences. This score measures the informativeness of the generated answers as a proportion of generated _unnecessary_ answer sentences among all generated answer sentences.




    RedundancyScore=NumberofGeneratedUnnecessarySentencesTotalNumberofGeneratedSentences





    (4)


  - **Irrelevancy Score** quantifies _inappropriate_/ _potentially harmful_ answer sentences and penalizes a system for generating these sentences. The score measures the potential of the generated answer to mislead the reader.




    IrrelevancyScore=NumberofGeneratedInappropriateSentencesTotalNumberofGeneratedSentences





    (5)
- **Citation Quality:** A system-generated answer statement may be supported or contradicted by the documents provided as references. It is also possible that answer sentences may not include any references or may include references that are only topically relevant or irrelevant. The following metrics are designed to assess the quality of these references:
  - **Citation Coverage** measures how well the required and borderline generated answer sentences are backed by the appropriate (judged as supports) citations.




    CitationCoverage=NumberofSystemsGeneratedAnswerSentenceswithOneorMoreSupportiveCitationTotalNumberofGeneratedAnswerSentences





    (6)


  - **Citation Support Rate** assesses the proportion of the citations provided by a system that were judged by the experts as supporting the corresponding statement in the generated answer.




    CitationSupportRate=NumberofSupportsCitationsTotalNumberofCitations





    (7)


  - **Citation Contradiction Rate** assesses the proportion of the citations provided by a system that were judged by the experts as contradicting the corresponding statement in the generated answer. In a fact-verification task, this measure can indicate how effectively a system identifies contradictory evidence.




    CitationContradictRate=NumberofContradictCitationsTotalNumberofCitations





    (8)
- **Document Relevancy:** By pooling all documents judged relevant to a given topic, we compute standard recall and precision. The set of relevant documents includes documents judged as supporting, contradicting, or neutral.




Recall=Numberofrelevantretrieveddocumentsallrelevantdocuments





(9)







Precision=NumberofrelevantretrieveddocumentsNumberofreferencesprovided





(10)



### MedAESQA Dataset Analysis

We performed a detailed analysis of the developed MedAESQA dataset by calculating dataset statistics at various levels of granularity. We found 1,108 out of 1,200 machine-generated answers were deemed acceptable by experts. We also analyzed the answer sentence relevancy assessed by the experts and found that 3,958 answer sentences were judged _required_ out of 5,162 generated answer sentences. Similarly, in our analysis of the evidence relation, we found 5489 references out of 8111 _supported_ the assertion made in the generated answers. The detailed analysis of different categories of answer sentence relevancy and evidence relation is presented in Fig. [3](https://www.nature.com/articles/s41597-025-05233-z#Fig3).

**Fig. 3**

[![figure 3](https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41597-025-05233-z/MediaObjects/41597_2025_5233_Fig3_HTML.png)](https://www.nature.com/articles/s41597-025-05233-z/figures/3)

Distribution of the assessed labels for answer accuracy **(a)**, answer sentence relevance **(b)**, and evidence relation **(c)** in MedAESQA dataset.

[Full size image](https://www.nature.com/articles/s41597-025-05233-z/figures/3)

The MedAESQA dataset comprises 40 questions along with their expert-curated and machine-generated answers. Each question is associated with 30 machine-generated answers, resulting in a total of 1,240 answers. For each machine-generated answer, a sentence-level assessment is conducted to evaluate answer accuracy, sentence relevance, and evidence relation. The dataset includes a total of 5,162 answer sentences. In the expert-curated answers, we identified 316 references, with a minimum of 3 and a maximum of 10 references per answer. On the other hand, the machine-generated answers yielded a minimum of 0 and a maximum of 31 references per answer. The MedAESQA dataset also contains a total of 7,651 human-curated evidence excerpts from referring documents to support the assessed evidence relations. A detailed analysis of the MedAESQA dataset is provided in Table [5](https://www.nature.com/articles/s41597-025-05233-z#Tab5).

### Benchmarking Evaluation

We evaluated the performance of methods used to generate answers in the created MedAESQA dataset on various evaluation metrics. The detailed results are presented in Tables [6](https://www.nature.com/articles/s41597-025-05233-z#Tab6), [7](https://www.nature.com/articles/s41597-025-05233-z#Tab7) and [8](https://www.nature.com/articles/s41597-025-05233-z#Tab8). On the answer accuracy metric, 26 out of 30 methods achieved more than 92% accuracy with a maximum of 100% (11 methods) and a minimum of 92.5% (2 methods). More than one-third of the methods (11 out of 30) achieved perfect accuracy which shows the acceptable quality of the generated answers. For precision of the answer, method M17 achieved the best performance with a precision score of 90.23. The precision score for 17 out of 30 methods was in the range of 73.54 to 85.54 and for 3 out of 30 methods, the precision score was in the range of 85.54 to 90.23. The redundancy scores were high (>15%) for only five methods. Method M4 recorded the lowest redundancy score of 4.04% with a precision of 79.08. Harmfulness is a key metric that aims to assess the tendency of the system to generate harmful sentences. We found that 19 of 30 methods achieved a perfect harmfulness score of 0%. The remaining methods also perform well and seem cautious while generating the answers as the highest harmfulness score we recorded was only 1.88. Nevertheless, we believe the harmful sentences are a good source for training and testing approaches for identifying and mitigating harmful answers.

**Table 6 Performance comparison of the multiple machine-generated answers regarding answer quality focusing on accuracy, precision, redundancy, and harmfulness metrics.**

[Full size table](https://www.nature.com/articles/s41597-025-05233-z/tables/6)

**Table 7 Performance comparison of the multiple machine-generated answers regarding answer quality focusing on completeness (recall) metric using two different approaches to form the cluster: Sentence Transformer and SimCSE.**

[Full size table](https://www.nature.com/articles/s41597-025-05233-z/tables/7)

**Table 8 Performance comparison of the multiple machine-generated answers regarding citation quality and document relevance on their respective metrics.**

[Full size table](https://www.nature.com/articles/s41597-025-05233-z/tables/8)

To measure the answer completeness (recall), we followed a clustering approach where we clustered all the generated answers for a given question together to assess the distinct aspects that are covered in the machine-generated answers. We utilized two different answer sentence representation approaches Sentence Transformer and SimCSE. The comparative results under three different settings (S+R, R, and R+B) are demonstrated in Table [7](https://www.nature.com/articles/s41597-025-05233-z#Tab7). S+R, which considers only supported and required facts, is the most strict evaluation and R+B, which includes all required and borderline statements disregarding the support, is the most lenient evaluation. We found that method M24 (Sentence Transfomer) obtained the best recall scores with 40.25 and 42.75 on the S+R and R+B settings respectively. A similar trend is also observed while using SimCSE as the sentence representation. Some methods recorded recall scores of 0 under S+R settings because those methods did not generate the citations for the generated answers.

While analyzing the results of the citation quality of the different approaches to the machine-generated answers, we observed that the majority of the approaches (18) achieved citation coverage in the range of 62 to 93. Method M1 recorded the highest citation coverage of 91.92 and M28 (excluding M25 and M27 as these methods did not generate citations along with the answer) as the lowest citation coverage of 6.38. For the citation support rate (CSR) the method M26 obtained the highest score of 77.88. Citation contradiction rate (CCR) is another key metric to evaluate citation quality as it assesses how often a system is citing a contradictory document with an assertion. We found some of the best CSR score systems, M6 and M26 recorded CCR values of 3.86 and 1.82 which signify that the answer generation systems were good in citing the appropriate documents. We also analyzed the document recall and precision scores and observed that most of the systems yield low recall however the precision scores were moderate. Method M24 achieved the highest document recall of 23.77 with a precision of 74.98.

#### Expert-curated Answers vs. Machine-generated Answers

We also analyze how close the machine-generated answers are to the expert-curated answers. Towards this, we analyze answer-level and citation-level similarities between two different modes of the answer-curation. For the answer-level similarity, we computed the sentence similarly between the expert-curated and each machine-generated answer and reported the BLEU, BLEU-4, ROUGE-2, and ROUGE-L metrics. Since an answer can be stated in different way, n-gram similarities may not always be the best choice to measure the similarity, therefore, we also reported the semantic similarity by reporting the BERTScore between the the expert-curated and each machine-generated answer. The detailed results are shown in Table [9](https://www.nature.com/articles/s41597-025-05233-z#Tab9). On n-gram similarities-based metrics, we found that method M11 achieved the highest BLEU and ROUGE scores (0.0117 and 0.1845) and comparable BERTScore (0.8514). The method M20 records competitive scores to M11 on both n-gram-based evaluation (BLEU: 0.117, ROUGE: 0.1825) and semantic similarity (BERTScore: 0.8517) metrics. For citation-level similarity, we created a list of cited documents in the answer to a given question for expert citation and machine-generated citation. We considered the expert citation list as the ground truth citation and computed the true positives (generated citations that are also present in the expert citation), false positives (generated citations that are not present in the expert citation), and false negatives (expert citations that are not present in the generated citation). With these, we computed the citation precision, recall, and f-score and reported the performance in Table [9](https://www.nature.com/articles/s41597-025-05233-z#Tab9). We found that method M1 achieved the highest citation F-score of 13.63, however, method M11 and M20 (best on answer level similarity) also recorded the competitive F-Score of 13.00 and 12.46 respectively.

**Table 9 Comparison of the different approaches of machine-generated answer to the expert-curated answer on answer level and citation level.**

[Full size table](https://www.nature.com/articles/s41597-025-05233-z/tables/9)

## Usage Notes

We have provided detailed instructions in the README file of the Open Science Framework repository ( [https://osf.io/ydbzq](https://osf.io/ydbzq)) describing how to process the MedAESQA datasets. The source code to evaluate the system performance can be found in the GitHub repository ( [https://github.com/deepaknlp/MedAESQA](https://github.com/deepaknlp/MedAESQA)).

## Code availability

The code to process the MedAESQA and evaluate the system can be found at GitHub ( [https://github.com/deepaknlp/MedAESQA](https://github.com/deepaknlp/MedAESQA)).

## References

01. Singhal, K. _et al_. Large language models encode clinical knowledge. _Nature_ **620**, 172–180 (2023).

    [Article](https://doi.org/10.1038%2Fs41586-023-06291-2) [ADS](http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&bibcode=2023Natur.620..172S) [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3sXhsVKju7zP) [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=37438534) [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC10396962) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Large%20language%20models%20encode%20clinical%20knowledge&journal=Nature&doi=10.1038%2Fs41586-023-06291-2&volume=620&pages=172-180&publication_year=2023&author=Singhal%2CK)

02. Demner-Fushman, D. & Lin, J. Answering clinical questions with knowledge-based and statistical techniques. _Computational Linguistics_ **33**, 63–103 (2007).

    [Article](https://doi.org/10.1162%2Fcoli.2007.33.1.63) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Answering%20clinical%20questions%20with%20knowledge-based%20and%20statistical%20techniques&journal=Computational%20Linguistics&doi=10.1162%2Fcoli.2007.33.1.63&volume=33&pages=63-103&publication_year=2007&author=Demner-Fushman%2CD&author=Lin%2CJ)

03. Bašaragin, B. _et al_. How do you know that? teaching generative language models to reference answers to biomedical questions. In Demner-Fushman, D., Ananiadou, S., Miwa, M., Roberts, K. & Tsujii, J. (eds.) _Proceedings of the 23rd Workshop on Biomedical Natural Language Processing_, 536–547, [https://doi.org/10.18653/v1/2024.bionlp-1.44](https://doi.org/10.18653/v1/2024.bionlp-1.44) (Association for Computational Linguistics, Bangkok, Thailand, 2024).

04. Biden, J. R. Executive order on the safe, secure, and trustworthy development and use of artificial intelligence (2023).

05. Lewis, P. _et al_. Retrieval-augmented generation for knowledge-intensive nlp tasks. _Advances in Neural Information Processing Systems_ **33**, 9459–9474 (2020).

    [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Retrieval-augmented%20generation%20for%20knowledge-intensive%20nlp%20tasks&journal=Advances%20in%20Neural%20Information%20Processing%20Systems&volume=33&pages=9459-9474&publication_year=2020&author=Lewis%2CP)

06. Gao, L. _et al_. Rarr: Researching and revising what language models say, using language models. In _The 61st Annual Meeting Of The Association For Computational Linguistics_ (2023).

07. Jain, P., Baldini Soares, L. & Kwiatkowski, T. From RAG to riches: Retrieval interlaced with sequence generation. In Al-Onaizan, Y., Bansal, M. & Chen, Y.-N. (eds.) _Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing_, 8887–8904, [https://doi.org/10.18653/v1/2024.emnlp-main.502](https://doi.org/10.18653/v1/2024.emnlp-main.502) (Association for Computational Linguistics, Miami, Florida, USA, 2024).

08. Kwiatkowski, T. _et al_. Natural questions: A benchmark for question answering research. _Transactions of the Association for Computational Linguistics_ **7**, 452–466, [https://doi.org/10.1162/tacl\_a\_00276](https://doi.org/10.1162/tacl_a_00276) (2019).

    [Article](https://doi.org/10.1162%2Ftacl_a_00276) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Natural%20questions%3A%20A%20benchmark%20for%20question%20answering%20research&journal=Transactions%20of%20the%20Association%20for%20Computational%20Linguistics&doi=10.1162%2Ftacl_a_00276&volume=7&pages=452-466&publication_year=2019&author=Kwiatkowski%2CT)

09. Krithara, A., Nentidis, A., Bougiatiotis, K. & Paliouras, G. Bioasq-qa: A manually curated corpus for biomedical question answering. _Scientific Data_ **10**, 170 (2023).

    [Article](https://doi.org/10.1038%2Fs41597-023-02068-4) [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=36973320) [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC10042099) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Bioasq-qa%3A%20A%20manually%20curated%20corpus%20for%20biomedical%20question%20answering&journal=Scientific%20Data&doi=10.1038%2Fs41597-023-02068-4&volume=10&publication_year=2023&author=Krithara%2CA&author=Nentidis%2CA&author=Bougiatiotis%2CK&author=Paliouras%2CG)

10. Savery, M., Abacha, A. B., Gayen, S. & Demner-Fushman, D. Question-driven summarization of answers to consumer health questions. _Scientific Data_ **7**, 322 (2020).

    [Article](https://doi.org/10.1038%2Fs41597-020-00667-z) [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=33009402) [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7532186) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Question-driven%20summarization%20of%20answers%20to%20consumer%20health%20questions&journal=Scientific%20Data&doi=10.1038%2Fs41597-020-00667-z&volume=7&publication_year=2020&author=Savery%2CM&author=Abacha%2CAB&author=Gayen%2CS&author=Demner-Fushman%2CD)

11. MedlinePlus \[Internet\]. National Library of Medicine (US) (1998 -). Last accessed 7 April 2025, Available from: [https://medlineplus.gov](https://medlineplus.gov/).

12. Kilicoglu, H. _et al_. Semantic annotation of consumer health questions. _BMC bioinformatics_ **19**, 1–28 (2018).

    [Article](https://link.springer.com/doi/10.1186/s12859-018-2045-1) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Semantic%20annotation%20of%20consumer%20health%20questions&journal=BMC%20bioinformatics&doi=10.1186%2Fs12859-018-2045-1&volume=19&pages=1-28&publication_year=2018&author=Kilicoglu%2CH)

13. Gildea, D. & Jurafsky, D. Automatic labeling of semantic roles. _Computational linguistics_ **28**, 245–288 (2002).

    [Article](https://doi.org/10.1162%2F089120102760275983) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Automatic%20labeling%20of%20semantic%20roles&journal=Computational%20linguistics&doi=10.1162%2F089120102760275983&volume=28&pages=245-288&publication_year=2002&author=Gildea%2CD&author=Jurafsky%2CD)

14. Attal, K., Ondov, B. & Demner-Fushman, D. A dataset for plain language adaptation of biomedical abstracts. _Scientific Data_ **10**, 8 (2023).

    [Article](https://doi.org/10.1038%2Fs41597-022-01920-3) [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=36599892) [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC9811873) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=A%20dataset%20for%20plain%20language%20adaptation%20of%20biomedical%20abstracts&journal=Scientific%20Data&doi=10.1038%2Fs41597-022-01920-3&volume=10&publication_year=2023&author=Attal%2CK&author=Ondov%2CB&author=Demner-Fushman%2CD)

15. PubMed \[Internet\]. National Library of Medicine (US) (1996 -). Last accessed 7 April 2025, Available from: [https://pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/).

16. Voorhees, E. M. & Buckland, L. Overview of the trec 2003 question answering track. In _TREC_, vol. 2003, 54–68 (2003).

17. Lindberg, D. A., Humphreys, B. L. & McCray, A. T. The unified medical language system. _Yearbook of medical informatics_ **2**, 41–51 (1993).

    [Article](https://doi.org/10.1055%2Fs-0038-1637976) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=The%20unified%20medical%20language%20system&journal=Yearbook%20of%20medical%20informatics&doi=10.1055%2Fs-0038-1637976&volume=2&pages=41-51&publication_year=1993&author=Lindberg%2CDA&author=Humphreys%2CBL&author=McCray%2CAT)

18. Gupta, D., Demner-Fushman, D., Hersh, W., Bedrick, S. & Roberts, K. Overview of TREC 2024 biomedical generative retrieval (BioGen) track. In _The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024)_ (2024). Last accessed 7 April 2025, [https://trec.nist.gov/pubs/trec33/papers/Overview\_biogen.pdf](https://trec.nist.gov/pubs/trec33/papers/Overview_biogen.pdf).

19. Robertson, S. & Zaragoza, H. _et al_. The probabilistic relevance framework: Bm25 and beyond. _Foundations and Trends in Information Retrieval_ **3**, 333–389 (2009).

    [Article](https://doi.org/10.1561%2F1500000019) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=The%20probabilistic%20relevance%20framework%3A%20Bm25%20and%20beyond&journal=Foundations%20and%20Trends%20in%20Information%20Retrieval&doi=10.1561%2F1500000019&volume=3&pages=333-389&publication_year=2009&author=Robertson%2CS&author=Zaragoza%2CH)

20. Nogueira, R., Jiang, Z., Pradeep, R. & Lin, J. Document ranking with a pretrained sequence-to-sequence model. In _Findings of the Association for Computational Linguistics: EMNLP 2020_, 708–718 (2020).

21. Hofstätter, S., Lin, S.-C., Yang, J.-H., Lin, J. & Hanbury, A. Efficiently teaching an effective dense retriever with balanced topic aware sampling. In _Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval_, 113–122 (2021).

22. Xiong, L. _et al_. Approximate nearest neighbor negative contrastive learning for dense text retrieval. In _International Conference on Learning Representations_ (2020).

23. Sun, W. _et al_. Is chatgpt good at search? investigating large language models as re-ranking agents. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, 14918–14937 (2023).

24. Jiang, A. Q. _et al_. Mistral 7B \[Internet\] (2023). Last accessed 7 April 2025, [https://mistral.ai/news/announcing-mistral-7b](https://mistral.ai/news/announcing-mistral-7b).

25. Dubey, A. _et al_. The llama 3 herd of models \[Internet\] (2024). Last accessed 7 April 2025, [https://ai.meta.com/research/publications/the-llama-3-herd-of-models/](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/).

26. Gemini \[Internet\]. Google (2025). Last accessed 7 April 2025, [https://blog.google/technology/ai/google-gemini-ai/](https://blog.google/technology/ai/google-gemini-ai/).

27. Achiam, J. _et al_. Gpt-4 technical report \[Internet\] (2023). Last accessed 7 April 2025, [https://cdn.openai.com/papers/gpt-4.pdf](https://cdn.openai.com/papers/gpt-4.pdf).

28. Centaur Labs \[Internet\]. Accurate and scalable health data labeling and model evaluation (2025). Last accessed 7 April 2025, [https://centaur.ai](https://centaur.ai/).

29. Gupta, D., Bartels, D. & Demner-Fushman, D. Medaesqa, [https://doi.org/10.17605/OSF.IO/YDBZQ](https://doi.org/10.17605/OSF.IO/YDBZQ) (2025).


[Download references](https://citation-needed.springer.com/v2/references/10.1038/s41597-025-05233-z?format=refman&flavour=references)

## Acknowledgements

This research was supported by the Intramural Research Program of the National Library of Medicine at the NIH. This work utilized the computational resources of the NIH HPC Biowulf cluster ( [https://hpc.nih.gov](https://hpc.nih.gov/)). Creation of this dataset would not have been possible without the LLM-generated answers provided by the teams that participated in the Text REtrieval Conference (TREC) hosted by the National Institute of Standards (NIST): The Information Engineering Lab (University of Queensland, CSIRO), Australia; IIUoT, University of Tsukuba, Japan; UR-IW, University of Regensburg, Webis, Friedrich-Schiller-Universität Jena and Leipzig University; H2oloo, University of Waterloo. The authors thank Srishti Kapur, Centaur Labs for expertly managing the manual evaluation process.

## Funding

Open access funding provided by the National Institutes of Health.

## Author information

### Authors and Affiliations

1. National Library of Medicine, National Institutes of Health, HHS, Bethesda, MD, USA

Deepak Gupta, Davis Bartels & Dina Demner-Fushman


Authors

1. Deepak Gupta


[View author publications](https://www.nature.com/search?author=Deepak%20Gupta)





Search author on:[PubMed](https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Deepak%20Gupta) [Google Scholar](https://scholar.google.co.uk/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=%22Deepak%20Gupta%22&as_publication=&as_ylo=&as_yhi=&as_allsubj=all&hl=en)

2. Davis Bartels


[View author publications](https://www.nature.com/search?author=Davis%20Bartels)





Search author on:[PubMed](https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Davis%20Bartels) [Google Scholar](https://scholar.google.co.uk/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=%22Davis%20Bartels%22&as_publication=&as_ylo=&as_yhi=&as_allsubj=all&hl=en)

3. Dina Demner-Fushman


[View author publications](https://www.nature.com/search?author=Dina%20Demner-Fushman)





Search author on:[PubMed](https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Dina%20Demner-Fushman) [Google Scholar](https://scholar.google.co.uk/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=%22Dina%20Demner-Fushman%22&as_publication=&as_ylo=&as_yhi=&as_allsubj=all&hl=en)


### Contributions

D.G. contributed to project design, implemented data processing code and pipelines conducted baseline and machine learning experiments, contributed to collection creation, and wrote and edited the manuscript. D.B. contributed to the collection creation and writing and editing of the manuscript. D.D.-F. conceived of the project led and contributed to data generation, contributed to writing and editing the manuscript, and otherwise provided feedback on all aspects of the study.

### Corresponding author

Correspondence to
[Deepak Gupta](mailto:deepak.gupta@nih.gov).

## Ethics declarations

### Competing interests

Dina Demner-Fushman is currently an Editorial Board Member for Scientific Data. She has not been involved with any of the editorial aspects or decisions for this publication.

## Additional information

**Publisher’s note** Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.

## Supplementary information

### [Dataset 1](https://static-content.springer.com/esm/art%3A10.1038%2Fs41597-025-05233-z/MediaObjects/41597_2025_5233_MOESM1_ESM.xlsx)

### [Supplementary Information](https://static-content.springer.com/esm/art%3A10.1038%2Fs41597-025-05233-z/MediaObjects/41597_2025_5233_MOESM2_ESM.pdf)

## Rights and permissions

**Open Access** This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit [http://creativecommons.org/licenses/by/4.0/](http://creativecommons.org/licenses/by/4.0/).

[Reprints and permissions](https://s100.copyright.com/AppDispatchServlet?title=A%20Dataset%20of%20Medical%20Questions%20Paired%20with%20Automatically%20Generated%20Answers%20and%20Evidence-supported%20References&author=Deepak%20Gupta%20et%20al&contentID=10.1038%2Fs41597-025-05233-z&copyright=The%20Author%28s%29&publication=2052-4463&publicationDate=2025-06-19&publisherName=SpringerNature&orderBeanReset=true&oa=CC%20BY)

## About this article

[![Check for updates. Verify currency and authenticity via CrossMark](<Base64-Image-Removed>)](https://crossmark.crossref.org/dialog/?doi=10.1038/s41597-025-05233-z)

### Cite this article

Gupta, D., Bartels, D. & Demner-Fushman, D. A Dataset of Medical Questions Paired with Automatically Generated Answers and Evidence-supported References.
_Sci Data_ **12**, 1035 (2025). https://doi.org/10.1038/s41597-025-05233-z

[Download citation](https://citation-needed.springer.com/v2/references/10.1038/s41597-025-05233-z?format=refman&flavour=citation)

- Received: 31 January 2025

- Accepted: 19 May 2025

- Published: 19 June 2025

- DOI: https://doi.org/10.1038/s41597-025-05233-z


### Share this article

Anyone you share the following link with will be able to read this content:

Get shareable link

Sorry, a shareable link is not currently available for this article.

Copy to clipboard

Provided by the Springer Nature SharedIt content-sharing initiative


### Subjects

- [Information technology](https://www.nature.com/subjects/information-technology)
- [Scientific data](https://www.nature.com/subjects/scientific-data)

Close bannerClose

![Nature Briefing AI and Robotics](https://www.nature.com/static/images/logos/nature-briefing-ai-and-robotics-logo-51b3cf6c52.svg)

Sign up for the _Nature Briefing: AI and Robotics_ newsletter — what matters in AI and robotics research, free to your inbox weekly.

Email address

Sign up

I agree my information will be processed in accordance with the _Nature_ and Springer Nature Limited [Privacy Policy](https://www.nature.com/info/privacy).

Close bannerClose

Get the most important science stories of the day, free in your inbox. [Sign up for Nature Briefing: AI and Robotics](https://www.nature.com/briefing/ai-and-robotics/?brieferEntryPoint=AIAndRoboticsBriefingBanner)

---

## Source 5: pmc.ncbi.nlm.nih.gov {#source-5}

**URL:** https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/
**Content Type:** Web Content
**Content Length:** 1,28,794 characters
**Scraped At:** 2025-06-29T12:37:49.902Z

**Detected Structure:** Tables

### Content:

[Skip to main content](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#main-content)

![](https://pmc.ncbi.nlm.nih.gov/static/img/icon-dot-gov.svg)

**Official websites use .gov**

A
**.gov** website belongs to an official
government organization in the United States.


![](https://pmc.ncbi.nlm.nih.gov/static/img/icon-https.svg)

**Secure .gov websites use HTTPS**

A **lock** (
Lock
Locked padlock icon
) or **https://** means you've safely
connected to the .gov website. Share sensitive
information only on official, secure websites.


[Home](https://pmc.ncbi.nlm.nih.gov/ "Home")

Search PMC Full-Text Archive

Search in PMC![Search](https://pmc.ncbi.nlm.nih.gov/static/img/usa-icons-bg/search--white.svg)

- [Advanced Search](https://www.ncbi.nlm.nih.gov/pmc/advanced/)
- [Journal List](https://pmc.ncbi.nlm.nih.gov/journals/)
- [User Guide](https://pmc.ncbi.nlm.nih.gov/about/userguide/)

NewTry this search in PMC Beta Search

- [View on publisher site](https://doi.org/10.1038/s41591-024-03113-4) View on publisher site
- [Download PDF](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/pdf/41591_2024_Article_3113.pdf) Download PDF
- Add to Collections
- Cite
- Permalink




## PERMALINK



Copy


As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,
the contents by NLM or the National Institutes of Health.

Learn more:
[PMC Disclaimer](https://pmc.ncbi.nlm.nih.gov/about/disclaimer/)
\|
[PMC Copyright Notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)

![Nature Portfolio logo](https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-npgopen.png)

Nat Med

. 2024 Jun 28;30(10):2838–2848. doi: [10.1038/s41591-024-03113-4](https://doi.org/10.1038/s41591-024-03113-4)

# The limits of fair medical imaging AI in real-world generalization

[Yuzhe Yang](https://pubmed.ncbi.nlm.nih.gov/?term=%22Yang%20Y%22%5BAuthor%5D)

### Yuzhe Yang

1Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA USA

Find articles by [Yuzhe Yang](https://pubmed.ncbi.nlm.nih.gov/?term=%22Yang%20Y%22%5BAuthor%5D)

1,✉,#, [Haoran Zhang](https://pubmed.ncbi.nlm.nih.gov/?term=%22Zhang%20H%22%5BAuthor%5D)

### Haoran Zhang

1Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA USA

Find articles by [Haoran Zhang](https://pubmed.ncbi.nlm.nih.gov/?term=%22Zhang%20H%22%5BAuthor%5D)

1,#, [Judy W Gichoya](https://pubmed.ncbi.nlm.nih.gov/?term=%22Gichoya%20JW%22%5BAuthor%5D)

### Judy W Gichoya

2Department of Radiology, Emory University School of Medicine, Atlanta, GA USA

Find articles by [Judy W Gichoya](https://pubmed.ncbi.nlm.nih.gov/?term=%22Gichoya%20JW%22%5BAuthor%5D)

2, [Dina Katabi](https://pubmed.ncbi.nlm.nih.gov/?term=%22Katabi%20D%22%5BAuthor%5D)

### Dina Katabi

1Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA USA

Find articles by [Dina Katabi](https://pubmed.ncbi.nlm.nih.gov/?term=%22Katabi%20D%22%5BAuthor%5D)

1, [Marzyeh Ghassemi](https://pubmed.ncbi.nlm.nih.gov/?term=%22Ghassemi%20M%22%5BAuthor%5D)

### Marzyeh Ghassemi

1Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA USA

3Institute for Medical Engineering & Science, Massachusetts Institute of Technology, Cambridge, MA USA

Find articles by [Marzyeh Ghassemi](https://pubmed.ncbi.nlm.nih.gov/?term=%22Ghassemi%20M%22%5BAuthor%5D)

1,3

- Author information
- Article notes
- Copyright and License information

1Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA USA

2Department of Radiology, Emory University School of Medicine, Atlanta, GA USA

3Institute for Medical Engineering & Science, Massachusetts Institute of Technology, Cambridge, MA USA

✉

Corresponding author.

#

Contributed equally.

Received 2023 Dec 8; Accepted 2024 Jun 5; Issue date 2024.

© The Author(s) 2024

**Open Access** This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit [http://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/).

[PMC Copyright notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)

PMCID: PMC11485237  PMID: [38942996](https://pubmed.ncbi.nlm.nih.gov/38942996/)

## Abstract

As artificial intelligence (AI) rapidly approaches human-level performance in medical imaging, it is crucial that it does not exacerbate or propagate healthcare disparities. Previous research established AI’s capacity to infer demographic data from chest X-rays, leading to a key concern: do models using demographic shortcuts have unfair predictions across subpopulations? In this study, we conducted a thorough investigation into the extent to which medical AI uses demographic encodings, focusing on potential fairness discrepancies within both in-distribution training sets and external test sets. Our analysis covers three key medical imaging disciplines—radiology, dermatology and ophthalmology—and incorporates data from six global chest X-ray datasets. We confirm that medical imaging AI leverages demographic shortcuts in disease classification. Although correcting shortcuts algorithmically effectively addresses fairness gaps to create ‘locally optimal’ models within the original data distribution, this optimality is not true in new test settings. Surprisingly, we found that models with less encoding of demographic attributes are often most ‘globally optimal’, exhibiting better fairness during model evaluation in new test environments. Our work establishes best practices for medical imaging models that maintain their performance and fairness in deployments beyond their initial training contexts, underscoring critical considerations for AI clinical deployments across populations and sites.

**Subject terms:** Diagnosis, Radiography

* * *

When tested across tasks, diseases and imaging modalities, the performance of AI models depends on encoding of demographic shortcuts, and correcting for them decreases their ability to generalize in new populations.

## Main

As artificial intelligence (AI) models are increasingly deployed in real-world clinical settings[1](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR1), [2](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR2), it is crucial to evaluate not only model performance but also potential biases toward specific demographic groups[3](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR3), [4](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR4). Although deep learning has achieved human-level performance in numerous medical imaging tasks[5](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR5), [6](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR6), existing literature indicates a tendency for these models to manifest existing biases in the data, causing performance disparities between protected subgroups[7](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR7)– [11](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR11). For instance, chest X-ray (CXR) classifiers trained to predict the presence of disease systematically underdiagnose Black patients[12](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR12), potentially leading to delays in care. To ensure the responsible and equitable deployment of such models, it is essential to understand the source of such biases and, where feasible, take actions to correct them[13](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR13), [14](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR14).

Recent studies have unveiled the surprising ability of deep models to predict demographic information, such as self-reported race[15](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR15), sex and age[16](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR16), from medical images, achieving performance far beyond that of radiologists. These insights raise the concern of disease prediction models leveraging demographic features as heuristic ‘shortcuts’[17](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR17), [18](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR18)—correlations that are present in the data but have no real clinical basis[18](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR18), for instance deep models using the hospital as a shortcut for disease prediction[19](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR19), [20](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR20).

In this work, we investigated four questions. First, we consider whether disease classification models also use demographic information as shortcuts and whether such demographic shortcuts result in biased predictions. Second, we evaluate the extent to which state-of-the-art methods can remove such shortcuts and create ‘locally optimal’ models that are also fair. Third, we consider real-world clinical deployment settings where shortcuts may not be valid in the out-of-distribution (OOD) data, to dissect the interplay between algorithmic fairness and shortcuts when data shift. Finally, we explore which algorithms and model selection criteria can lead to ‘globally optimal’ models that maintain fairness when deployed in an OOD setting.

We performed a systematic investigation into how medical AI leverages demographic shortcuts through these questions, with an emphasis on fairness disparities across both in-distribution (ID) training and external test sets. Our primary focus is on CXR prediction models, with further validation in dermatology (Extended Data Fig. [1](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig6)) and ophthalmology (Extended Data Fig. [2](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig7)). Our X-ray analysis draws upon six extensive, international radiology datasets: MIMIC-CXR[21](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR21), CheXpert[22](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR22), NIH[23](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR23), SIIM[24](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR24), PadChest[25](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR25) and VinDr[26](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR26). We explored fairness within both individual and intersectional subgroups spanning race, sex and age[12](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR12). Our assessment uncovers compelling new insights into how medical AI encodes demographics and the impact that this has on various fairness considerations, especially when models are applied outside their training context during real-world domain shifts, with actionable insights on what models to select for fairness under distribution shift.

### Extended Data Fig. 1. Evaluation results for models on the ISIC dataset for No Finding prediction.

[![Extended Data Fig. 1](https://cdn.ncbi.nlm.nih.gov/pmc/blobs/eb1c/11485237/fe0996779a47/41591_2024_3113_Fig6_ESM.jpg)](https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&p=PMC3&id=11485237_41591_2024_3113_Fig6_ESM.jpg)

[Open in a new tab](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/figure/Fig6/)

**a**, Dataset statistics. **b**, Subgroup fairness gaps of the ERM model as defined by FPR. Each subgroup contains at least 100 samples for analysis (age: subgroup ‘60–80’ vs ‘18–40’; sex: subgroup ‘female’ vs ‘male’). We find that disparities in FPR are small and statistically insignificant in the case of age. Each bar and its error bar indicate the mean and standard deviation across 3 independent runs. **c**, Trade-off between the fairness gap and overall AUROC for all trained models, evaluated against sensitive attribute age and sex, respectively. We find that most models, including ERM, achieve a good fairness-performance tradeoff.

### Extended Data Fig. 2. Evaluation results for models on the ODIR dataset for Retinopathy prediction.

[![Extended Data Fig. 2](https://cdn.ncbi.nlm.nih.gov/pmc/blobs/eb1c/11485237/6e474321628d/41591_2024_3113_Fig7_ESM.jpg)](https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&p=PMC3&id=11485237_41591_2024_3113_Fig7_ESM.jpg)

[Open in a new tab](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/figure/Fig7/)

**a**, Dataset statistics. **b**, Subgroup fairness gaps of the ERM model as defined by FNR. Each subgroup contains at least 100 samples for analysis (age: subgroup ‘60–80’ vs ‘18–40’; sex: subgroup ‘female’ vs ‘male’). We find a significant FNR gap between age groups. Each bar and its error bar indicate the mean and standard deviation across 3 independent runs. **c**, Trade-off between the fairness gap and overall AUROC for all trained models, evaluated against sensitive attribute age and sex, respectively. We find, similar to the chest X-ray setting, that algorithms for fixing demographic shortcuts could improve in-distribution fairness while incurring minimal detriments to performance.

## Results

### Datasets and model training

We used six publicly available CXR datasets, as described in Table [1](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Tab1). We focused on four binary classification tasks that have been shown to have disparate performance between protected groups[7](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR7), [27](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR27): ‘No Finding’, ‘Effusion’, ‘Pneumothorax’ and ‘Cardiomegaly’. The detailed prevalence rates of the diseases for each demographic subgroup are shown in Extended Data Table [1](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Tab2).

#### Table 1.

Demographic and label characteristics of the six X-ray datasets used in this study

|  |  | MIMIC | CheXpert | NIH | SIIM | PadChest | VinDr |
| --- | --- | --- | --- | --- | --- | --- | --- |
|  | Location | Boston, MA | Stanford, CA | Bethesda, MD | Bethesda, MD | Alicante, Spain | Hanoi, Vietnam |
| --- | --- | --- | --- | --- | --- | --- | --- |
| No. of images | 357,167 | 222,792 | 112,120 | 11,582 | 144,478 | 6,354 |
| --- | --- | --- | --- | --- | --- | --- |
| Percent frontal | 64.5 | 85.5 | 100.0 | 100.0 | 69.1 | 100.0 |
| --- | --- | --- | --- | --- | --- | --- |
| Sex (%) | Female | 47.8 | 40.7 | 43.5 | 44.6 | 50.4 | 43.1 |
| Male | 52.2 | 59.3 | 56.5 | 55.4 | 49.6 | 56.9 |
| Race (%) | Asian | 3.1 | 10.5 | – | – | – | – |
| Black | 15.6 | 5.4 | – | – | – | – |
| White | 61.0 | 56.4 | – | – | – | – |
| Other | 20.3 | 27.8 | – | – | – | – |
| Age (%) | 0–18 | – | – | 4.8 | 5.0 | 3.7 | 21.8 |
| 18–40 | 13.8 | 13.9 | 27.7 | 27.3 | 9.2 | 16.0 |
| 40–60 | 31.1 | 31.1 | 43.9 | 42.9 | 26.5 | 27.1 |
| 60–80 | 40.0 | 39.0 | 22.7 | 23.9 | 38.0 | 30.0 |
| 80–100 | 15.1 | 16.0 | 0.9 | 0.9 | 22.6 | 5.1 |
| Intersection (%) | Asian female | 1.5 | 4.5 | – | – | – | – |
| Asian male | 1.6 | 6.0 | – | – | – | – |
| Black female | 9.3 | 2.6 | – | – | – | – |
| Black male | 6.3 | 2.7 | – | – | – | – |
| White female | 27.3 | 22.2 | – | – | – | – |
| White male | 33.8 | 34.1 | – | – | – | – |
| Others female | 9.8 | 11.3 | – | – | – | – |
| Others male | 10.5 | 16.5 | – | – | – | – |
| Task prevalence (%) | No Finding | 39.8 | 10.0 | 53.8 | – | 34.9 | 41.2 |
| Effusion | 20.0 | 38.6 | 11.9 | – | 5.9 | 7.5 |
| Pneumothorax | 3.4 | 8.7 | 4.7 | 28.4 | 0.3 | 0.7 |
| Cardiomegaly | 14.9 | 12.1 | 2.5 | – | 9.5 | 22.6 |

[Open in a new tab](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/table/Tab1/)

#### Extended Data Table 1.

Prevalence rates for each demographic subgroup of the datasets used for training models in this study

[![graphic file with name 41591_2024_3113_Tab1_ESM.jpg](https://cdn.ncbi.nlm.nih.gov/pmc/blobs/eb1c/11485237/a7e886dc2f36/41591_2024_3113_Tab1_ESM.jpg)](https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&p=PMC3&id=11485237_41591_2024_3113_Tab1_ESM.jpg)

[Open in a new tab](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/table/Tab2/)

We also examined medical AI applications in dermatology and ophthalmology. Specifically, we used the ISIC dataset[28](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR28) with ‘No Finding’ as the task for dermatological imaging (Extended Data Fig. [1a](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig6)) and the ODIR dataset[29](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR29) with ‘Retinopathy’ as the task for ophthalmology images (Extended Data Fig. [2a](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig7)).

To evaluate fairness, we examined the class-conditioned error rate that is likely to lead to worse patient outcomes for a screening model. For ‘No Finding’, a false positive indicates falsely predicting that a patient is healthy when they are ill, which could lead to delays in treatment[12](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR12); we, therefore, evaluated the differences in false-positive rate (FPR) between demographic groups. For all other diseases, we evaluated the false-negative rate (FNR) for the same reason. Equality in these metrics is equivalent to equality of opportunity[30](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR30). We choose to study fairness through the notion of equalized odds, as it has been widely used in previous work in the CXR and fairness literature[7](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR7), [12](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR12). In addition, shortcut learning using a particular demographic attribute leads to differences in class-conditioned error rates (that is, FPR and FNR gaps) across attributes[31](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR31), [32](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR32), and so studying these gaps allows us to glean insight into the severity of shortcut learning. Finally, FPR and FNR (as enforced to be equal by equalized odds) are meaningful metrics in the clinical setting, as they correspond to error rates of decision-making at the individual level[12](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR12).

To understand and quantify the types and degrees of distribution shifts in our study, we examined whether there are significant statistical differences in distributions between demographic groups in the ID settings as well as across different datasets in the OOD settings. Specifically, we analyzed prevalence shifts P(Y\|A) and representation shifts P(X\|A) across different subgroups for ID scenarios and added label shifts P(Y) and covariate shifts P(X) for OOD scenarios ( [Methods](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Sec11)). Our analyses indicate that all the distributions that we examined show statistically significant shifts, affecting most demographic groups in the ID context (Extended Data Table [2](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Tab3)) and across various sites in the OOD context (Extended Data Table [3](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Tab4)). We note that our analysis does not presuppose specific types of distribution shifts; instead, we simulated real-world deployment conditions where any of these shifts might occur, aiming for results that are generalizable to complex, real-world scenarios.

#### Extended Data Table 2.

Distribution shift quantification in the ID settings between demographic groups on MIMIC-CXR

[![graphic file with name 41591_2024_3113_Tab2_ESM.jpg](https://cdn.ncbi.nlm.nih.gov/pmc/blobs/eb1c/11485237/7fba84d0572f/41591_2024_3113_Tab2_ESM.jpg)](https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&p=PMC3&id=11485237_41591_2024_3113_Tab2_ESM.jpg)

[Open in a new tab](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/table/Tab3/)

**a**, Prevalence shift P(Y\|A) was derived using the total variational distance between the probability distributions of Y conditioned on different groups. _P_ values were computed using a two-sided proportion _z_-test. **b**, Representation shift P(X\|A) was derived by first encoding input into representations from a frozen foundation model _f_ (that is, MedCLIP[79](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR79)) and then computing the MMD distance with a Gaussian kernel[81](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR81). _P_ values were computed using a two-sided permutation test using this distance as the test statistic[81](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR81). All _P_ values were adjusted for multiple testing using Bonferroni correction[78](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR78).

#### Extended Data Table 3.

Distribution shift quantification in the OOD settings between different datasets

[![graphic file with name 41591_2024_3113_Tab3_ESM.jpg](https://cdn.ncbi.nlm.nih.gov/pmc/blobs/eb1c/11485237/5eb6488e525a/41591_2024_3113_Tab3_ESM.jpg)](https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&p=PMC3&id=11485237_41591_2024_3113_Tab3_ESM.jpg)

[Open in a new tab](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/table/Tab4/)

**a**, Label shift P(Y) was derived using the total variational distance between the probability distributions of Y between ID and OOD datasets. _P_ values were computed using a two-sided proportion _z_-test. **b**, Covariate shift P(X) was derived by first encoding input into representations from a frozen foundation model _f_ (that is, MedCLIP[79](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR79)) and then computing the MMD distance with a Gaussian kernel[81](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR81) between ID and OOD datasets. _P_ values were computed using a two-sided permutation test using this distance as the test statistic[81](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR81). **c**, Prevalence shift P(Y\|A = a) was derived using the total variational distance conditioned on specific subgroups between ID and OOD datasets. _P_ values were computed using a two-sided proportion _z_-test. **d**, Representation shift P(X\|A = a) was derived using the MMD distance on specific subgroups between ID and OOD datasets[81](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR81). _P_ values were computed using a two-sided permutation test using this distance as the test statistic[81](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR81). All _P_ values were adjusted for multiple testing using Bonferroni correction[78](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR78).

We trained a grid of deep convolutional neural networks[33](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR33) on MIMIC-CXR (radiology), CheXpert (radiology), ODIR (ophthalmology) and ISIC (dermatology), varying the classification task. Our approach follows previous work that achieves state-of-the-art performance in these tasks[8](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR8), [12](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR12) using empirical risk minimization (ERM)[34](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR34). We also evaluated algorithms designed to remove spurious correlations or increase model fairness during training. We categorized these algorithms into those that (1) reweight samples based on their group to combat underrepresentation (ReSample[35](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR35) and GroupDRO[36](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR36)); (2) adversarially remove group information from model representations (DANN[37](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR37) and CDANN[38](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR38)); and (3) more generically attempt to improve model generalization—that is, exponential moving average (MA[39](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR39)). In total, our analysis encompassed a total of 3,456 models trained on MIMIC-CXR, corresponding to the cartesian product of four tasks, four demographic attributes, six algorithms, 12 hyperparameter settings and three random seeds. We summarized our experimental pipeline in Fig. [1](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig1).

#### Fig. 1. Overall experimental pipeline.

[![Fig. 1](https://cdn.ncbi.nlm.nih.gov/pmc/blobs/eb1c/11485237/69809af9b9a8/41591_2024_3113_Fig1_HTML.jpg)](https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&p=PMC3&id=11485237_41591_2024_3113_Fig1_HTML.jpg)

[Open in a new tab](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/figure/Fig1/)

**a**, We trained a grid of deep learning models on medical images from a variety of modalities on several clinical tasks. We applied a variety of state-of-the-art algorithms to mitigate shortcuts, for up to four demographic attributes (where available). **b**, We evaluated each model ID (that is, on the same dataset where it is trained), along the axis of performance, fairness, amount of demographic encoded and calibration. **c**, We evaluated the performance and fairness of CXR classification models on OOD domains. To mimic a realistic deployment setting where OOD samples are not observed, we chose the ‘best’ model based on several ID selection criteria.

### Algorithmic encoding of attributes leads to fairness gaps

We separately trained deep learning models for our four distinct CXR prediction tasks (‘No Finding’, ‘Cardiomegaly’, ‘Effusion’ and ‘Pneumothorax’) as well as ‘Retinopathy’ in ophthalmology and ‘No Finding’ in dermatology. Each model consists of a feature extractor followed by a disease prediction head. We then employed a transfer learning approach, wherein we kept the weights of the feature extractor frozen and retrained the model to predict sensitive attributes (for example, race). This allowed us to assess the amount of attribute-related information present in the features learned by each model as measured by the area under the receiver operating characteristic curve (AUROC) for attribute prediction ( [Methods](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Sec11)). Previous work[15](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR15), [40](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR40) demonstrated that deep models trained for disease classification encode demographic attributes, and such encoding could lead to algorithmic bias[41](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR41). We extend the investigation to a broader array of datasets, attributes and imaging modalities. As Fig. [2a,c,e](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig2) confirms, the penultimate layer of different disease models contains significant information about four demographic attributes (age, race, sex and the intersection of sex and race), and that is consistent across different tasks and medical imaging modalities.

#### Fig. 2. Medical imaging models encode sensitive attributes and are unfair across subgroups.

[![Fig. 2](https://cdn.ncbi.nlm.nih.gov/pmc/blobs/eb1c/11485237/5273826efe19/41591_2024_3113_Fig2_HTML.jpg)](https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&p=PMC3&id=11485237_41591_2024_3113_Fig2_HTML.jpg)

[Open in a new tab](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/figure/Fig2/)

**a**, The AUROC of demographic attribute prediction from frozen representations for the best ERM model. We trained ERM models on MIMIC-CXR to predict four different binary tasks. ERM representations encode demographic attributes to a high degree. **b**, The fairness gap, as defined by the FPR gap for ‘No Finding’, and the FNR gap for all other tasks for the best ERM model. ERM models exhibit high fairness gaps, especially between age groups. **c**, The AUROC of demographic attribute prediction from frozen representations for the best ERM model on the ODIR dataset (ophthalmology), following the same experimental setup. **d**, The fairness gap for the best ERM model on the ODIR dataset (ophthalmology). **e**, The AUROC of demographic attribute prediction from frozen representations for the best ERM model on the ISIC dataset (dermatology), following the same experimental setup. **f**, The fairness gap for the best ERM model on the ISIC dataset (dermatology). **a**– **f**, Each bar and its error bar indicate the mean and standard deviation across three independent runs. **g**, The correlation between attribute prediction performance and fairness for all learned models. We excluded models with suboptimal performance—that is, with an overall validation AUROC below 0.7. The attribute prediction AUROC shows a high correlation with the fairness gap (‘No Finding’, age: _R_ = 0.82, _P_ = 4.7 × 10−8; ‘No Finding’, sex and race: _R_ = 0.81, _P_ = 8.4 × 10−9; ‘Cardiomegaly’, age: _R_ = 0.81, _P_ = 1.9 × 10−7; ‘Effusion’, race: _R_ = 0.71, _P_ = 6.4 × 10−6; ‘Pneumothorax’, sex: _R_ = 0.59, _P_ = 2.3 × 10−3; all using two-sided _t_-test). The center line and the shadow denote the mean and 95% CI, respectively.

We then assessed the fairness of these models across demographic subgroups as defined by equal opportunity[30](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR30)—that is, discrepancies in the model’s FNR or FPR for demographic attributes. We focused on underdiagnosis[12](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR12)—that is, discrepancies in FPR for ‘No Finding’ and discrepancies in FNR for other diseases. For each demographic attribute, we identified two key subgroups with sufficient sample sizes: age groups ‘80–100’ ( _n_ = 8,063) and ‘18–40’ ( _n_ = 7,319); race groups ‘White’ ( _n_ = 32,732) and ‘Black’ ( _n_ = 8,279); sex groups ‘female’ ( _n_ = 25,782) and ‘male’ ( _n_ = 27,794); and sex and race groups ‘White male’ ( _n_ = 18,032) and ‘Black female’ ( _n_ = 5,027). In all tasks, we observed that the models displayed biased performance within the four demographic attributes, as evidenced by the FNR disparities (Fig. [2b](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig2)). The observed gaps can be as large as 30% for age. The same results hold for the other two imaging modalities (Fig. [2d,f](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig2)). Similar results for overdiagnosis (FNR of ‘No Finding’ and FPR for disease prediction) can be found in Extended Data Fig. [3](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig8).

#### Extended Data Fig. 3. Trade-offs between the FPR gap and the FNR gap for each task and attribute, for models trained on MIMIC-CXR or CheXpert and evaluated on the same dataset for (a) No Finding and (b) Effusion prediction.

[![Extended Data Fig. 3](https://cdn.ncbi.nlm.nih.gov/pmc/blobs/eb1c/11485237/14c1bab31916/41591_2024_3113_Fig8_ESM.jpg)](https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&p=PMC3&id=11485237_41591_2024_3113_Fig8_ESM.jpg)

[Open in a new tab](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/figure/Fig8/)

We evaluate these metrics across age (‘80–100’ vs ‘18–40’), sex (‘female’ vs ‘male’), race (‘White’ vs. ‘Black’), and the intersection of sex and race (‘White male’ vs. ‘Black female’). We find for the most part, there is a positive correlation, indicating that fairer models achieve fairness with respect to both FPR and FNR (that is, equal odds). All p values are calculated using two-sided t-test. The center line and the shadow denote the mean and 95% CI, respectively.

We further investigated the degree to which demographic attribute encoding ‘shortcuts’ may impact model fairness. When models use demographic variables as shortcuts, previous work showed that they can exhibit gaps in subgroup FPR and FNR[31](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR31), [40](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR40). We note that a model encoding demographic information does not necessarily imply a fairness violation, as the model may not necessarily use this information for its prediction. For each task and attribute combination, we trained different models with varying hyperparameters ( [Methods](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Sec11)). We focused on the correlation between the degree of encoding of different attributes and the fairness gaps as assessed by underdiagnosis. Figure [2g](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig2) shows that a stronger encoding of demographic information is significantly correlated with stronger model unfairness (‘No Finding’, age: _R_ = 0.82, _P_ = 4.7 × 10−8; ‘No Finding’, sex and race: _R_ = 0.81, _P_ = 8.4 × 10−9; ‘Cardiomegaly’, age: _R_ = 0.81, _P_ = 1.9 × 10−7; ‘Effusion’, race: _R_ = 0.71, _P_ = 6.4 × 10−6; ‘Pneumothorax’, sex: _R_ = 0.59, _P_ = 2.3 × 10−3; all using two-sided _t_-test). Such consistent observations indicate that models using demographic encodings as heuristic shortcuts also have larger fairness disparities, as measured by discrepancies in FPR and FNR.

### Mitigating shortcuts creates locally optimal models

We performed model evaluations first in the ID setting, where ERM models trained and tested on data from the same source performed well. We compared ERM to state-of-the-art robustness methods that were designed to effectively address fairness gaps while maintaining overall performance. As shown in Fig. [3a](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig3), ERM models exhibited large fairness gaps across age groups when predicting ‘Cardiomegaly’ (that is, models centered in the top right corner, FNR gap of 20% between groups ‘80–100’ and ‘18–40’). By applying data rebalancing methods to address prevalence shifts during training (for example, ReSample), we observed reduced fairness gaps in certain contexts. By applying debiasing robustness methods that correct demographic shortcuts, such as GroupDRO and DANN, the resulting models were able to close the FNR gap while achieving similar AUROCs (for example, the bottom right corner). Our results hold when using the worst group AUROC as the performance metric (Extended Data Fig. [4](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig9)) and across different combinations of diseases and attributes (Fig. [3b](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig3) and Extended Data Fig. [4](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig9)).

#### Fig. 3. Algorithms for removing demographic shortcuts mitigate ID fairness gaps and maintain performance.

[![Fig. 3](https://cdn.ncbi.nlm.nih.gov/pmc/blobs/eb1c/11485237/320ff9e2ea23/41591_2024_3113_Fig3_HTML.jpg)](https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&p=PMC3&id=11485237_41591_2024_3113_Fig3_HTML.jpg)

[Open in a new tab](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/figure/Fig3/)

**a**, Tradeoff between the fairness gap and overall AUROC for all trained models, for ‘Cardiomegaly’ prediction using ‘age’ as the attribute. We plotted the Pareto front—the best achievable fairness gap with a minimum constraint on the performance. **b**, Tradeoff between the fairness gap and overall AUROC for all trained models, with more disease prediction tasks and attributes. **c**, Tradeoff between the fairness gap and the overall AUROC on the ODIR dataset (ophthalmology). **d**, Tradeoff between the fairness gap and the overall AUROC on the ISIC dataset (dermatology).

#### Extended Data Fig. 4. Algorithms for removing demographic shortcuts mitigate in-distribution fairness gaps and maintain performance.

[![Extended Data Fig. 4](https://cdn.ncbi.nlm.nih.gov/pmc/blobs/eb1c/11485237/700c750be3dd/41591_2024_3113_Fig9_ESM.jpg)](https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&p=PMC3&id=11485237_41591_2024_3113_Fig9_ESM.jpg)

[Open in a new tab](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/figure/Fig9/)

**a, b**, Trade-off between the fairness gap and overall AUROC for all trained models. **c, d**, Trade-off between the fairness gap and the worst-group AUROC for all trained models. Each plot represents a specific disease prediction task (for example, Cardiomegaly) with a specific attribute (for example, age). In each case, we plot the Pareto front, the best achievable fairness gap with a minimum constraint on the performance.

To demonstrate the value of model debiasing, we further plotted the set of ‘locally optimal models’—those on the Pareto front[42](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR42) that balance the performance–fairness tradeoff most optimally on ID data (Fig. [3a](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig3)). Those models that lie on this front are ‘locally optimal’, as they have the smallest fairness gap that can be achieved for a fixed performance constraint (for example, AUROC > 0.8). In the ID setting, we found several existing algorithms that consistently achieve high ID fairness without losing notable overall performance for disease prediction (Fig. [3a,b](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig3) and Extended Data Fig. [4](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig9)).

Similar to our observations in radiology, we identified fairness gaps within subgroups based on age and sex in dermatology and ophthalmology, respectively (Fig. [2d,f](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig2)). We further verified the Pareto front for both attributes, where similar observations hold that algorithms for fixing demographic shortcuts could improve ID fairness while incurring minimal detriments to performance (Fig. [3c,d](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig3)). The steepness of the Pareto front suggests that small sacrifices in performance could yield substantial gains in fairness.

### Locally optimal models exhibit tradeoffs in other metrics

We examined how locally optimal models that balance fairness and AUROC impact other metrics, as previous work showed that it is a theoretical impossibility to balance fairness measured by probabilistic equalized odds and calibration by group[43](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR43). We found that optimizing fairness alone leads to worse results for other clinically meaningful metrics in some cases, indicating an inherent tradeoff between fairness and other metrics. First, for the ‘No Finding’ prediction task, enforcing fair predictions across groups results in worse expected calibration error gap (ECE Gap; Extended Data Fig. [5a](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig10)) between groups. Across different demographic attributes, we found a consistent statistically significant negative correlation between ECE Gap and Fairness Gap (age: _R_ = −0.85, _P_ = 7.5 × 10−42; race: _R_ = −0.64, _P_ = 6.1 × 10−15; sex: _R_ = −0.73, _P_ = 4.4 × 10−28; sex and race: _R_ = −0.45, _P_ = 1.9 × 10−8; all using two-sided _t_-test).

#### Extended Data Fig. 5. Trade-offs between the fairness gap and the expected calibration error (ECE) gap, average F1, and average precision, for models trained and evaluated on MIMIC-CXR for (a) No Finding and (b) Effusion prediction.

[![Extended Data Fig. 5](https://cdn.ncbi.nlm.nih.gov/pmc/blobs/eb1c/11485237/364365535e82/41591_2024_3113_Fig10_ESM.jpg)](https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&p=PMC3&id=11485237_41591_2024_3113_Fig10_ESM.jpg)

[Open in a new tab](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/figure/Fig10/)

We evaluate these metrics across age (‘80–100’ vs ‘18–40’), sex (‘female’ vs ‘male’), race (‘White’ vs. ‘Black’), and the intersection of sex and race (‘White male’ vs. ‘Black female’). We find that fairer models tend to exhibit larger ECE gaps, worse average F1, and worse average precision, indicating the undesirable tradeoff between fairness with other performance and calibration metrics. All p values are calculated using two-sided t-test. The center line and the shadow denote the mean and 95% CI, respectively.

We explored the relationship between fairness and other metrics, including average precision and average F1 score. For ‘No Finding’ prediction, fairer models lead to both worse average precision and F1 score (Extended Data Fig. [5a](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig10)). The same trend holds across different diseases—for example, for ‘Effusion’ (Extended Data Fig. [5b](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig10)). These findings stress that these models, although being locally optimal, exhibit worse results on other important and clinically relevant performance metrics. This uncovers the limitation of blindly optimizing fairness, emphasizing the necessity for more comprehensive evaluations to ensure the reliability of medical AI models.

### Local fairness does not transfer under distribution shift

When deploying AI models in real settings, it is crucial to ensure that models can generalize to data from unseen institutions or environments. We directly tested all trained models in the OOD setting, where we report results on external test datasets that are unseen during model development. Figure [4](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig4) illustrates that the correlation between ID and OOD performance is high across different settings, which was observed in previous work[44](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR44). However, we found that there was no consistent correlation between ID and OOD fairness. For example, Fig. [4b](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig4) shows an instance where the correlation between ID fairness and OOD fairness is strongly positive (‘Effusion’ with ‘age’ as the attribute; _R_ = 0.98, _P_ = 3.0 × 10−36, two-sided _t_-test), whereas Fig. [4c](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig4) shows an instance where the correlation between these metrics is actually significantly negative (‘Pneumothorax’ with ‘sex and race’ as the attribute; _R_ = −0.50, _P_ = 4.4 × 10−3, two-sided _t_-test). Across 16 combinations of task and attribute, we found that five such settings exhibited this negative correlation, and three additional settings exhibited only a weak ( _R_ < 0.5) positive correlation (see Extended Data Fig. [6a,b](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig11) for additional correlation plots). Thus, improving ID fairness may not lead to improvements in OOD fairness, highlighting the complex interplay between fairness and distribution shift[45](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR45), [46](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR46).

#### Fig. 4. The transfer of performance (overall AUROC) and fairness between the ID (MIMIC-CXR) and OOD datasets.

[![Fig. 4](https://cdn.ncbi.nlm.nih.gov/pmc/blobs/eb1c/11485237/8a25e0d48d6d/41591_2024_3113_Fig4_HTML.jpg)](https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&p=PMC3&id=11485237_41591_2024_3113_Fig4_HTML.jpg)

[Open in a new tab](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/figure/Fig4/)

**a**, We plotted the Pearson correlation coefficient of ID versus OOD performance versus the Pearson correlation coefficient of ID versus OOD fairness. Here, each point was derived from a grid of models trained on a particular combination of task and attribute. We found that there was a high correlation between ID and OOD performance in all cases, but the correlation between ID and OOD fairness was tenuous. **b**, One particular point where fairness transfers between ID and OOD datasets (‘Effusion’ with ‘age’ as the attribute; _R_ = 0.98, _P_ = 3.0 × 10−36, two-sided _t_-test). The center line and the shadow denote the mean and 95% CI, respectively. **c**, One particular point where fairness does not transfer between ID and OOD datasets (‘Pneumothorax’ with ‘sex and race’ as the attribute; _R_ = −0.50, _P_ = 4.4 × 10−3, two-sided _t_-test). The center line and the shadow denote the mean and 95% CI, respectively. **d**, The ID Pareto front for ‘Cardiomegaly’ prediction using ‘race’ as the attribute. **e**, The transformation of the ID Pareto front to the OOD Pareto front, for ‘Cardiomegaly’ prediction using ‘race’ as the attribute. Models that are Pareto optimal ID often do not maintain Pareto optimality OOD.

#### Extended Data Fig. 6. The transfer of performance (overall AUROC) and fairness between the ID (MIMIC-CXR) and OOD datasets.

[![Extended Data Fig. 6](https://cdn.ncbi.nlm.nih.gov/pmc/blobs/eb1c/11485237/c348c6da1f3e/41591_2024_3113_Fig11_ESM.jpg)](https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&p=PMC3&id=11485237_41591_2024_3113_Fig11_ESM.jpg)

[Open in a new tab](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/figure/Fig11/)

**a**, The OOD performance versus the ID performance for each task and attribute combination. **b**, The OOD fairness versus the ID fairness for each task and attribute combination. **c**, The Pareto front between fairness and performance for ID and OOD, finding that models that are Pareto optimal ID often do not maintain Pareto optimality OOD. All p values are calculated using two-sided t-test. The center line and the shadow denote the mean and 95% CI, respectively.

In addition, we investigated whether models achieving ID Pareto optimality between fairness and performance will maintain in OOD settings. As shown for ‘Cardiomegaly’ prediction using race as the attribute, models originally on the Pareto front ID (Fig. [4d](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig4)) do not guarantee to maintain Pareto optimality when deployed in a different OOD setting (Fig. [4e](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig4)). We show additional examples of this phenomenon in Extended Data Fig. [6c](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig11).

### Dissecting model fairness under distribution shift

To disentangle the OOD fairness gap, we present a way to decompose model fairness under distribution shift. Specifically, we decompose and attribute the change in fairness between ID and OOD to be the difference in performance change for each of the groups—that is, the change in fairness is determined by how differently the distribution shift affects each group ( [Methods](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Sec11)).

In Extended Data Fig. [7](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig12), we show examples of transferring a trained model from ID setting to OOD setting. For example, Extended Data Fig. [7d](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig12) illustrates an ERM model trained to predict ‘No Finding’ on CheXpert (ID) and transferred to MIMIC-CXR (OOD) while evaluating fairness across sex. We found that the model was fair with respect to the FPR gap in the ID setting (−0.1% gap, not significant) but had a significant FPR gap when deployed in the OOD setting (3.2%), with females being underdiagnosed at a higher rate (Extended Data Fig. [7e](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig12)). We then segmented this FPR gap by sex and found that females experienced an increase in FPR of 3.9%, whereas males experienced an increase in FPR of 0.8% (Extended Data Fig. [7f](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig12)). In other words, the model becomes worse for both groups in an OOD setting but to a much larger extent for female patients. This decomposition suggests that mitigation strategies that reduce the impact of the distribution shift on females could be effective in reducing the OOD fairness gap in this instance.

#### Extended Data Fig. 7. Examining the behavior of specific ERM models transferred from ID setting to OOD setting.

[![Extended Data Fig. 7](https://cdn.ncbi.nlm.nih.gov/pmc/blobs/eb1c/11485237/9422abe2c1e0/41591_2024_3113_Fig12_ESM.jpg)](https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&p=PMC3&id=11485237_41591_2024_3113_Fig12_ESM.jpg)

[Open in a new tab](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/figure/Fig12/)

**a, d, g, j**, The receiver operating characteristic (ROC) curves for each group and each dataset, marking the operating point of the model. **b, e, h, k**, This shift in the operating point results in a change in the FPR and FNR values of subgroups on the OOD dataset. Each bar and its error bar indicate the mean and standard deviation across 3 independent runs. **c, f, i, l**, We decompose the OOD fairness gap as a function of the ID fairness gap, and the change in FPR for each of the groups. Each bar and its error bar indicate the mean and standard deviation across 3 independent runs.

We further extended this study to a larger set of tasks and protected attributes (Extended Data Fig. [7](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig12)). Across all settings, the disparate impact of distribution shift on each group was a significant component, indicating that mitigating the impact of distribution shift is as important as mitigating ID fairness, if the goal is to achieve a fair model OOD.

### Globally optimal model selection for OOD fairness

Figure [4](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig4) shows that selecting a model based on ID fairness may not lead to a model with optimal OOD fairness. Here, we examined alternate model selection criteria that may lead to better OOD fairness, when we have access only to ID data. Our goal is to find ‘globally optimal’ models that maintain their performance and fairness in new domains. First, we subsetted our selection only to models that had satisfactory ID overall performance (defined as those with overall validation AUROC no less than 5% of the best ERM model). This set of models also had satisfactory OOD performance (Supplementary Fig. [1](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#MOESM1)).

Next, we proposed eight candidate model selection criteria (Fig. [5a](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig5)), corresponding to selecting the model from this set that minimizes or maximizes some ID metric. We evaluated the selected model by its OOD fairness across five external datasets, each containing up to four attributes and up to four tasks, corresponding to a total of 42 settings. We compared the OOD fairness of the selected model to the OOD fairness of an ‘oracle’, which observes samples from the OOD dataset and directly chooses the model with the smallest OOD fairness gap. For each setting, we computed the increase in fairness gap of each selection criteria relative to the oracle. In Fig. [5a](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig5), we report the mean across the 42 settings as well as the 95% confidence interval (CI) computed from 1,000 bootstrap iterations. We found that, surprisingly, selecting the model with the minimum ID fairness gap may not be optimal. Instead, two other criteria based on selecting models where the embedding contains the least attribute information lead to a lower average OOD fairness gap. For instance, we observed a significantly lower increase in OOD fairness gap by selecting models with the ‘Minimum Attribute Prediction Accuracy’ as compared to ‘Minimum Fairness Gap’ ( _P_ = 9.60 × 10−94, one-tailed Wilcoxon rank-sum test). The result echoes our finding in Fig. [2](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig2) that the encoding of demographic attributes is positively correlated with ID fairness.

#### Fig. 5. OOD fairness of models with different model selection criteria and for different algorithms.

[![Fig. 5](https://cdn.ncbi.nlm.nih.gov/pmc/blobs/eb1c/11485237/45f63335059f/41591_2024_3113_Fig5_HTML.jpg)](https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&p=PMC3&id=11485237_41591_2024_3113_Fig5_HTML.jpg)

[Open in a new tab](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/figure/Fig5/)

**a**, We varied the ID model selection criteria and compared the selected model against the oracle that chooses the model that is most fair OOD. We plotted the increase in OOD fairness gap of the selected model over the oracle, averaged across 42 combinations of OOD dataset, task and attribute. We used non-parametric bootstrap sampling ( _n_ = 1,000) to define the bootstrap distribution for the metric. We found that selection criteria based on choosing models with minimum attribute encoding achieve better OOD fairness than naively selecting based on ID fairness or other aggregate performance metrics (‘Minimum Attribute Prediction Accuracy’ versus ‘Minimum Fairness Gap’: _P_ = 9.60 × 10−94, one-tailed Wilcoxon rank-sum test; ‘Minimum Attribute Prediction AUROC’ versus ‘Minimum Fairness Gap’: _P_ = 1.95 × 10−12, one-tailed Wilcoxon rank-sum test). **b**, We selected the model for each algorithm with the minimum ID fairness gap. We evaluated its OOD fairness against the oracle on the same 42 settings. We found that removing demographic encoding (that is, DANN) leads to the best OOD fairness (‘DANN’ versus ‘ERM’: _P_ = 1.86 × 10−117, one-tailed Wilcoxon rank-sum test). On each box, the central line indicates the median, and the bottom and top edges of the box indicate the 25th and 75th percentiles, respectively. The whiskers extend to 1.5 times the interquartile range. Points beyond the whiskers are plotted individually using the ‘+’ symbol.

Finally, we studied the fairness of each algorithm in the OOD setting. We maintained the performance cutoff described above and selected the model for each algorithm with the lowest ID fairness gap. In Fig. [5b](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig5), we report the mean increase in OOD fairness gap relative to the oracle across the same 42 settings. We found that methods that remove demographic information from embeddings (specifically, DANN) lead to the lowest average OOD fairness gap (‘DANN’ versus ‘ERM’: _P_ = 1.86 × 10−117, one-tailed Wilcoxon rank-sum test). Our findings demonstrate that evaluating and removing demographic information encoded by the model ID may be the key to ‘globally optimal’ models that transfer both performance and fairness to external domains.

## Discussion

We demonstrated the interplays between the demographic encoding of attributes as ‘shortcuts’ in medical imaging AI models and how they change under distribution shifts. Notably, we validated our findings across global-scale datasets in radiology (Table [1](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Tab1)) and across multiple medical imaging modalities (Extended Data Figs. [1](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig6) and [2](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig7)). The results show that algorithmic encoding of protected attributes leads to unfairness (Fig. [2](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig2)) and mitigating shortcuts can reduce ID fairness gaps and maintain performance (Fig. [3](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig3)). However, our results also show that there exists an inherent tradeoff for clinically meaningful metrics beyond fairness (Extended Data Fig. [5](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig10)), and such fairness does not transfer under distribution shift (Fig. [4](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig4)). We provide initial strategies to dissect and explain the model fairness under distribution shifts (Extended Data Fig. [7](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig12)). Our results further reveal actionable algorithm and model selection strategies for OOD fairness (Fig. [5](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig5)).

Our results have multiple implications. First, they offer a cautionary tale on the efficacy and consequences of eliminating demographic shortcuts in disease classification models. On the one hand, removing shortcuts addresses ID fairness, which is a crucial consideration in fair clinical decision-making[12](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR12). On the other hand, the resulting tradeoffs with other metrics and non-transferability to OOD settings raises the question about the long-term utility in removing such shortcuts. This is particularly complex in the healthcare setting, where the relationship between demographics and the disease or outcome label is complex[47](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR47), variables can be mislabeled[48](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR48) and distribution shifts between domains are difficult to quantify[1](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR1).

Second, we frame demographic features as potential ‘shortcuts’, which should not be used by the model to make disease predictions. However, some demographic variables could be a direct causal factor in some diseases (for example, sex as a causal factor of breast cancer). In these cases, it would not be desirable to remove all demographic reliance but, instead, match the reliance of the model on the demographic attribute to its true causal effect[49](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR49). In the tasks that we examined here, demographic variables, such as race, may have an indirect effect on disease (for example, through socioeconomic status)[50](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR50), which may vary across geographic location or even time period[51](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR51). Whether demographic variables should serve as proxies for these causal factors is a decision that should rest with the model deployers[14](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR14), [47](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR47), [52](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR52), [53](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR53).

Third, we present a preliminary decomposition for diagnosing OOD model fairness changes, by expressing it as a function of the ID fairness gap and the performance change of each group. We found that the disparate impact of distribution shift on per-group performance is a major contributor to lack of fairness in OOD settings. Our work suggests that, for practitioners trying to achieve fairness in models deployed in a different domain, mitigating ID fairness is at least as important as mitigating the impact of distribution shift for particular groups. However, building models robust to arbitrary domain shifts is, in general, a challenging task[54](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR54), [55](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR55). Having some knowledge or data about how the distributions may shift, or even the ability to actively collect data for particular groups, may be necessary[56](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR56). Developing methods and deriving theoretical characterizations of fairness under distribution shift is an active area of research[45](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR45), [46](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR46).

Fourth, the US Food & Drug Administration (FDA), as the primary regulatory body for medical technologies, does not require external validation of clinical AI models, relying instead on the assessment by the product creator[57](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR57). Our findings underscore the necessity for regular evaluation of model performance under distribution shift[58](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR58), [59](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR59), challenging the popular opinion of a single fair model across different settings[60](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR60). This questions the effectiveness of developer assurances on model fairness at the time of testing and highlights the need for regulatory bodies to consider real-world performance monitoring, including fairness degradation[61](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR61). Finally, when a model is deployed in any clinical environment, both its overall and per-group performance, as well as associated clinical outcomes, should be continuously monitored[62](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR62).

Finally, although we imply that smaller ‘fairness gaps’ are better, enforcing these group fairness definitions can lead to worse utility and performance for all groups[43](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR43), [63](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR63)– [66](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR66), and other fairness definitions may be better suited to the clinical setting[8](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR8), [67](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR67). We note that these invariant notions of fairness could have drawbacks[66](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR66), as equalized odds are incompatible with calibration by group (Extended Data Fig. [5](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig10)), and enforcing equalized odds often lead to the ‘leveling down’ effect in overall performance[63](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR63), [64](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR64). We present the Pareto curve showing the tradeoff between fairness and accuracy, allowing the practitioner to select a model that best fits their deployment scenario. In general, we encourage practitioners to choose a fairness definition that is best suited to their use case and carefully consider the performance–equality tradeoff. The impact of minimizing algorithmic bias on real-world health disparities, the ultimate objective, is complex[68](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR68), and there is no guarantee that deploying a fair model will lead to equitable outcomes. In addition, although we constructed several models for clinical risk prediction, we do not advocate for deployment of these models in real-world clinical settings without practitioners carefully testing models on their data and taking other considerations into account (for example, privacy, regulation and interpretability)[1](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR1), [3](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR3).

## Methods

### Datasets and pre-processing

The datasets used in this study are summarized in Extended Data Table [1](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Tab2). Unless otherwise stated, we trained models on MIMIC-CXR[21](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR21) and evaluated on an OOD dataset created by merging CheXpert[22](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR22), NIH[23](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR23), SIIM[24](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR24), PadChest[25](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR25) and VinDr[26](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR26). We included all images (both frontal and lateral) and split each dataset into 70% train, 15% validation and 15% test sets. Note that only MIMIC-CXR and CheXpert have patient race information available, and we extracted race (and other attributes) following established protocols[69](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR69). For MIMIC-CXR, demographic information was obtained by merging with MIMIC-IV[70](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR70). For CheXpert, separate race labels were obtained from the Stanford Center for Artificial Intelligence in Medicine & Imaging ( [https://aimi.stanford.edu/](https://aimi.stanford.edu/)) website. Where applicable, we dropped patients with missing values for any attribute.

For all datasets, we excluded samples where the corresponding patient has missing age or sex. For ODIR and ISIC, we dropped samples from patients younger than 18 years and older than 80 years due to small sample sizes (that is, smaller than 3% of the total dataset).

Owing to computational constraints, we mainly chose four prediction tasks for CXRs (that is, ‘No Finding’, ‘Effusion’, ‘Cardiomegaly’ and ‘Pneumothorax’). We selected these tasks for several reasons: (1) diversity in presentation: ‘Effusion’, ‘Cardiomegaly’ and ‘Pneumothorax’ each present distinctively and occur in different locations on a CXR, allowing for a comprehensive evaluation across varied pathologies and underlying causes; (2) prevalence in clinical and research settings: these labels are not only common in clinical practice but also frequently studied in prior academic work[7](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR7), [12](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR12), [63](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR63) and used in commercial diagnostic systems[71](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR71); and (3) performance and fairness considerations: these labels are among those with both the highest diagnostic accuracy and substantial fairness gaps on MIMIC-CXR, making them particularly relevant for exploring the relationship between model performance and fairness[7](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR7), [12](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR12).

We scaled all images to 224 × 224 for input to the model. We applied the following image augmentations during training only: random flipping of the images along the horizontal axis, random rotation of up to 10° and a crop of a random size (70–100%) and a random aspect ratio (3/4 to 4/3).

### Evaluation methods

To evaluate the performance of disease classification in medical imaging, we used the following metrics: AUROC, TPR, TNR and ECE.

The TPR and TNR are calculated as (FN, false negative; FP, false positive; TP, true positive; TN, true negative):

|     |
| --- |
| TPR=TPTP+FN |

|     |
| --- |
| TNR=TNTN+FP |

When reporting the sensitivity and specificity, we followed previous work[12](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR12), [72](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR72) in selecting the threshold that maximizes the F1 score. This threshold optimization procedure is conducted separately for each dataset, task, algorithm and attribute combination. We followed standard procedures to calculate the 95% CI for sensitivity and specificity.

We also reported AUC, which is the area under the corresponding ROC curves showing an aggregate measure of detection performance. Finally, we report the expected calibration error (ECE)[73](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR73), which we computed using the netcal library[74](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR74).

#### Assessing the fairness of machine learning models

To assess the fairness of machine learning models, we evaluated the metrics described above for each demographic group as well as the difference in the value of the metric between groups. Equality of TPR and TNR between demographic groups is known in the algorithmic fairness literature as equal odds[75](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR75). As the models that we studied in this work are likely to be used as screening or triage tools, the cost of an FP may be different from the cost of an FN. In particular, for ‘No Finding’ prediction, FPs (corresponding to underdiagnosis[12](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR12)) would be more costly than FNs, and so we focused on the FPR (or TNR) for this task. For all remaining disease prediction tasks, we focused on the FNR (or TPR) for the same reason. Equality in one of the class-conditioned error rates is an instance of equal opportunity[30](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR30).

Finally, we also examined the per-group ECE and ECE gap between groups. Note that zero ECE for both groups (that is, calibration per group) implies the fairness definition known as sufficiency of the risk score[75](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR75). We emphasize that differences in calibration between groups is a significant source of disparity, as consistent under-estimation or over-estimation of risk for a particular group could lead to under-treatment or over-treatment for that group at a fixed operating threshold relative to the true risk[76](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR76).

### Quantifying the distribution shifts

We examined and quantified the types and degrees of distribution shifts in both ID and OOD settings in this study. Inspired by previous work[46](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR46), [77](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR77), we performed a series of hypothesis tests to determine if there were significant statistical differences in distributions between demographic groups and across different pairs of datasets. All _P_ values were adjusted for multiple testing using Bonferroni correction[78](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR78).

#### ID setting

We studied the following distribution shifts in the ID setting:

##### Prevalence shift: P(Y\|A)

For binary outcomes Y across groups, we calculated the total variational distance between the probability distributions of Y conditioned on different groups and used a two-sample binomial proportion test, where the null hypothesis corresponds to P(Y\|A = _a_ 1) = P(Y\|A = _a_ 2):

|     |
| --- |
| 𝑑Y⁡(𝑎1,𝑎2)=𝑑TV⁡(P⁡(Y∣A=𝑎1),P⁡(Y∣A=𝑎2)) |

##### Representation shift: P(X\|A)

When comparing the distribution of the input images X, we first encoded them into representations derived from a frozen foundation model _f_ that is trained in a self-supervised manner on diverse CXR datasets[79](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR79), [80](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR80). We then used the mean maximum discrepancy (MMD) distance and a permutation-based hypothesis test following ref. [81](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR81) to test if demographic groups differed statistically in their distribution of representations:

|     |
| --- |
| (𝑑X⁡(𝑎1,𝑎2)=𝑑MMD⁡(P⁡(𝑓⁡(X)∣⁢A=𝑎1),P⁡(𝑓⁡(X)∣A=𝑎2)) |

#### OOD setting

We studied the following distribution shifts in the OOD setting (the null hypothesis is PID(·) = POOD(·)):

##### Label shift: P(Y)

We calculated the total variational distance between the probability distributions of binary outcomes Y across ID and OOD datasets using a two-sample binomial proportion test:

|     |
| --- |
| 𝑑Y=𝑑T⁢V⁡(PI⁢D⁡(Y),PO⁢O⁢D⁡(Y)) |

##### Prevalence shift: P(Y\|A = a)

We similarly evaluated the distance between the distributions of Y conditioned on specific demographic subgroups (A) between ID and OOD datasets:

|     |
| --- |
| 𝑑Y\|A⁡(𝑎)=𝑑T⁢V⁡(PI⁢D⁡(Y\|A=𝑎),PO⁢O⁢D⁡(Y\|A=𝑎)) |

##### Covariate shift: P(X)

We again encoded X into representations derived from a frozen foundation model _f_ and then used the MMD distance and a permutation-based hypothesis test[81](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR81) to examine if ID and OOD datasets differed statistically in their distribution of representations:

|     |
| --- |
| 𝑑X=𝑑M⁢M⁢D⁡(PI⁢D⁡(𝑓⁡(X)),PO⁢O⁢D⁡(𝑓⁡(X))) |

##### Representation shift: P(X\|A = a)

Similarly, we calculated the MMD distance conditioned on subgroup A to evaluate shifts in the representation space:

|     |
| --- |
| 𝑑X\|𝐴⁡(𝑎)=𝑑M⁢M⁢D⁡(PI⁢D⁡(𝑓⁡(X)\|A=𝑎),PO⁢O⁢D⁡(𝑓⁡(X)\|A=𝑎)) |

We provide additional results on quantifying various distribution shifts in both ID and OOD settings in the [Supplementary Information](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#MOESM1) (Supplementary Tables [1](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#MOESM1)– [3](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#MOESM1)).

### Training details

We trained DenseNet-121 (ref. [33](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR33)) models on each task, initializing with ImageNet[82](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR82) pre-trained weights. We evaluated six algorithms: ERM[34](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR34), ReSample[35](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR35), GroupDRO[36](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR36), DANN[37](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR37), CDANN[38](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR38) and MA[39](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR39).

For each combination of task, algorithm and demographic attribute, we conducted a random hyperparameter search[83](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR83) with 12 runs. During training, for a particular attribute, we evaluated the validation set worst-group validation AUROC every 1,000 steps and early stopped if this metric had not improved for five evaluations. We tuned the learning rate and weight decay for all algorithms and also tuned algorithm-specific hyperparameters as mentioned in the original works. We selected the hyperparameter setting that maximized the worst-attribute validation AUROC. CIs were computed as the standard deviation across three different random seeds for each hyperparameter setting.

We also explored a multi-label training setup, where models were trained simultaneously on 14 binary labels available in MIMIC-CXR[7](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR7). We followed the same experimental protocol as outlined in the main paper, including hyperparameter tuning and model selection. Our findings in the multi-label setup mirrored those seen in the binary task setup (Supplementary Figs. [2](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#MOESM1) and [3](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#MOESM1)).

To obtain the level of demographic encoding within representations (Fig. [2](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig2)), we first computed representations using a trained disease prediction model. We froze these representations and trained a multi-class multi-nomial logistic regression model to predict the demographic group using the training set using the scikit-learn library[84](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR84). We varied the L2 regularization strength between 10−5 and 10 and selected the model with the best macro-averaged AUROC on the validation set. We report the macro-averaged AUROC on the test set.

### Decomposing OOD fairness

Here we present a first approach toward decomposing the fairness gap in an OOD environment as a function of the ID fairness gap and the impact that the distribution shift has in each group. In particular, let _D_ src and _D_ tar be the source and target datasets, respectively. Let _g ∈ G_ be a particular group from a set of groups. Let _L_ _f_( _g_, _D_) be an evaluation metric for a model _f_, which is decomposable over individual samples—that is, _L_ _f_( _g_, _D_) = 𝐿𝑓⁡(𝑔,𝐷)=∑(𝑥,𝑦,𝑔′)∈𝐷;𝑔′=𝑔𝑙⁡(𝑓⁡(𝑥),𝑦). Examples of such metrics are the accuracy, TPR or TNR. Then, we can decompose:

|     |
| --- |
| 𝐿𝑓⁡(𝑔1,𝐷tar)−𝐿𝑓⁡(𝑔2,𝐷tar)=\[𝐿𝑓⁡(𝑔1,𝐷src)−𝐿𝑓⁡(𝑔2,𝐷src)\]+\[𝐿𝑓⁡(𝑔2,𝐷src)−𝐿𝑓⁡(𝑔2,𝐷tar)\]−\[𝐿𝑓⁡(𝑔1,𝐷src)−𝐿𝑓⁡(𝑔1,𝐷tar)\]. |

The left-hand term is the fairness gap in the OOD environment, and the three terms on the right are (1) the fairness gap in the ID data, (2) the impact of the distribution shift on g2 and (3) the impact of the distribution shift on g1. We note that, to achieve a low fairness gap in the OOD environment, it is important not only to minimize the ID fairness gap (term 1) but also to minimize the difference in how the distribution shift impacts each group (term 2 − term 3).

### Evaluation with different medical imaging modalities

In addition to radiology, we also examined medical AI applications in dermatology and ophthalmology to corroborate our findings. Specifically, Extended Data Fig. [1](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig6) shows the results for dermatological imaging. We used the ISIC dataset[28](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR28), which contains 32,259 images sourced from multiple international sites. We focused on the ‘No Finding’ task, taking into account ‘sex’ and ‘age’ as the sensitive demographic attributes (Extended Data Fig. [1a](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig6)). Similar to our observations in radiology, we identified fairness gaps within subgroups based on age and sex (Extended Data Fig. [1b](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig6)), although these disparities were less significant than those observed in CXR assessments (for example, fairness gaps smaller than 2%). This was further confirmed by the Pareto front plot, where most models, including ERM, could achieve a good performance–fairness tradeoff (Extended Data Fig. [1c](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig6)).

We extended our analysis to ophthalmology images, specifically focusing on retinopathy detection, using the ODIR dataset[29](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR29) with 6,800 images (Extended Data Fig. [2](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig7)). The task that we considered was ‘Retinopathy’, with ‘sex’ and ‘age’ being used as demographic attributes (Extended Data Fig. [2a](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig7)). Notably, significant subgroup fairness gaps were observed in age (43% FNR gap between groups ‘60–80’ and ‘18–40’). In contrast, the fairness gap based on sex was less significant, with a 3% FNR difference between ‘female’ and ‘male’ subgroups. We further verified the Pareto front for both attributes, where similar observations hold that algorithms for fixing demographic shortcuts could improve ID fairness while incurring minimal detriments to performance (measured in AUROC).

### Analysis on underdiagnosis versus overdiagnosis

In evaluating fairness metrics, our primary study centered on underdiagnosis, specifically the disparities in FPR for ‘No Finding’ and discrepancies in FNR for other conditions. However, an alternative approach involves focusing on overdiagnosis, defined as variances in FNR for ‘No Finding’ and differences in FPR for other diseases. We present findings between their relationship in Extended Data Fig. [3](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig8). An analysis spanning two datasets (MIMIC and CheXpert) and various tasks revealed a consistent pattern: larger gaps in underdiagnosis tend to correspond with more significant overdiagnosis discrepancies. Nonetheless, certain task and attribute combinations exhibited more complex trends, indicating a necessity for deeper exploration and informed decision-making regarding the most appropriate fairness metrics for critical disease evaluations in practical medical settings.

### Direct prediction of demographic attributes

We provide analyses for demographic encoding of attribute information. In the main paper, we analyzed the predictiveness of attributes (for example, age, sex and race) based on the embeddings from a disease classification model. The distinct predictiveness between attributes in these domains could be attributed to the intrinsic characteristics of the datasets or the nature of the conditions being studied. To delve deeper, we conducted an experiment training an ERM model to predict these attributes directly using the dermatology dataset (ISIC), and we show the results in Supplementary Table [4](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#MOESM1). We observed that certain attributes are indeed less predictive compared to others (that is, age versus sex), suggesting that age may be inherently more challenging to encode within the studied dermatology dataset. Furthermore, the results reveal variations in the predictiveness of age across different subgroups (for example, age groups ‘18–40’ and ‘60–80’ exhibit higher AUROC than the ‘40–60’ group).

### Analysis using multi-label models

Prior works[7](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR7) studied CXR classification in the multi-label setting, where a model contains an encoder, followed by an individual linear classification head for each of the downstream tasks. We followed the setup[7](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR7) to study the following 14 binary labels in MIMIC-CXR: Atelectasis, Cardiomegaly, Consolidation, Edema, Enlarged Cardiomediastinum, Fracture, Lung Lesion, Airspace Opacity, No Finding, Effusion, Pleural Other, Pneumonia, Pneumothorax and Support Devices.

We adapted the following methods to the multi-label setting, (1) ERM, (2) DANN and (3) CDANN, and we followed the same experimental protocol in the main paper in terms of hyperparameter tuning. Note that GroupDRO and ReSample are challenging to adapt to the multi-label setting, as the number of groups is exponential in the number of tasks. For each combination of algorithm and attribute, we selected the multi-label model that maximizes the worst-attribute AUROC, averaged across the 14 tasks.

First, we examined the level of demographic encoding present in the embeddings of the best multi-task ERM model and found that it also encodes a variety of demographic information, similar to the single-label case (Supplementary Fig. [2a](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#MOESM1)). We further showed the fairness gaps of this best multi-label ERM model and observed that a variety of fairness gaps exist and are statistically significant across all tasks (Supplementary Fig. [2b](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#MOESM1)). In addition, we plotted the correlation between the fairness gap and the attribute prediction AUROC, across all trained multi-label models. We found a strong and statistically significant positive correlation among all combinations of task and attribute, similar to the single-label case (Supplementary Fig. [2c](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#MOESM1)).

We also present Pareto plots showing the tradeoff between the fairness gap and overall AUROC across all models, for each combination of task and attribute (Supplementary Fig. [3](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#MOESM1)). Overall, we found that the Pareto fronts for the multi-label models demonstrate similar behavior as the single-task models—where the multi-label ERM exhibits the best overall AUROC but has high fairness gap. In addition, with debiasing methods such as multi-label DANN and CDANN, we are able to achieve fair models with minimal loss in overall AUROC.

### Test set rebalancing for prevalence shift

We investigated whether eliminating the prevalence shift in the test set would address the observed fairness gaps. Following prior work[40](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#CR40), we balanced the test set for multiple attributes—age and race—ensuring that demographic proportions (for example, ‘White’ aged ‘20–40’ versus ‘Black’ aged ‘60–80’) and disease prevalence are uniform across all attribute combinations. This approach aims to eliminate prevalence shifts within the test set. Our findings in Supplementary Fig. [4](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#MOESM1) suggest that, although test set rebalancing can reduce fairness gaps for certain task and attribute combinations (for example, ‘No Finding’ for ‘race’ and ‘Cardiomegaly’ for ‘age’, as compared to Fig. [2b](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#Fig2)), there exists significant gaps even after rebalancing the test set, indicating that fairness gaps are influenced by multiple shifts beyond just prevalence shifts.

### Statistical analysis

#### Correlation

To calculate the correlations between variables, we used Pearson correlation coefficients and their associated _P_ value (two-sided _t_-test, α = 0.05). 95% CI for the Pearson correlation coefficient was calculated.

#### Increase in OOD fairness gap

One-tailed Wilcoxon rank-sum test (α = 0.05) was used to assess the increase in OOD fairness gap compared to oracle models.

#### CIs

We used non-parametric bootstrap sampling to generate CIs: random samples of size _n_ (equal to the size of the original dataset) were repeatedly sampled 1,000 times from the original dataset with replacement. We then estimated the increase in OOD fairness gap compared to oracle using each bootstrap sample (α = 0.05).

All statistical analysis was performed with Python version 3.9 (Python Software Foundation).

### Reporting summary

Further information on research design is available in the [Nature Portfolio Reporting Summary](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/#MOESM2) linked to this article.

## Online content

Any methods, additional references, Nature Portfolio reporting summaries, source data, extended data, supplementary information, acknowledgements, peer review information; details of author contributions and competing interests; and statements of data and code availability are available at 10.1038/s41591-024-03113-4.

## Supplementary information

[41591\_2024\_3113\_MOESM1\_ESM.pdf](https://pmc.ncbi.nlm.nih.gov/articles/instance/11485237/bin/41591_2024_3113_MOESM1_ESM.pdf) (1.1MB, pdf)

Supplementary Tables 1–4, Figs. 1–4 and Note 1.

[Reporting Summary](https://pmc.ncbi.nlm.nih.gov/articles/instance/11485237/bin/41591_2024_3113_MOESM2_ESM.pdf) (2.1MB, pdf)

## Acknowledgements

H.Z. was supported, in part, by a Google Research Scholar Award. J.W.G. is a 2022 Robert Wood Johnson Foundation Harold Amos Medical Faculty Development Program awardee and declares support from Radiological Society of North America health disparities grant number EIHD2204, the Lacuna Fund (no. 67), the Gordon and Betty Moore Foundation, a National Institutes of Health (National Institute of Biomedical Imaging and Bioengineering) Medical Imaging and Data Resource Center grant under contracts 75N92020C00008 and 75N92020C00021 and National Heart, Lung, and Blood Institute award number R01HL167811. The funders had no role in study design, data collection and analysis, decision to publish or preparation of the manuscript.

### Extended data

## Author contributions

Y.Y., H.Z. and M.G. conceived and designed the study. Y.Y. and H.Z. performed data collection, processing and experimental analysis. Y.Y., H.Z., J.W.G., D.K. and M.G. interpreted experimental results and provided feedback on the study. Y.Y., H.Z., J.W.G. and M.G. wrote the original manuscript. M.G. supervised the research. All authors reviewed and approved the manuscript.

## Peer review

### Peer review information

_Nature Medicine_ thanks Melanie Roschewitz and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. Primary Handling Editor: Lorenzo Righetto, in collaboration with the _Nature Medicine_ team.

## Data availability

All datasets used in this study are publicly available. The MIMIC-CXR ( [https://www.physionet.org/content/mimic-cxr-jpg/2.1.0/](https://www.physionet.org/content/mimic-cxr-jpg/2.1.0/)) and VinDr-CXR ( [https://physionet.org/content/vindr-cxr/1.0.0/](https://physionet.org/content/vindr-cxr/1.0.0/)) datasets are available from PhysioNet after completion of a data use agreement and a credentialing procedure. The CheXpert dataset ( [https://stanfordmlgroup.github.io/competitions/chexpert/](https://stanfordmlgroup.github.io/competitions/chexpert/)), along with associated race labels, is available from the Stanford Center for Artificial Intelligence in Medicine & Imaging website ( [https://aimi.stanford.edu/](https://aimi.stanford.edu/)). The ChestX-ray14 (National Institutes of Health) dataset ( [https://nihcc.app.box.com/v/ChestXray-NIHCC](https://nihcc.app.box.com/v/ChestXray-NIHCC)) is available to download from the National Institutes of Health Clinical Center. The PadChest dataset ( [https://academictorrents.com/details/96ebb4f92b85929eadfb16761f310a6d04105797](https://academictorrents.com/details/96ebb4f92b85929eadfb16761f310a6d04105797)) can be downloaded from the Medical Imaging Databank of the Valencia Region. The SIIM-ACR Pneumothorax Segmentation dataset ( [https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation](https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation)) can be downloaded from its Kaggle contest page. The ISIC 2020 dataset ( [https://challenge.isic-archive.com/data/#2020](https://challenge.isic-archive.com/data/#2020)) can be downloaded from the SIIM-ISIC Melanoma Classification Challenge page. The ODIR dataset ( [https://odir2019.grand-challenge.org/dataset/](https://odir2019.grand-challenge.org/dataset/)) can be obtained from the ODIR 2019 challenge hosted by Grand Challenges.

## Code availability

Code that supports the findings of this study is publicly available with an open-source license at [https://github.com/YyzHarry/shortcut-ood-fairness](https://github.com/YyzHarry/shortcut-ood-fairness).

## Competing interests

D.K. is a co-founder of Emerald Innovations, Inc. and serves on the scientific advisory board of Janssen and on the data and analytics advisory board of Amgen. The other authors declare no competing interests.

## Footnotes

**Publisher’s note** Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.

These authors contributed equally: Yuzhe Yang, Haoran Zhang.

### Extended data

is available for this paper at 10.1038/s41591-024-03113-4.

### Supplementary information

The online version contains supplementary material available at 10.1038/s41591-024-03113-4.

## References

- 1.Zhang, A., Xing, L., Zou, J. & Wu, J. C. Shifting machine learning for healthcare from development to deployment and from models to data. _Nat. Biomed. Eng._ **6**, 1330–1345 (2022).
\[ [DOI](https://doi.org/10.1038/s41551-022-00898-y)\] \[ [PMC free article](https://pmc.ncbi.nlm.nih.gov/articles/PMC12063568/)\] \[ [PubMed](https://pubmed.ncbi.nlm.nih.gov/35788685/)\] \[ [Google Scholar](https://scholar.google.com/scholar_lookup?Zhang,%20A.,%20Xing,%20L.,%20Zou,%20J.%20&%20Wu,%20J.%20C.%20Shifting%20machine%20learning%20for%20healthcare%20from%20development%20to%20deployment%20and%20from%20models%20to%20data.%20Nat.%20Biomed.%20Eng.6,%201330%E2%80%931345%20(2022).)\]
- 2.Sendak, M. P. et al. A path for translation of machine learning products into healthcare delivery. _EMJ Innov_. 10.33590/emjinnov/19-00172 (2020).
- 3.Wiens, J. et al. Do no harm: a roadmap for responsible machine learning for health care. _Nat. Med._ **25**, 1337–1340 (2019).
\[ [DOI](https://doi.org/10.1038/s41591-019-0548-6)\] \[ [PubMed](https://pubmed.ncbi.nlm.nih.gov/31427808/)\] \[ [Google Scholar](https://scholar.google.com/scholar_lookup?Wiens,%20J.%20et%20al.%20Do%20no%20harm:%20a%20roadmap%20for%20responsible%20machine%20learning%20for%20health%20care.%20Nat.%20Med.25,%201337%E2%80%931340%20(2019).)\]
- 4.Ahmad, M. A., Patel, A., Eckert, C., Kumar, V. & Teredesai, A. Fairness in machine learning for healthcare. In _Proc. 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_ 3529–3530 (Association for Computing Machinery, 2020).
- 5.McKinney, S. M. et al. International evaluation of an AI system for breast cancer screening. _Nature_ **577**, 89–94 (2020).
\[ [DOI](https://doi.org/10.1038/s41586-019-1799-6)\] \[ [PubMed](https://pubmed.ncbi.nlm.nih.gov/31894144/)\] \[ [Google Scholar](https://scholar.google.com/scholar_lookup?McKinney,%20S.%20M.%20et%20al.%20International%20evaluation%20of%20an%20AI%20system%20for%20breast%20cancer%20screening.%20Nature577,%2089%E2%80%9394%20(2020).)\]
- 6.Burlina, P. et al. Utility of deep learning methods for referability classification of age-related macular degeneration. _JAMA Ophthalmol._ **136**, 1305–1307 (2018).
\[ [DOI](https://doi.org/10.1001/jamaophthalmol.2018.3799)\] \[ [PMC free article](https://pmc.ncbi.nlm.nih.gov/articles/PMC6248178/)\] \[ [PubMed](https://pubmed.ncbi.nlm.nih.gov/30193354/)\] \[ [Google Scholar](https://scholar.google.com/scholar_lookup?Burlina,%20P.%20et%20al.%20Utility%20of%20deep%20learning%20methods%20for%20referability%20classification%20of%20age-related%20macular%20degeneration.%20JAMA%20Ophthalmol.136,%201305%E2%80%931307%20(2018).)\]
- 7.Seyyed-Kalantari, L., Liu, G., McDermott, M., Chen, I. Y. & Ghassemi, M. _CheXclusion: Fairness Gaps in Deep Chest X-ray Classifiers_. Pacific Symposium on Biocomputing (World Scientific Publishing Company, 2020); [https://psb.stanford.edu/psb-online/proceedings/psb21/seyyed-kalantari.pdf](https://psb.stanford.edu/psb-online/proceedings/psb21/seyyed-kalantari.pdf) \[ [PubMed](https://pubmed.ncbi.nlm.nih.gov/33691020/)\]
- 8.Zong, Y., Yang, Y. & Hospedales, T. MEDFAIR: benchmarking fairness for medical imaging. In _Proc. 11th International Conference on Learning Representations_ (ICLR, 2023); [https://openreview.net/forum?id=6ve2CkeQe5S](https://openreview.net/forum?id=6ve2CkeQe5S)
- 9.Kinyanjui, N. M. et al. Fairness of classifiers across skin tones in dermatology. In _Medical Image Computing and Computer Assisted Intervention_ _—_ _MICCAI 2020_ Vol. 12266 (eds Martel, A. L. et al.) 320–329 (Springer, 2020); 10.1007/978-3-030-59725-2\_31
- 10.Lin, M. et al. Improving model fairness in image-based computer-aided diagnosis. _Nat. Commun._ **14**, 6261 (2023).
\[ [DOI](https://doi.org/10.1038/s41467-023-41974-4)\] \[ [PMC free article](https://pmc.ncbi.nlm.nih.gov/articles/PMC10558498/)\] \[ [PubMed](https://pubmed.ncbi.nlm.nih.gov/37803009/)\] \[ [Google Scholar](https://scholar.google.com/scholar_lookup?Lin,%20M.%20et%20al.%20Improving%20model%20fairness%20in%20image-based%20computer-aided%20diagnosis.%20Nat.%20Commun.14,%206261%20(2023).)\]
- 11.Weng, N., Bigdeli, S., Petersen, E. & Feragen, A. Are sex-based physiological differences the cause of gender bias for chest X-ray diagnosis? In _Clinical Image-Based Procedures, Fairness of AI in Medical Imaging, and Ethical and Philosophical Issues in Medical Imaging (CLIP 2023, EPIMI 2023, FAIMI 2023)_ Vol. 14242 (eds Wesarg, S. et al.) 142–152 (Springer, 2023); 10.1007/978-3-031-45249-9\_14
- 12.Seyyed-Kalantari, L., Zhang, H., McDermott, M. B., Chen, I. Y. & Ghassemi, M. Underdiagnosis bias of artificial intelligence algorithms applied to chest radiographs in under-served patient populations. _Nat. Med._ **27**, 2176–2182 (2021).
\[ [DOI](https://doi.org/10.1038/s41591-021-01595-0)\] \[ [PMC free article](https://pmc.ncbi.nlm.nih.gov/articles/PMC8674135/)\] \[ [PubMed](https://pubmed.ncbi.nlm.nih.gov/34893776/)\] \[ [Google Scholar](https://scholar.google.com/scholar_lookup?Seyyed-Kalantari,%20L.,%20Zhang,%20H.,%20McDermott,%20M.%20B.,%20Chen,%20I.%20Y.%20&%20Ghassemi,%20M.%20Underdiagnosis%20bias%20of%20artificial%20intelligence%20algorithms%20applied%20to%20chest%20radiographs%20in%20under-served%20patient%20populations.%20Nat.%20Med.27,%202176%E2%80%932182%20(2021).)\]
- 13.Adamson, A. S. & Smith, A. Machine learning and health care disparities in dermatology. _JAMA Dermatol._ **154**, 1247–1248 (2018).
\[ [DOI](https://doi.org/10.1001/jamadermatol.2018.2348)\] \[ [PubMed](https://pubmed.ncbi.nlm.nih.gov/30073260/)\] \[ [Google Scholar](https://scholar.google.com/scholar_lookup?Adamson,%20A.%20S.%20&%20Smith,%20A.%20Machine%20learning%20and%20health%20care%20disparities%20in%20dermatology.%20JAMA%20Dermatol.154,%201247%E2%80%931248%20(2018).)\]
- 14.McCradden, M. D., Joshi, S., Mazwi, M. & Anderson, J. A. Ethical limitations of algorithmic fairness solutions in health care machine learning. _Lancet Digit. Health_ **2**, e221–e223 (2020).
\[ [DOI](https://doi.org/10.1016/S2589-7500(20)30065-0)\] \[ [PubMed](https://pubmed.ncbi.nlm.nih.gov/33328054/)\] \[ [Google Scholar](https://scholar.google.com/scholar_lookup?McCradden,%20M.%20D.,%20Joshi,%20S.,%20Mazwi,%20M.%20&%20Anderson,%20J.%20A.%20Ethical%20limitations%20of%20algorithmic%20fairness%20solutions%20in%20health%20care%20machine%20learning.%20Lancet%20Digit.%20Health2,%20e221%E2%80%93e223%20(2020).)\]
- 15.Gichoya, J. W. et al. AI recognition of patient race in medical imaging: a modelling study. _Lancet Digit. Health_ **4**, e406–e414 (2022).
\[ [DOI](https://doi.org/10.1016/S2589-7500(22)00063-2)\] \[ [PMC free article](https://pmc.ncbi.nlm.nih.gov/articles/PMC9650160/)\] \[ [PubMed](https://pubmed.ncbi.nlm.nih.gov/35568690/)\] \[ [Google Scholar](https://scholar.google.com/scholar_lookup?Gichoya,%20J.%20W.%20et%20al.%20AI%20recognition%20of%20patient%20race%20in%20medical%20imaging:%20a%20modelling%20study.%20Lancet%20Digit.%20Health4,%20e406%E2%80%93e414%20(2022).)\]
- 16.Adleberg, J. et al. Predicting patient demographics from chest radiographs with deep learning. _J. Am. Coll. Radiol._ **19**, 1151–1161 (2022).
\[ [DOI](https://doi.org/10.1016/j.jacr.2022.06.008)\] \[ [PubMed](https://pubmed.ncbi.nlm.nih.gov/35964688/)\] \[ [Google Scholar](https://scholar.google.com/scholar_lookup?Adleberg,%20J.%20et%20al.%20Predicting%20patient%20demographics%20from%20chest%20radiographs%20with%20deep%20learning.%20J.%20Am.%20Coll.%20Radiol.19,%201151%E2%80%931161%20(2022).)\]
- 17.Geirhos, R. et al. Shortcut learning in deep neural networks. _Nat. Mach. Intell._ **2**, 665–673 (2020). \[ [Google Scholar](https://scholar.google.com/scholar_lookup?Geirhos,%20R.%20et%20al.%20Shortcut%20learning%20in%20deep%20neural%20networks.%20Nat.%20Mach.%20Intell.2,%20665%E2%80%93673%20(2020).)\]
- 18.Banerjee, I. et al. ‘Shortcuts’ causing bias in radiology artificial intelligence: causes, evaluation, and mitigation. _J. Am. Coll. Radiol._ **20**, 842–851 (2023).
\[ [DOI](https://doi.org/10.1016/j.jacr.2023.06.025)\] \[ [PMC free article](https://pmc.ncbi.nlm.nih.gov/articles/PMC11192466/)\] \[ [PubMed](https://pubmed.ncbi.nlm.nih.gov/37506964/)\] \[ [Google Scholar](https://scholar.google.com/scholar_lookup?Banerjee,%20I.%20et%20al.%20%E2%80%98Shortcuts%E2%80%99%20causing%20bias%20in%20radiology%20artificial%20intelligence:%20causes,%20evaluation,%20and%20mitigation.%20J.%20Am.%20Coll.%20Radiol.20,%20842%E2%80%93851%20(2023).)\]
- 19.Zech, J. R. et al. Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: a cross-sectional study. _PLoS Med._ **15**, e1002683 (2018).
\[ [DOI](https://doi.org/10.1371/journal.pmed.1002683)\] \[ [PMC free article](https://pmc.ncbi.nlm.nih.gov/articles/PMC6219764/)\] \[ [PubMed](https://pubmed.ncbi.nlm.nih.gov/30399157/)\] \[ [Google Scholar](https://scholar.google.com/scholar_lookup?Zech,%20J.%20R.%20et%20al.%20Variable%20generalization%20performance%20of%20a%20deep%20learning%20model%20to%20detect%20pneumonia%20in%20chest%20radiographs:%20a%20cross-sectional%20study.%20PLoS%20Med.15,%20e1002683%20(2018).)\]
- 20.DeGrave, A. J., Janizek, J. D. & Lee, S.-I. AI for radiographic COVID-19 detection selects shortcuts over signal. _Nat. Mach. Intell._ **3**, 610–619 (2021). \[ [Google Scholar](https://scholar.google.com/scholar_lookup?DeGrave,%20A.%20J.,%20Janizek,%20J.%20D.%20&%20Lee,%20S.-I.%20AI%20for%20radiographic%20COVID-19%20detection%20selects%20shortcuts%20over%20signal.%20Nat.%20Mach.%20Intell.3,%20610%E2%80%93619%20(2021).)\]
- 21.Johnson, A. E. et al. MIMIC-CXR-JPG, a large publicly available database of labeled chest radiographs. Preprint at [https://arxiv.org/abs/1901.07042](https://arxiv.org/abs/1901.07042) (2019). \[ [DOI](https://doi.org/10.1038/s41597-019-0322-0)\] \[ [PMC free article](https://pmc.ncbi.nlm.nih.gov/articles/PMC6908718/)\] \[ [PubMed](https://pubmed.ncbi.nlm.nih.gov/31831740/)\]
- 22.Irvin, J. et al. CheXpert: a large chest radiograph dataset with uncertainty labels and expert comparison. In _Proc. of the AAAI Conference on Artificial Intelligence_ Vol. 33, 590–597 (Association for Computing Machinery, 2019); 10.1609/aaai.v33i01.3301590
- 23.Wang, X. et al. ChestX-ray8: hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. In _Proc. of the IEEE Conference on Computer Vision and Pattern Recognition_ 2097–2106 (IEEE, 2017); [https://openaccess.thecvf.com/content\_cvpr\_2017/papers/Wang\_ChestX-ray8\_Hospital-Scale\_Chest\_CVPR\_2017\_paper.pdf](https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.pdf)
- 24.Zawacki, A. et al. SIIM-ACR pneumothorax segmentation. _kaggle_ [https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation/](https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation/) (2019).
- 25.Bustos, A., Pertusa, A., Salinas, J.-M. & De La Iglesia-Vaya, M. PadChest: a large chest x-ray image dataset with multi-label annotated reports. _Med. Image Anal._ **66**, 101797 (2020).
\[ [DOI](https://doi.org/10.1016/j.media.2020.101797)\] \[ [PubMed](https://pubmed.ncbi.nlm.nih.gov/32877839/)\] \[ [Google Scholar](https://scholar.google.com/scholar_lookup?Bustos,%20A.,%20Pertusa,%20A.,%20Salinas,%20J.-M.%20&%20De%20La%20Iglesia-Vaya,%20M.%20PadChest:%20a%20large%20chest%20x-ray%20image%20dataset%20with%20multi-label%20annotated%20reports.%20Med.%20Image%20Anal.66,%20101797%20(2020).)\]
- 26.Nguyen, H. Q. et al. VinDr-CXR: an open dataset of chest X-rays with radiologist’s annotations. _Sci. Data_ **9**, 429 (2022).
\[ [DOI](https://doi.org/10.1038/s41597-022-01498-w)\] \[ [PMC free article](https://pmc.ncbi.nlm.nih.gov/articles/PMC9300612/)\] \[ [PubMed](https://pubmed.ncbi.nlm.nih.gov/35858929/)\] \[ [Google Scholar](https://scholar.google.com/scholar_lookup?Nguyen,%20H.%20Q.%20et%20al.%20VinDr-CXR:%20an%20open%20dataset%20of%20chest%20X-rays%20with%20radiologist%E2%80%99s%20annotations.%20Sci.%20Data9,%20429%20(2022).)\]
- 27.Larrazabal, A. J., Nieto, N., Peterson, V., Milone, D. H. & Ferrante, E. Gender imbalance in medical imaging datasets produces biased classifiers for computer-aided diagnosis. _Proc. Natl Acad. Sci. USA_ **117**, 12592–12594 (2020).
\[ [DOI](https://doi.org/10.1073/pnas.1919012117)\] \[ [PMC free article](https://pmc.ncbi.nlm.nih.gov/articles/PMC7293650/)\] \[ [PubMed](https://pubmed.ncbi.nlm.nih.gov/32457147/)\] \[ [Google Scholar](https://scholar.google.com/scholar_lookup?Larrazabal,%20A.%20J.,%20Nieto,%20N.,%20Peterson,%20V.,%20Milone,%20D.%20H.%20&%20Ferrante,%20E.%20Gender%20imbalance%20in%20medical%20imaging%20datasets%20produces%20biased%20classifiers%20for%20computer-aided%20diagnosis.%20Proc.%20Natl%20Acad.%20Sci.%20USA117,%2012592%E2%80%9312594%20(2020).)\]
- 28.Rotemberg, V. et al. A patient-centric dataset of images and metadata for identifying melanomas using clinical context. _Sci. Data_ **8**, 34 (2021).
\[ [DOI](https://doi.org/10.1038/s41597-021-00815-z)\] \[ [PMC free article](https://pmc.ncbi.nlm.nih.gov/articles/PMC7843971/)\] \[ [PubMed](https://pubmed.ncbi.nlm.nih.gov/33510154/)\] \[ [Google Scholar](https://scholar.google.com/scholar_lookup?Rotemberg,%20V.%20et%20al.%20A%20patient-centric%20dataset%20of%20images%20and%20metadata%20for%20identifying%20melanomas%20using%20clinical%20context.%20Sci.%20Data8,%2034%20(2021).)\]
- 29.Ocular disease recognition. _kaggle_ [https://www.kaggle.com/datasets/andrewmvd/ocular-disease-recognition-odir5k](https://www.kaggle.com/datasets/andrewmvd/ocular-disease-recognition-odir5k) (accessed 5 September 2023).
- 30.Hardt, M., Price, E. & Srebro, N. Equality of opportunity in supervised learning. In _Proc. 30th Conference on Neural Information Processing Systems_ (NIPS, 2016); [https://proceedings.neurips.cc/paper\_files/paper/2016/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2016/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf)
- 31.Brown, A. et al. Detecting shortcut learning for fair medical AI using shortcut testing. _Nat. Commun._ **14**, 4314 (2023).
\[ [DOI](https://doi.org/10.1038/s41467-023-39902-7)\] \[ [PMC free article](https://pmc.ncbi.nlm.nih.gov/articles/PMC10354021/)\] \[ [PubMed](https://pubmed.ncbi.nlm.nih.gov/37463884/)\] \[ [Google Scholar](https://scholar.google.com/scholar_lookup?Brown,%20A.%20et%20al.%20Detecting%20shortcut%20learning%20for%20fair%20medical%20AI%20using%20shortcut%20testing.%20Nat.%20Commun.14,%204314%20(2023).)\]
- 32.Makar, M. et al _._ Causally motivated shortcut removal using auxiliary labels. In _Proc. 25th International Conference on Artificial Intelligence and Statistics_ Vol. 151, 739–766 (PMLR, 2022); [https://proceedings.mlr.press/v151/makar22a/makar22a.pdf](https://proceedings.mlr.press/v151/makar22a/makar22a.pdf)
- 33.Huang, G., Liu, Z., Van Der Maaten, L. & Weinberger, K. Q. Densely connected convolutional networks. In _Proc. of the 2017 IEEE Conference on Computer Vision and Pattern Recognition_ 4700–4708 (IEEE, 2017); [https://doi.ieeecomputersociety.org/10.1109/CVPR.2017.243](https://doi.ieeecomputersociety.org/10.1109/CVPR.2017.243)
- 34.Vapnik, V. Principles of risk minimization for learning theory. In _Advances in Neural Information Processing Systems 4_ (NeurIPS, 1991); [https://proceedings.neurips.cc/paper\_files/paper/1991/file/ff4d5fbbafdf976cfdc032e3bde78de5-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/1991/file/ff4d5fbbafdf976cfdc032e3bde78de5-Paper.pdf)
- 35.Idrissi, B. Y., Arjovsky, M., Pezeshki, M. & Lopez-Paz, D. Simple data balancing achieves competitive worst-group-accuracy. In _Proc. 1st Conference on Causal Learning and Reasoning_ 336–351 (PMLR, 2022); [https://proceedings.mlr.press/v177/idrissi22a.html](https://proceedings.mlr.press/v177/idrissi22a.html)
- 36.Sagawa, S., Koh, P. W., Hashimoto, T. B. & Liang, P. Distributionally robust neural networks for group shifts: on the importance of regularization for worst-case generalization. In _Proc. of the International Conference on Learning Representations_ (ICLR, 2020); [https://openreview.net/pdf?id=ryxGuJrFvS](https://openreview.net/pdf?id=ryxGuJrFvS)
- 37.Ganin, Y. et al. Domain-adversarial training of neural networks. _J. Mach. Learn. Res._ **17**, 2096–2030 (2016). \[ [Google Scholar](https://scholar.google.com/scholar_lookup?Ganin,%20Y.%20et%20al.%20Domain-adversarial%20training%20of%20neural%20networks.%20J.%20Mach.%20Learn.%20Res.17,%202096%E2%80%932030%20(2016).)\]
- 38.Li, Y. et al. Deep domain generalization via conditional invariant adversarial networks. In _Proc. of the European Conference on Computer Vision (ECCV)_ 624–639 (ECCV, 2018); [https://openaccess.thecvf.com/content\_ECCV\_2018/papers/Ya\_Li\_Deep\_Domain\_Generalization\_ECCV\_2018\_paper.pdf](https://openaccess.thecvf.com/content_ECCV_2018/papers/Ya_Li_Deep_Domain_Generalization_ECCV_2018_paper.pdf)
- 39.Polyak, B. T. & Juditsky, A. B. Acceleration of stochastic approximation by averaging. _SIAM J. Control Optim._ **30**, 838–855 (1992). \[ [Google Scholar](https://scholar.google.com/scholar_lookup?Polyak,%20B.%20T.%20&%20Juditsky,%20A.%20B.%20Acceleration%20of%20stochastic%20approximation%20by%20averaging.%20SIAM%20J.%20Control%20Optim.30,%20838%E2%80%93855%20(1992).)\]
- 40.Glocker, B., Jones, C., Bernhardt, M. & Winzeck, S. Algorithmic encoding of protected characteristics in chest X-ray disease detection models. _EBioMedicine_ **89**, 104467 (2023). \[ [DOI](https://doi.org/10.1016/j.ebiom.2023.104467)\] \[ [PMC free article](https://pmc.ncbi.nlm.nih.gov/articles/PMC10025760/)\] \[ [PubMed](https://pubmed.ncbi.nlm.nih.gov/36791660/)\]
- 41.Jones, C., Roschewitz, M. & Glocker, B. The role of subgroup separability in group-fair medical image classification. In _Medical Image Computing and Computer Assisted Intervention_ _—_ _MICCAI 2023_ Vol. 14222 (eds Greenspan, H. et al.) 179–188 (Springer, 2023); 10.1007/978-3-031-43898-1\_18
- 42.Wei, S. & Niethammer, M. The fairness-accuracy Pareto front. _Stat. Anal. Data Min._ **15**, 287–302 (2022). \[ [Google Scholar](https://scholar.google.com/scholar_lookup?Wei,%20S.%20&%20Niethammer,%20M.%20The%20fairness-accuracy%20Pareto%20front.%20Stat.%20Anal.%20Data%20Min.15,%20287%E2%80%93302%20(2022).)\]
- 43.Kleinberg, J., Mullainathan, S. & Raghavan, M. Inherent tradeoffs in the fair determination of risk scores. In _Proc. 8th Innovations in Theoretical Computer Science Conference (ITCS 2017)_ Vol. 67 (ed. Papadimitriou, C. H.) 1–23 (2017); 10.4230/LIPIcs.ITCS.2017
- 44.Miller, J. P. et al. Accuracy on the line: on the strong correlation between out-of-distribution and in-distribution generalization. In _Proc. 38th International Conference on Machine Learning_ 7721–7735 (PMLR, 2021); [https://proceedings.mlr.press/v139/miller21b/miller21b.pdf](https://proceedings.mlr.press/v139/miller21b/miller21b.pdf)
- 45.An, B., Che, Z., Ding, M. & Huang, F. Transferring fairness under distribution shifts via fair consistency regularization. In _Proc. 36th Conference on Neural Information Processing Systems_ (NeurIPS, 2022); [https://proceedings.neurips.cc/paper\_files/paper/2022/file/d1dbaabf454a479ca86309e66592c7f6-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/d1dbaabf454a479ca86309e66592c7f6-Paper-Conference.pdf)
- 46.Schrouff, J. et al. Diagnosing failures of fairness transfer across distribution shift in real-world medical settings. In _Proc. 36th Conference on Neural Information Processing Systems_ (NeurIPS, 2022); [https://proceedings.neurips.cc/paper\_files/paper/2022/file/7a969c30dc7e74d4e891c8ffb217cf79-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/7a969c30dc7e74d4e891c8ffb217cf79-Paper-Conference.pdf)
- 47.Vyas, D. A., Eisenstein, L. G. & Jones, D. S. Hidden in plain sight—reconsidering the use of race correction in clinical algorithms. _N. Engl. J. Med._ **383**, 874–882 (2020).
\[ [DOI](https://doi.org/10.1056/NEJMms2004740)\] \[ [PubMed](https://pubmed.ncbi.nlm.nih.gov/32853499/)\] \[ [Google Scholar](https://scholar.google.com/scholar_lookup?Vyas,%20D.%20A.,%20Eisenstein,%20L.%20G.%20&%20Jones,%20D.%20S.%20Hidden%20in%20plain%20sight%E2%80%94reconsidering%20the%20use%20of%20race%20correction%20in%20clinical%20algorithms.%20N.%20Engl.%20J.%20Med.383,%20874%E2%80%93882%20(2020).)\]
- 48.Jain, S. et al. VisualCheXbert: addressing the discrepancy between radiology report labels and image labels. In _Proc. of the Conference on Health, Inference, and Learning_ 105–115 (Association for Computing Machinery, 2021); 10.1145/3450439.3451862
- 49.Kumar, A., Deshpande, A. & Sharma, A. Causal effect regularization: automated detection and removal of spurious attributes. In _Proc. 37th Conference on Neural Information Processing Systems_ [https://openreview.net/pdf?id=V5Oh7Aqfft](https://openreview.net/pdf?id=V5Oh7Aqfft) (NeurIPS, 2023).
- 50.Basu, A. Use of race in clinical algorithms. _Sci. Adv._ **9**, eadd2704 (2023).
\[ [DOI](https://doi.org/10.1126/sciadv.add2704)\] \[ [PMC free article](https://pmc.ncbi.nlm.nih.gov/articles/PMC10219586/)\] \[ [PubMed](https://pubmed.ncbi.nlm.nih.gov/37235647/)\] \[ [Google Scholar](https://scholar.google.com/scholar_lookup?Basu,%20A.%20Use%20of%20race%20in%20clinical%20algorithms.%20Sci.%20Adv.9,%20eadd2704%20(2023).)\]
- 51.Chandra, A. & Skinner, J. _Geography and Racial Health Disparities_ (National Bureau of Economic Research, 2003); 10.3386/w9513
- 52.Suriyakumar, V. M., Ghassemi, M. & Ustun, B. When personalization harms: reconsidering the use of group attributes in prediction. In _Proc. 40th International Conference on Machine Learning_ 33209–33228 (PMLR, 2023); [https://proceedings.mlr.press/v202/suriyakumar23a.html](https://proceedings.mlr.press/v202/suriyakumar23a.html)
- 53.Manski, C. F., Mullahy, J. & Venkataramani, A. S. Using measures of race to make clinical predictions: decision making, patient health, and fairness. _Proc. Natl Acad. Sci. USA_ **120**, e2303370120 (2023).
\[ [DOI](https://doi.org/10.1073/pnas.2303370120)\] \[ [PMC free article](https://pmc.ncbi.nlm.nih.gov/articles/PMC10469015/)\] \[ [PubMed](https://pubmed.ncbi.nlm.nih.gov/37607231/)\] \[ [Google Scholar](https://scholar.google.com/scholar_lookup?Manski,%20C.%20F.,%20Mullahy,%20J.%20&%20Venkataramani,%20A.%20S.%20Using%20measures%20of%20race%20to%20make%20clinical%20predictions:%20decision%20making,%20patient%20health,%20and%20fairness.%20Proc.%20Natl%20Acad.%20Sci.%20USA120,%20e2303370120%20(2023).)\]
- 54.Gulrajani, I. & Lopez-Paz, D. In search of lost domain generalization. In _Proc. of the International Conference on Learning Representations_ (ICLR, 2021); [https://openreview.net/pdf?id=lQdXeXDoWtI](https://openreview.net/pdf?id=lQdXeXDoWtI)
- 55.Zhang, H. et al. An empirical framework for domain generalization in clinical settings. In _Proc. of the Conference on Health, Inference, and Learning_ 279–290 (Association for Computing Machinery, 2021); 10.1145/3450439.3451878
- 56.Branchaud-Charron, F., Atighehchian, P., Rodríguez, P., Abuhamad, G. & Lacoste, A. Can active learning preemptively mitigate fairness issues? Preprint at [https://arxiv.org/abs/2104.06879](https://arxiv.org/abs/2104.06879) (2021).
- 57.Artificial intelligence and machine learning (AI/ML)-enabled medical devices. _US Food & Drug Administration_ [https://www.fda.gov/medical-devices/software-medical-device-samd/artificial-intelligence-and-machine-learning-aiml-enabled-medical-devices](https://www.fda.gov/medical-devices/software-medical-device-samd/artificial-intelligence-and-machine-learning-aiml-enabled-medical-devices) (2022).
- 58.Koh, P. W. et al. WILDS: a benchmark of in-the-wild distribution shifts. In _Proc. 38th_ _International Conference on Machine Learning_ 5637–5664 (PMLR, 2021); [https://cs.stanford.edu/people/jure/pubs/wilds-icml21.pdf](https://cs.stanford.edu/people/jure/pubs/wilds-icml21.pdf)
- 59.Yang, Y., Zhang, H., Katabi, D. & Ghassemi, M. Change is hard: a closer look at subpopulation shift. In _Proc. 40th International Conference on Machine Learning_ 39584–39622 (Association for Computing Machinery, 2023); [https://proceedings.mlr.press/v202/yang23s/yang23s.pdf](https://proceedings.mlr.press/v202/yang23s/yang23s.pdf)
- 60.Mitchell, M. et al. Model cards for model reporting. In _Proc. of the Conference on Fairness, Accountability, and Transparency_ 220–229 (Association for Computing Machinery, 2019); 10.1145/3287560.3287596
- 61.Joint statement on enforcement efforts against discrimination and bias in automated systems. _Federal Trade Commission_ [https://www.ftc.gov/legal-library/browse/cases-proceedings/public-statements/joint-statement-enforcement-efforts-against-discrimination-bias-automated-systems](https://www.ftc.gov/legal-library/browse/cases-proceedings/public-statements/joint-statement-enforcement-efforts-against-discrimination-bias-automated-systems) (2023).
- 62.Gallifant, J. et al. Disparity dashboards: an evaluation of the literature and framework for health equity improvement. _Lancet Digit. Health_ **5**, e831–e839 (2023).
\[ [DOI](https://doi.org/10.1016/S2589-7500(23)00150-4)\] \[ [PMC free article](https://pmc.ncbi.nlm.nih.gov/articles/PMC10639125/)\] \[ [PubMed](https://pubmed.ncbi.nlm.nih.gov/37890905/)\] \[ [Google Scholar](https://scholar.google.com/scholar_lookup?Gallifant,%20J.%20et%20al.%20Disparity%20dashboards:%20an%20evaluation%20of%20the%20literature%20and%20framework%20for%20health%20equity%20improvement.%20Lancet%20Digit.%20Health5,%20e831%E2%80%93e839%20(2023).)\]
- 63.Zhang, H. et al. Improving the fairness of chest x-ray classifiers. In _Proc. of the_ _Conference on Health, Inference, and Learning_ Vo. 174, 204–233 (PMLR, 2022); [https://proceedings.mlr.press/v174/zhang22a/zhang22a.pdf](https://proceedings.mlr.press/v174/zhang22a/zhang22a.pdf)
- 64.Zietlow, D. et al. Leveling down in computer vision: Pareto inefficiencies in fair deep classifiers. In _Proc. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ 10410–10421 (IEEE, 2022); [https://openaccess.thecvf.com/content/CVPR2022/papers/Zietlow\_Leveling\_Down\_in\_Computer\_Vision\_Pareto\_Inefficiencies\_in\_Fair\_Deep\_CVPR\_2022\_paper.pdf](https://openaccess.thecvf.com/content/CVPR2022/papers/Zietlow_Leveling_Down_in_Computer_Vision_Pareto_Inefficiencies_in_Fair_Deep_CVPR_2022_paper.pdf)
- 65.Petersen, E., Holm, S., Ganz, M. & Feragen, A. The path toward equal performance in medical machine learning. _Patterns_ **4**, 100790 (2023).
\[ [DOI](https://doi.org/10.1016/j.patter.2023.100790)\] \[ [PMC free article](https://pmc.ncbi.nlm.nih.gov/articles/PMC10382979/)\] \[ [PubMed](https://pubmed.ncbi.nlm.nih.gov/37521051/)\] \[ [Google Scholar](https://scholar.google.com/scholar_lookup?Petersen,%20E.,%20Holm,%20S.,%20Ganz,%20M.%20&%20Feragen,%20A.%20The%20path%20toward%20equal%20performance%20in%20medical%20machine%20learning.%20Patterns4,%20100790%20(2023).)\]
- 66.Petersen, E., Ferrante, E., Ganz, M. & Feragen, A. Are demographically invariant models and representations in medical imaging fair? Preprint at [https://arxiv.org/abs/2305.01397](https://arxiv.org/abs/2305.01397) (2023).
- 67.Martinez, N., Bertran, M. & Sapiro G. Minimax Pareto fairness: a multi objective perspective. In _Proc. of the International Conference on Machine Learning_ Vol. 119, 6755–6764 (PMLR, 2020); [https://proceedings.mlr.press/v119/martinez20a.html](https://proceedings.mlr.press/v119/martinez20a.html) \[ [PMC free article](https://pmc.ncbi.nlm.nih.gov/articles/PMC7912461/)\] \[ [PubMed](https://pubmed.ncbi.nlm.nih.gov/33644764/)\]
- 68.Paulus, J. K. & Kent, D. M. Predictably unequal: understanding and addressing concerns that algorithmic clinical prediction may increase health disparities. _npj Digit. Med._ **3**, 99 (2020). \[ [DOI](https://doi.org/10.1038/s41746-020-0304-9)\] \[ [PMC free article](https://pmc.ncbi.nlm.nih.gov/articles/PMC7393367/)\] \[ [PubMed](https://pubmed.ncbi.nlm.nih.gov/32821854/)\]
- 69.Movva, R. et al. Coarse race data conceals disparities in clinical risk score performance. _In Proc. of the 8th Machine Learning for Healthcare Conference_. [https://proceedings.mlr.press/v219/movva23a.html](https://proceedings.mlr.press/v219/movva23a.html) 443-472 (PMLR, 2023).
- 70.Johnson, A. E. W. et al. MIMIC-IV, a freely accessible electronic health record dataset. _Sci. Data_ **10**, 1 (2023).
\[ [DOI](https://doi.org/10.1038/s41597-022-01899-x)\] \[ [PMC free article](https://pmc.ncbi.nlm.nih.gov/articles/PMC9810617/)\] \[ [PubMed](https://pubmed.ncbi.nlm.nih.gov/36596836/)\] \[ [Google Scholar](https://scholar.google.com/scholar_lookup?Johnson,%20A.%20E.%20W.%20et%20al.%20MIMIC-IV,%20a%20freely%20accessible%20electronic%20health%20record%20dataset.%20Sci.%20Data10,%201%20(2023).)\]
- 71.Fanni, S. C. et al. Artificial intelligence-based software with CE mark for chest X-ray interpretation: opportunities and challenges. _Diagnostics (Basel)_ **13**, 2020 (2023).
\[ [DOI](https://doi.org/10.3390/diagnostics13122020)\] \[ [PMC free article](https://pmc.ncbi.nlm.nih.gov/articles/PMC10297610/)\] \[ [PubMed](https://pubmed.ncbi.nlm.nih.gov/37370915/)\] \[ [Google Scholar](https://scholar.google.com/scholar_lookup?Fanni,%20S.%20C.%20et%20al.%20Artificial%20intelligence-based%20software%20with%20CE%20mark%20for%20chest%20X-ray%20interpretation:%20opportunities%20and%20challenges.%20Diagnostics%20(Basel)13,%202020%20(2023).)\]
- 72.Lipton, Z. C., Elkan, C. & Naryanaswamy, B. Optimal thresholding of classifiers to maximize F1 measure. _Mach. Learn. Knowl. Discov. Databases_ 10.1007/978-3-662-44851-9\_15 (2014). \[ [DOI](https://doi.org/10.1007/978-3-662-44851-9_15)\] \[ [PMC free article](https://pmc.ncbi.nlm.nih.gov/articles/PMC4442797/)\] \[ [PubMed](https://pubmed.ncbi.nlm.nih.gov/26023687/)\]
- 73.Guo, C., Pleiss, G., Sun, Y. & Weinberger, K. Q. On calibration of modern neural networks. In _Proc. of the 34th International Conference on Machine Learning_ 1321–1330 (PMLR, 2017); [https://proceedings.mlr.press/v70/guo17a/guo17a.pdf](https://proceedings.mlr.press/v70/guo17a/guo17a.pdf)
- 74.Kuppers, F., Kronenberger, J., Shantia, A. & Haselhoff A. Multivariate confidence calibration for object detection. In _Proc. of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)_ 1322–1330 (IEEE, 2020); 10.1109/CVPRW50498.2020.00171
- 75.Barocas, S., Hardt, M. & Narayanan, A. _Fairness and Machine Learning: Limitations and Opportunities_ (MIT Press, 2019).
- 76.Pfohl, S. et al. Net benefit, calibration, threshold selection, and training objectives for algorithmic fairness in healthcare. In _Proc. of the 2022 ACM Conference on Fairness, Accountability, and Transparency_ 1039–1052 (Association for Computing Machinery, 2022); 10.1145/3531146.3533166
- 77.Bernhardt, M., Jones, C. & Glocker, B. Potential sources of dataset bias complicate investigation of underdiagnosis by machine learning algorithms. _Nat. Med._ **28**, 1157–1158 (2022).
\[ [DOI](https://doi.org/10.1038/s41591-022-01846-8)\] \[ [PubMed](https://pubmed.ncbi.nlm.nih.gov/35710993/)\] \[ [Google Scholar](https://scholar.google.com/scholar_lookup?Bernhardt,%20M.,%20Jones,%20C.%20&%20Glocker,%20B.%20Potential%20sources%20of%20dataset%20bias%20complicate%20investigation%20of%20underdiagnosis%20by%20machine%20learning%20algorithms.%20Nat.%20Med.28,%201157%E2%80%931158%20(2022).)\]
- 78.Aickin, M. & Gensler, H. Adjusting for multiple testing when reporting research results: the Bonferroni vs Holm methods. _Am. J. Public Health_ **5**, 726–728 (1996). \[ [DOI](https://doi.org/10.2105/ajph.86.5.726)\] \[ [PMC free article](https://pmc.ncbi.nlm.nih.gov/articles/PMC1380484/)\] \[ [PubMed](https://pubmed.ncbi.nlm.nih.gov/8629727/)\] \[ [Google Scholar](https://scholar.google.com/scholar_lookup?Aickin,%20M.%20&%20Gensler,%20H.%20Adjusting%20for%20multiple%20testing%20when%20reporting%20research%20results:%20the%20Bonferroni%20vs%20Holm%20methods.%20Am.%20J.%20Public%20Health5,%20726%E2%80%93728%20(1996).)\]
- 79.Wang, Z., Wu, Z., Agarwal, D. & Sun, J. MedCLIP: contrastive learning from unpaired medical images and text. In _Proc. of the 2022 Conference on Empirical Methods in Natural Language Processing_ (eds Goldberg, Y. et al.) 3876–3887 (Association for Computational Linguistics, 2022); 10.18653/v1/2022.emnlp-main.256 \[ [DOI](https://doi.org/10.18653/v1/2022.emnlp-main.256)\] \[ [PMC free article](https://pmc.ncbi.nlm.nih.gov/articles/PMC11323634/)\] \[ [PubMed](https://pubmed.ncbi.nlm.nih.gov/39144675/)\]
- 80.Tiu, E. et al. Expert-level detection of pathologies from unannotated chest X-ray images via self-supervised learning. _Nat. Biomed. Eng._ **6**, 1399–1406 (2022).
\[ [DOI](https://doi.org/10.1038/s41551-022-00936-9)\] \[ [PMC free article](https://pmc.ncbi.nlm.nih.gov/articles/PMC9792370/)\] \[ [PubMed](https://pubmed.ncbi.nlm.nih.gov/36109605/)\] \[ [Google Scholar](https://scholar.google.com/scholar_lookup?Tiu,%20E.%20et%20al.%20Expert-level%20detection%20of%20pathologies%20from%20unannotated%20chest%20X-ray%20images%20via%20self-supervised%20learning.%20Nat.%20Biomed.%20Eng.6,%201399%E2%80%931406%20(2022).)\]
- 81.Rabanser, S., Günnemann, S. & Lipton, Z. Failing loudly: an empirical study of methods for detecting dataset shift. In _Proc. 33rd Conference on Neural Information Processing Systems_ (NeurIPS, 2019); [https://proceedings.neurips.cc/paper\_files/paper/2019/file/846c260d715e5b854ffad5f70a516c88-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2019/file/846c260d715e5b854ffad5f70a516c88-Paper.pdf)
- 82.Deng, J. et al. Imagenet: a large-scale hierarchical image database. In _Proc. of the 2009 IEEE Conference on Computer Vision and Pattern Recognition_ 248–255 (IEEE, 2009); 10.1109/CVPR.2009.5206848
- 83.Bergstra, J. & Bengio, Y. Random search for hyper-parameter optimization. _J. Mach. Learn. Res._ **13**, 281–305 (2012). \[ [Google Scholar](https://scholar.google.com/scholar_lookup?Bergstra,%20J.%20&%20Bengio,%20Y.%20Random%20search%20for%20hyper-parameter%20optimization.%20J.%20Mach.%20Learn.%20Res.13,%20281%E2%80%93305%20(2012).)\]
- 84.Pedregosa, F. et al. Scikit-learn: machine learning in Python. _J. Mach. Learn. Res._ **12**, 2825–2830 (2011). \[ [Google Scholar](https://scholar.google.com/scholar_lookup?Pedregosa,%20F.%20et%20al.%20Scikit-learn:%20machine%20learning%20in%20Python.%20J.%20Mach.%20Learn.%20Res.12,%202825%E2%80%932830%20(2011).)\]

## Associated Data

_This section collects any data citations, data availability statements, or supplementary materials included in this article._

### Supplementary Materials

[41591\_2024\_3113\_MOESM1\_ESM.pdf](https://pmc.ncbi.nlm.nih.gov/articles/instance/11485237/bin/41591_2024_3113_MOESM1_ESM.pdf) (1.1MB, pdf)

Supplementary Tables 1–4, Figs. 1–4 and Note 1.

[Reporting Summary](https://pmc.ncbi.nlm.nih.gov/articles/instance/11485237/bin/41591_2024_3113_MOESM2_ESM.pdf) (2.1MB, pdf)

### Data Availability Statement

All datasets used in this study are publicly available. The MIMIC-CXR ( [https://www.physionet.org/content/mimic-cxr-jpg/2.1.0/](https://www.physionet.org/content/mimic-cxr-jpg/2.1.0/)) and VinDr-CXR ( [https://physionet.org/content/vindr-cxr/1.0.0/](https://physionet.org/content/vindr-cxr/1.0.0/)) datasets are available from PhysioNet after completion of a data use agreement and a credentialing procedure. The CheXpert dataset ( [https://stanfordmlgroup.github.io/competitions/chexpert/](https://stanfordmlgroup.github.io/competitions/chexpert/)), along with associated race labels, is available from the Stanford Center for Artificial Intelligence in Medicine & Imaging website ( [https://aimi.stanford.edu/](https://aimi.stanford.edu/)). The ChestX-ray14 (National Institutes of Health) dataset ( [https://nihcc.app.box.com/v/ChestXray-NIHCC](https://nihcc.app.box.com/v/ChestXray-NIHCC)) is available to download from the National Institutes of Health Clinical Center. The PadChest dataset ( [https://academictorrents.com/details/96ebb4f92b85929eadfb16761f310a6d04105797](https://academictorrents.com/details/96ebb4f92b85929eadfb16761f310a6d04105797)) can be downloaded from the Medical Imaging Databank of the Valencia Region. The SIIM-ACR Pneumothorax Segmentation dataset ( [https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation](https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation)) can be downloaded from its Kaggle contest page. The ISIC 2020 dataset ( [https://challenge.isic-archive.com/data/#2020](https://challenge.isic-archive.com/data/#2020)) can be downloaded from the SIIM-ISIC Melanoma Classification Challenge page. The ODIR dataset ( [https://odir2019.grand-challenge.org/dataset/](https://odir2019.grand-challenge.org/dataset/)) can be obtained from the ODIR 2019 challenge hosted by Grand Challenges.

Code that supports the findings of this study is publicly available with an open-source license at [https://github.com/YyzHarry/shortcut-ood-fairness](https://github.com/YyzHarry/shortcut-ood-fairness).

![Close](https://pmc.ncbi.nlm.nih.gov/static/img/usa-icons/close.svg)

## ACTIONS

- [View on publisher site](https://doi.org/10.1038/s41591-024-03113-4)
- [PDF (11.4 MB)](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/pdf/41591_2024_Article_3113.pdf)
- Cite
- Collections
- Permalink




## PERMALINK



Copy


## RESOURCES

### Similar articles

### Cited by other articles

### Links to NCBI Databases

Back to Top

---

## Content Analysis Summary

- **Total Sources Processed:** 5
- **Average Content Length:** 65,761 characters
- **Content Types Found:** Tables, Structured Text, Plain Text, Tables
